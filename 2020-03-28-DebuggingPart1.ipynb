{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Debugging the Mid and High Level DataBlock.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tduj_egq2m8j",
        "colab_type": "text"
      },
      "source": [
        "# Debugging the API - Part 1\n",
        "> A warm introduction to `fastai`'s built in debugging tools\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- image: images/chart-preview.png\n",
        "- category: DataBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT5Ztlcs3G5X",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this:\n",
        "\n",
        "* `fastai2`: 0.0.15\n",
        "* `fastcore`: 0.1.16\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GVlMdyO3Tym",
        "colab_type": "text"
      },
      "source": [
        "# `DataBlock` Part 3 - Debugging the High-Level API\n",
        "\n",
        "So we have this `DataBlock` API where we can build this blueprint of how we want to build our data, but how can we debug this? Is it possible for us to see if we can even build a `DataLoader`? Or forsee any bugs that may be happening at a later date?\n",
        "\n",
        "**Yes!** Thanks to the valient efforts of Sylvain Gugger this is possible through `datablock.summary()`. In today's article we will be exploring just how useful `datablock.summary()` can be and how we can apply this to the Mid-Level API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDifRrhM31ck",
        "colab_type": "text"
      },
      "source": [
        "We'll again be working with our example problems nested in Computer Vision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oskYpsYY3z4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai2.vision.all import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQik8CEC3v77",
        "colab_type": "text"
      },
      "source": [
        "To begin with let's make our simple `PETS` `DataBlock` step by step:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwvj1fu84R2V",
        "colab_type": "text"
      },
      "source": [
        "First grab our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOepxE1H3LLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.PETS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCHDS4iI4Tuv",
        "colab_type": "text"
      },
      "source": [
        "Then how we want to grab the class from the filename using a Regular Expression and some way to split the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITAVp40i4EnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pat = r'([^/]+)_\\d+.*$'\n",
        "splitter = RandomSplitter(valid_pct=0.2, seed=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iumHSsl14ZGN",
        "colab_type": "text"
      },
      "source": [
        "And then finally our augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sga7fw34Q2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "item_tfms = [Resize(224, method='crop')]\n",
        "batch_tfms=[*aug_transforms(size=256), Normalize.from_stats(*imagenet_stats)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSTGHxgo4hu7",
        "colab_type": "text"
      },
      "source": [
        "Now to build the `DataBlock` we are used to doing something like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzn2DO6U4gFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
        "                   get_items=get_image_files,\n",
        "                   splitter=RandomSplitter(),\n",
        "                   get_y=RegexLabeller(pat=pat),\n",
        "                   item_tfms=item_tfms,\n",
        "                   batch_tfms=batch_tfms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrcbzynk4u9S",
        "colab_type": "text"
      },
      "source": [
        "And then we'd do something like this to build our `DataLoaders`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic7w1EE04uaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dls = dblock.dataloaders(path/'images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htPeH_Fl5DVh",
        "colab_type": "text"
      },
      "source": [
        "However! We won't be doing that today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBPC5FOc5FGU",
        "colab_type": "text"
      },
      "source": [
        "# `dblock.summary()`\n",
        "\n",
        "With each instance of `DataBlock` we make we can call `.summary()` passing in however our `DataBlock` is expected to be built with. For our problem this is passing in the `path/'images`. \n",
        "\n",
        "Now what *exactly* does `dblock.summary()` do? It will attempt to run through your entire `DataBlock` and attempt to build one batch of data, printing out each time it walks through the `DataBlock` and what it's doing. \n",
        "\n",
        "Let's see that here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMRcBaZ84z7J",
        "colab_type": "code",
        "outputId": "000922b7-3df4-496e-ee28-af09c4586336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "dblock.summary(path/'images')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting-up type transforms pipelines\n",
            "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
            "Found 7390 items\n",
            "2 datasets of sizes 5912,1478\n",
            "Setting up Pipeline: PILBase.create\n",
            "Setting up Pipeline: RegexLabeller -> Categorize\n",
            "\n",
            "Building one sample\n",
            "  Pipeline: PILBase.create\n",
            "    starting from\n",
            "      /root/.fastai/data/oxford-iiit-pet/images/Ragdoll_10.jpg\n",
            "    applying PILBase.create gives\n",
            "      PILImage mode=RGB size=500x375\n",
            "  Pipeline: RegexLabeller -> Categorize\n",
            "    starting from\n",
            "      /root/.fastai/data/oxford-iiit-pet/images/Ragdoll_10.jpg\n",
            "    applying RegexLabeller gives\n",
            "      Ragdoll\n",
            "    applying Categorize gives\n",
            "      TensorCategory(8)\n",
            "\n",
            "Final sample: (PILImage mode=RGB size=500x375, TensorCategory(8))\n",
            "\n",
            "\n",
            "Setting up after_item: Pipeline: Resize -> ToTensor\n",
            "Setting up before_batch: Pipeline: \n",
            "Setting up after_batch: Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
            "\n",
            "Building one batch\n",
            "Applying item_tfms to the first sample:\n",
            "  Pipeline: Resize -> ToTensor\n",
            "    starting from\n",
            "      (PILImage mode=RGB size=500x375, TensorCategory(8))\n",
            "    applying Resize gives\n",
            "      (PILImage mode=RGB size=224x224, TensorCategory(8))\n",
            "    applying ToTensor gives\n",
            "      (TensorImage of size 3x224x224, TensorCategory(8))\n",
            "\n",
            "Adding the next 3 samples\n",
            "\n",
            "No before_batch transform to apply\n",
            "\n",
            "Collating items in a batch\n",
            "\n",
            "Applying batch_tfms to the batch built\n",
            "  Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
            "    starting from\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([ 8, 13,  0, 33]))\n",
            "    applying IntToFloatTensor gives\n",
            "      (TensorImage of size 4x3x224x224, TensorCategory([ 8, 13,  0, 33]))\n",
            "    applying AffineCoordTfm gives\n",
            "      (TensorImage of size 4x3x256x256, TensorCategory([ 8, 13,  0, 33]))\n",
            "    applying LightingTfm gives\n",
            "      (TensorImage of size 4x3x256x256, TensorCategory([ 8, 13,  0, 33]))\n",
            "    applying Normalize gives\n",
            "      (TensorImage of size 4x3x256x256, TensorCategory([ 8, 13,  0, 33]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpUyT4oL5gFV",
        "colab_type": "text"
      },
      "source": [
        "Wow that's a *lot* of information being thrown at us! Let's break it down piece by piece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0S52PAZ6D0l",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First we can see a bunch of `setup` being called:\n",
        "\n",
        "```python\n",
        "Setting-up type transforms pipelines\n",
        "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
        "Found 7390 items\n",
        "2 datasets of sizes 5912,1478\n",
        "Setting up Pipeline: PILBase.create\n",
        "Setting up Pipeline: RegexLabeller -> Categorize\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsRuhyQj6ONy",
        "colab_type": "text"
      },
      "source": [
        "1. First we went and grabbed all of our `x`'s (7390)\n",
        "2. Then we split it into a validation and training dataset (this is done via `RandomSplitter()` by default)\n",
        "3. Then we set up our two `type` transform `Pipelines`. Now all a `Pipeline` is is a set of transforms done in a particular order. \n",
        "  * So here we can see that for our `x`'s we set up one where we call `PILBase.create` (which was nested inside of our `ImageBlock`)\n",
        "  * For our `y`'s we setup a `Pipeline` to first call our `RegexLabeller` and then to further call `Categorize`. `Categorize` simply maps all of the possible categories to a value (IE `class_a` is 0, `class_b` is 1) and then sets up transforms for doing so.\n",
        "\n",
        "So now we've gone through the first few blocks! And it all checks out too! Let's dig a little deeper now:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKlgyTru7dDe",
        "colab_type": "text"
      },
      "source": [
        "## Building One Sample\n",
        "\n",
        "The next thing `dblock.summary()` will attempt to do is build *one* sample. This is one individual sample, **not** a batch. Let's read it's output:\n",
        "\n",
        "\n",
        "```python\n",
        "Building one sample\n",
        "  Pipeline: PILBase.create\n",
        "    starting from\n",
        "      /root/.fastai/data/oxford-iiit-pet/images/Ragdoll_10.jpg\n",
        "    applying PILBase.create gives\n",
        "      PILImage mode=RGB size=500x375\n",
        "  Pipeline: RegexLabeller -> Categorize\n",
        "    starting from\n",
        "      /root/.fastai/data/oxford-iiit-pet/images/Ragdoll_10.jpg\n",
        "    applying RegexLabeller gives\n",
        "      Ragdoll\n",
        "    applying Categorize gives\n",
        "      TensorCategory(8)\n",
        "\n",
        "Final sample: (PILImage mode=RGB size=500x375, TensorCategory(8))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s79_37mi7pQJ",
        "colab_type": "text"
      },
      "source": [
        "This reads very plainly, we started from a particular filename and applied `PILBase.create` to give us a `PILImage` to work with. We further apply our `RegexLabeller`and `Categorize` to give us our encoded category of `TensorCategory(0)`. And finally we can see the final sample shown. \n",
        "\n",
        "Great! We can build a sample now! What's next?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsOYz0oc7_FY",
        "colab_type": "text"
      },
      "source": [
        "## Building a Batch\n",
        "\n",
        "Now well try to build a batch from our data:\n",
        "\n",
        "```python\n",
        "Setting up after_item: Pipeline: Resize -> ToTensor\n",
        "Setting up before_batch: Pipeline: \n",
        "Setting up after_batch: Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
        "\n",
        "Building one batch\n",
        "Applying item_tfms to the first sample:\n",
        "  Pipeline: Resize -> ToTensor\n",
        "    starting from\n",
        "      (PILImage mode=RGB size=500x375, TensorCategory(8))\n",
        "    applying Resize gives\n",
        "      (PILImage mode=RGB size=224x224, TensorCategory(8))\n",
        "    applying ToTensor gives\n",
        "      (TensorImage of size 3x224x224, TensorCategory(8))\n",
        "```\n",
        "### `after_item` and `after_batch`\n",
        "Wait where did this `after_item` and `after_batch` stuff come from? \n",
        "\n",
        "Our `item_tfms` and `batch_tfms` will eventually turn into `after_item` and `after_batch` the further we dig in. So we can see that we setup some item transforms in a `Pipeline` to Resize and convert our `x`'s to a Tensor (`ToTensor`) and we setup some batch transforms in a `Pipeline` to convert our Tensor's to floats (for the GPU), and then we see these `AffineCoordTfm`, `LightingTfm`, and `Normalize`.\n",
        "\n",
        "But wait! That's not what I passed in! I passed in `*aug_transforms(size=256)` and `Normalize.from_stats(*imagenet_stats)`. Where did that come from? \n",
        "\n",
        "`AffineCoordTfm`'s are the class most augmentation inherit from, and if it has to do with any lighting same for the `LightingTfm`. Each have a particular way they deal with augmentation and so we can generalize. See below for `aug_transforms`. I'm going to remove the middle of the function so we just focus on what we need to see right now, which is the `return` statement:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkqAfPOR6y_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aug_transforms(mult=1.0, do_flip=True, flip_vert=False, max_rotate=10., max_zoom=1.1, max_lighting=0.2,\n",
        "                   max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=None, size=None,\n",
        "                   mode='bilinear', pad_mode=PadMode.Reflection, align_corners=True, batch=False, min_scale=1.):\n",
        "    \"Utility func to easily create a list of flip, rotate, zoom, warp, lighting transforms.\"\n",
        "    return setup_aug_tfms(res + L(xtra_tfms))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za5Vu4UG9Jbt",
        "colab_type": "text"
      },
      "source": [
        "We can see we call `setup_aug_tfms`. Let's look at that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH4DsgIr9MHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_aug_tfms(tfms):\n",
        "    \"Go through `tfms` and combines together affine/coord or lighting transforms\"\n",
        "    aff_tfms = [tfm for tfm in tfms if isinstance(tfm, AffineCoordTfm)]\n",
        "    lig_tfms = [tfm for tfm in tfms if isinstance(tfm, LightingTfm)]\n",
        "    others = [tfm for tfm in tfms if tfm not in aff_tfms+lig_tfms]\n",
        "    aff_tfm,lig_tfm =  _compose_same_tfms(aff_tfms),_compose_same_tfms(lig_tfms)\n",
        "    res = [aff_tfm] if aff_tfm is not None else []\n",
        "    if lig_tfm is not None: res.append(lig_tfm)\n",
        "    return res + others"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goof7lM19QJX",
        "colab_type": "text"
      },
      "source": [
        "And now we can see that `setup_aug_transforms` will return any `AffineCoordTfm` (`res`) and any `LightingTfm` (`light_tfms`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvL5iaCq99HD",
        "colab_type": "text"
      },
      "source": [
        "## Building the Batch (cont)\n",
        "\n",
        "Now if we continue along, we can see that it will first try to make three more samples from our data before then building a `batch` off of this data and applying any batch transforms:\n",
        "\n",
        "```python\n",
        "Adding the next 3 samples\n",
        "\n",
        "No before_batch transform to apply\n",
        "\n",
        "Collating items in a batch\n",
        "\n",
        "Applying batch_tfms to the batch built\n",
        "  Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
        "    starting from\n",
        "      (TensorImage of size 4x3x224x224, TensorCategory([ 8, 13,  0, 33]))\n",
        "    applying IntToFloatTensor gives\n",
        "      (TensorImage of size 4x3x224x224, TensorCategory([ 8, 13,  0, 33]))\n",
        "    applying AffineCoordTfm gives\n",
        "      (TensorImage of size 4x3x256x256, TensorCategory([ 8, 13,  0, 33]))\n",
        "    applying LightingTfm gives\n",
        "      (TensorImage of size 4x3x256x256, TensorCategory([ 8, 13,  0, 33]))\n",
        "    applying Normalize gives\n",
        "      (TensorImage of size 4x3x256x256, TensorCategory([ 8, 13,  0, 33]))\n",
        "```\n",
        "\n",
        "And we can see where each one gets applied and if there is any noticeable changes! Now this is all fine, as it didn't throw any errors or anything. But what happens if I forget something? Say I didn't make all my images the same size with a `Resize`? Let's check out that behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTvmEMqT_xpl",
        "colab_type": "text"
      },
      "source": [
        "# Breaking the `DataBlock`\n",
        "\n",
        "Let's remove our `item_tfms` from our `DataBlock`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMu4fiSX_7s2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
        "                   get_items=get_image_files,\n",
        "                   splitter=RandomSplitter(),\n",
        "                   get_y=RegexLabeller(pat=pat),\n",
        "                   item_tfms=[],\n",
        "                   batch_tfms=batch_tfms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfW4lT4T_83O",
        "colab_type": "text"
      },
      "source": [
        "And call `summary`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHFVBzGH5y_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dblock.summary(path/'images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtUnpd2rAEpo",
        "colab_type": "text"
      },
      "source": [
        "Aside from a `RuntimeError`, we get the following result:\n",
        "\n",
        "```python\n",
        "Setting-up type transforms pipelines\n",
        "Collecting items from /root/.fastai/data/oxford-iiit-pet/images\n",
        "Found 7390 items\n",
        "2 datasets of sizes 5912,1478\n",
        "Setting up Pipeline: PILBase.create\n",
        "Setting up Pipeline: RegexLabeller -> Categorize\n",
        "\n",
        "Building one sample\n",
        "  Pipeline: PILBase.create\n",
        "    starting from\n",
        "      /root/.fastai/data/oxford-iiit-pet/images/shiba_inu_26.jpg\n",
        "    applying PILBase.create gives\n",
        "      PILImage mode=RGB size=333x500\n",
        "  Pipeline: RegexLabeller -> Categorize\n",
        "    starting from\n",
        "      /root/.fastai/data/oxford-iiit-pet/images/shiba_inu_26.jpg\n",
        "    applying RegexLabeller gives\n",
        "      shiba_inu\n",
        "    applying Categorize gives\n",
        "      TensorCategory(33)\n",
        "\n",
        "Final sample: (PILImage mode=RGB size=333x500, TensorCategory(33))\n",
        "\n",
        "\n",
        "Setting up after_item: Pipeline: ToTensor\n",
        "Setting up before_batch: Pipeline: \n",
        "Setting up after_batch: Pipeline: IntToFloatTensor -> AffineCoordTfm -> LightingTfm -> Normalize\n",
        "\n",
        "Building one batch\n",
        "Applying item_tfms to the first sample:\n",
        "  Pipeline: ToTensor\n",
        "    starting from\n",
        "      (PILImage mode=RGB size=333x500, TensorCategory(33))\n",
        "    applying ToTensor gives\n",
        "      (TensorImage of size 3x500x333, TensorCategory(33))\n",
        "\n",
        "Adding the next 3 samples\n",
        "\n",
        "No before_batch transform to apply\n",
        "\n",
        "Collating items in a batch\n",
        "Error! It's not possible to collate your items in a batch\n",
        "Could not collate the 0-th members of your tuples because got the following shapes\n",
        "torch.Size([3, 500, 333]),torch.Size([3, 225, 300]),torch.Size([3, 500, 332]),torch.Size([3, 333, 500])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pITGPmkHAJ50",
        "colab_type": "text"
      },
      "source": [
        "Let's focus on it's tail end:\n",
        "\n",
        "```python\n",
        "Error! It's not possible to collate your items in a batch\n",
        "Could not collate the 0-th members of your tuples because got the following shapes\n",
        "torch.Size([3, 500, 333]),torch.Size([3, 225, 300]),torch.Size([3, 500, 332]),torch.Size([3, 333, 500])\n",
        "```\n",
        "\n",
        "This tells us very simply that we could not build a batch because our images were not all the same size! (We know these are our images because they have three channels). So the simple solution is to `Resize` them in an `Item` transform. \n",
        "\n",
        "From a personal takeaway, I *always* use `dblock.summary()` each time I work with the high-level API before I build any `DataLoader` as a simple piece of mind because if anything were to go wrong I know right away what it was!\n",
        "\n",
        "This is the first part of this two part mini-series on debugging. In the next blog we'll be looking at `.summary()` more closely and see if we can't apply its ideas to the low-level API"
      ]
    }
  ]
}