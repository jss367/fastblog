{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-06-08-fastinference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LY7y2kfCqEC",
        "colab_type": "text"
      },
      "source": [
        "# A walk with fastinference - Part 1\n",
        "> A guided walkthrough of the `fastinference` module\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- image: images/chart-preview.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6vT-yHhC3aK",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this:\n",
        "\n",
        "* `fastai2`: 0.0.17\n",
        "* `fastcore`: 0.1.18\n",
        "* `fastinference`: 0.0.4\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBW8AfruDKzl",
        "colab_type": "text"
      },
      "source": [
        "## `fastinference`, what is it and why do I need it?\n",
        "\n",
        "Over the last few months I've been trying to speed up inference for `fastai`, and more and more I was noticing that I was using the same \"realm\" of functions! So I decided to try to fit them into a cohesive library, with some special perks along with it. In this library we have (along with the speed-up modules), some interpretability modules as well as ONNX support. Each will be getting their own article to walk you through those, today we'll be covering the `fastai` speed ups!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3emxoiBhEBp5",
        "colab_type": "text"
      },
      "source": [
        "## Starting with `Vision`\n",
        "\n",
        "Let's begin with a vision problem. We'll use our *very* familiar `PETs` dataset and quickly train a model. What we'll be focusing on is two very specific functions, `.predict` and `.get_preds`, so first let's prepare:\n",
        "\n",
        "  * I will be skipping the explaination for this part, if you'd like an in-depth walkthrough of the API, see my article [here](https://muellerzr.github.io/fastblog/datablock/2020/03/21/DataBlockAPI.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1waPGUxDBFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai2.vision.all import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5wenuAYESiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "442b77e0-1f8a-4a72-9f80-b48bed214aa7"
      },
      "source": [
        "path = untar_data(URLs.PETS)\n",
        "pat = r'([^/]+)_\\d+.*$'\n",
        "splitter = RandomSplitter(valid_pct=0.2, seed=42)\n",
        "item_tfms = [Resize(224, method='crop')]\n",
        "batch_tfms=[*aug_transforms(size=256), Normalize.from_stats(*imagenet_stats)]\n",
        "fnames = get_image_files(path/'images')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw3CLvNGEj6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
        "                   get_items=get_image_files,\n",
        "                   splitter=RandomSplitter(),\n",
        "                   get_y=RegexLabeller(pat=pat),\n",
        "                   item_tfms=item_tfms,\n",
        "                   batch_tfms=batch_tfms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZwGjHfpElNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pets_dls = dblock.dataloaders(path/'images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SRLEvgVE2Uq",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our `DataLoader`, we'll use a regular `resnet34` so we can see the speed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4XeeVXIEnoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = cnn_learner(pets_dls, resnet34, metrics=accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A__GObezFEgW",
        "colab_type": "code",
        "outputId": "3f607bad-9bee-4895-ff4d-0832ab0beb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "learn.fine_tune(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.499775</td>\n",
              "      <td>0.304153</td>\n",
              "      <td>0.901218</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.453589</td>\n",
              "      <td>0.239638</td>\n",
              "      <td>0.924222</td>\n",
              "      <td>00:55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EnXNhpBIIvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('pets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCG4NSMDGASt",
        "colab_type": "text"
      },
      "source": [
        "Now we'll time a `get_preds` and a `predict` on both `CPU` and `CUDA`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK-QPkFlFGjc",
        "colab_type": "code",
        "outputId": "4412eff8-a953-4f4b-f2d4-c425b615d745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# CUDA:\n",
        "preds = learn.get_preds()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 990 ms, sys: 326 ms, total: 1.32 s\n",
            "Wall time: 10 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5oMPICrGK4e",
        "colab_type": "code",
        "outputId": "7ad09e94-c8d2-43d8-ebfb-98d7652cf88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# CUDA:\n",
        "pred = learn.predict(fnames[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 72.3 ms, sys: 140 ms, total: 212 ms\n",
            "Wall time: 253 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOXjXVsVGVWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.dls.device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUA45eBlGuQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# CPU:\n",
        "preds = learn.get_preds()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmrjlzJsHV3t",
        "colab_type": "text"
      },
      "source": [
        "* Note I skipped this, as it takes ~4:54 seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85LxMPm0GlOX",
        "colab_type": "code",
        "outputId": "52d003a8-6265-4d43-a1ca-ce8d44257be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# CPU:\n",
        "preds = learn.predict(fnames[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 243 ms, sys: 9.13 ms, total: 252 ms\n",
            "Wall time: 263 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0ZuXrFlG4GW",
        "colab_type": "text"
      },
      "source": [
        "Okay, not too bad right? But can we make it *better*. Let's bring in `fastinference`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWqzWekVHBhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastinference.inference import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjqEbtsMHMQ7",
        "colab_type": "text"
      },
      "source": [
        "Now for those of you Python-savvy, you'll notice we don't actually import anything. Why? We override `.predict` and `.get_preds`. As such, no adjustments are needed except for rebuilding `Learner`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMhyCXbzHlUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pets_dls.device= 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q7MAWiJHL1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = cnn_learner(pets_dls, resnet34, metrics=accuracy).load('pets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-kSqX6bIK4K",
        "colab_type": "text"
      },
      "source": [
        "Let's try our timings again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YImxw2qfHi0g",
        "colab_type": "code",
        "outputId": "9dc7d829-eb2d-405c-ccdd-3fdc0426384c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# CUDA:\n",
        "preds = learn.get_preds()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 649 ms, sys: 206 ms, total: 855 ms\n",
            "Wall time: 9.83 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_vJlkn8INif",
        "colab_type": "code",
        "outputId": "61487a7e-2a7e-455b-aaa2-ac238c242c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# CUDA:\n",
        "pred = learn.predict(fnames[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 32.1 ms, sys: 2.01 ms, total: 34.1 ms\n",
            "Wall time: 34.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vqKprGkIU5c",
        "colab_type": "text"
      },
      "source": [
        "So as you saw, we shaved down only ~0.2 seconds from `get_preds`, but `predict` we could reduce it by almost 200 milliseconds! This is only 13% of the time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ2IYcKaIThA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.dls.device = 'cpu'\n",
        "learn.model.to('cpu');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ0awG-TIobY",
        "colab_type": "code",
        "outputId": "a0d7ab09-ea2f-4136-a455-dfb7fabf3090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# CPU:\n",
        "preds = learn.predict(fnames[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 229 ms, sys: 6.8 ms, total: 236 ms\n",
            "Wall time: 239 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjXYcRgCIwcn",
        "colab_type": "text"
      },
      "source": [
        "*And* a 20 millsecond reduction on the CPU! Not bad at all! So what changed? `get_preds` now looks very reminiscent of a `PyTorch` loop, with no `fastai` parts. This is intentional, as now we can blend the two together. \n",
        "\n",
        "Okay... you've sped it up, what about these \"Quality of Life\" improvements you mentioned? Let's take a look:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayEy-cefJIEY",
        "colab_type": "text"
      },
      "source": [
        "## Quality of Life Improvements\n",
        "\n",
        "The `get_preds` and `predict` adjustments don't end there. We'll start with the major changes to `get_preds`:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltN7oW6AJlQs",
        "colab_type": "text"
      },
      "source": [
        "### `get_preds`\n",
        "\n",
        "`get_preds` allows for any of the usual `fastai` paramters, such as `ds_idx`, `dl`, and any other `**kwargs` you may want. But what has changed is now we have 3 other parameters:\n",
        "  * `raw_outs`\n",
        "  * `decoded_loss`\n",
        "  * `fully_decoded`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdCuP7C0J5SX",
        "colab_type": "text"
      },
      "source": [
        "`raw_outs` will let you choose to apply your loss functions `activation` or not (default is `False`, so it always will). Let's see what that means from a code perspective!\n",
        "\n",
        "Here is our `CrossEntropyLossFlat`, the loss function for our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHjzAd3CJHs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossEntropyLossFlat(BaseLoss):\n",
        "    \"Same as `nn.CrossEntropyLoss`, but flattens input and target.\"\n",
        "    y_int = True\n",
        "    def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n",
        "    def decodes(self, x):    return x.argmax(dim=self.axis)\n",
        "    def activation(self, x): return F.softmax(x, dim=self.axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrc6MSDVKPsh",
        "colab_type": "text"
      },
      "source": [
        "As you can see, we have an `activation` that applies a `softmax`. Let's disable it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsyganA7KIeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.dls.device = 'cuda'\n",
        "learn.model.to('cuda')\n",
        "preds = learn.get_preds(raw_outs=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAM8nkmcKeZ6",
        "colab_type": "text"
      },
      "source": [
        "So now, if we look at the second item returned:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EukYYAW2KX1J",
        "colab_type": "code",
        "outputId": "b8757059-1d22-4cd7-9423-c49a57b5ddeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "preds[1][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.0002726 , -0.24976277, -1.058503  , -3.0604253 , -1.0200855 ,\n",
              "       -2.0843894 , -0.5362588 , -3.2754717 , -1.7710001 , -2.2774923 ,\n",
              "       -1.7333597 , -4.1619153 ,  0.9354493 ,  3.4019513 , 12.7702875 ,\n",
              "        7.132491  ,  2.0602374 , -3.1376252 ,  1.4149432 ,  5.9531183 ,\n",
              "        4.3040857 ,  1.0570182 , -2.5935738 , -1.620131  , -0.4416199 ,\n",
              "        2.527862  , -0.9089688 ,  0.51930547, -3.564454  , -3.3547723 ,\n",
              "        6.0538034 , -1.4624698 , -4.14351   , -1.7813267 , -0.72445834,\n",
              "        2.1540437 ,  0.07214043], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytX_y53Kl78",
        "colab_type": "text"
      },
      "source": [
        "We have the non-scaled results, versus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt7V6t9wKhNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = learn.get_preds(raw_outs=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMVQgWHDKq-f",
        "colab_type": "code",
        "outputId": "c7a5d67c-fb2c-43cf-864b-a6d19a4d6089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "preds[1][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.68456448e-06, 2.20158699e-06, 9.80627874e-07, 1.32458638e-07,\n",
              "       1.01903447e-06, 3.51534027e-07, 1.65314952e-06, 1.06828480e-07,\n",
              "       4.80917777e-07, 2.89803666e-07, 4.99364774e-07, 4.40260024e-08,\n",
              "       7.20222897e-06, 8.48506534e-05, 9.93737578e-01, 3.53840739e-03,\n",
              "       2.21797090e-05, 1.22617607e-07, 1.16334395e-05, 1.08795962e-03,\n",
              "       2.09144753e-04, 8.13323913e-06, 2.11266979e-07, 5.59232149e-07,\n",
              "       1.81724408e-06, 3.54032127e-05, 1.13879651e-06, 4.75048228e-06,\n",
              "       8.00172444e-08, 9.86841755e-08, 1.20320532e-03, 6.54732105e-07,\n",
              "       4.48438442e-08, 4.75977203e-07, 1.36954998e-06, 2.43610393e-05,\n",
              "       3.03764227e-06], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bINjyOf5KxWq",
        "colab_type": "text"
      },
      "source": [
        "So here, if you wanted to apply some sigmoid threshold, etc for labelling on inference you may not have done while during training (such as tell your model \"I do not know\"), you can do this directly with your outputs!\n",
        "\n",
        "Now what is in slot `[0]` I hear you ask?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlOY-r3VKv-D",
        "colab_type": "code",
        "outputId": "fbcba62a-bb3f-414e-bc8c-3ddf180bd9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds[0][:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "['basset_hound', 'american_bulldog', 'english_setter', 'samoyed', 'British_Shorthair']"
            ],
            "text/plain": [
              "['basset_hound',\n",
              " 'american_bulldog',\n",
              " 'english_setter',\n",
              " 'samoyed',\n",
              " 'British_Shorthair']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T16vgqHEK9uR",
        "colab_type": "text"
      },
      "source": [
        "It's our decoded classes! Right away! This is extremely helpful for people who want to align their outputs. This relies on your `dls.vocab`, so let's see what happens if we don't have a `vocab`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P0PF5wGK9Ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = learn.dls.vocab\n",
        "learn.dls.vocab = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jutt0IZsLHdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = learn.get_preds(raw_outs=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GN4kG3aLKnz",
        "colab_type": "code",
        "outputId": "06b3296e-4b5b-489a-8639-4ffe3868e97d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds[0][:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14, 12, 19, 31,  4], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMFlQUhXLXFu",
        "colab_type": "text"
      },
      "source": [
        "You can see instead it's a `argmax`'d tensor of our previous probabilties, run through the `decodes` of the loss function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l-SBUPmLhpp",
        "colab_type": "text"
      },
      "source": [
        "Alright, anything else new with `get_preds`? We'll move on to a tabular model for our `fully_decoded`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqj_6Kv7LNa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai2.tabular.all import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1d5kmv3LwCn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "98c58922-9d60-4cd5-81ad-00b6be33e8fb"
      },
      "source": [
        "path = untar_data(URLs.ADULT_SAMPLE)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7Jf8T8L0Gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(path/'adult.csv')\n",
        "splits = RandomSplitter()(range_of(df))\n",
        "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
        "cont_names = ['age', 'fnlwgt', 'education-num']\n",
        "procs = [Categorify, FillMissing, Normalize]\n",
        "y_names = 'salary'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKpUrGL8MMwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dls = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
        "                   y_names=y_names, splits=splits).dataloaders(bs=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oA34C8pMPr7",
        "colab_type": "code",
        "outputId": "ae7ffbd6-0a97-413f-db52-cce95800618d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "learn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\n",
        "learn.fit(1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.380894</td>\n",
              "      <td>0.452286</td>\n",
              "      <td>0.820639</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFjx5-_FMV59",
        "colab_type": "text"
      },
      "source": [
        "Now we can make a `test_dl` to work with our `get_preds` too, let's look at one for `fully_decoded`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmoKAXEeMVN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dl = learn.dls.test_dl(df.iloc[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMGxp-5PMc1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = learn.get_preds(dl=test_dl, fully_decoded=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56K_uc74MiPh",
        "colab_type": "text"
      },
      "source": [
        "So first we have our classes and probabilities again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnH0RGMgMhHP",
        "colab_type": "code",
        "outputId": "573012bf-ed05-4458-cca9-663f565d17b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "preds[0], preds[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['>=50k', '>=50k', '<50k'], array([[0.48675266, 0.51324725],\n",
              "        [0.41627738, 0.58372265],\n",
              "        [0.72597593, 0.27402407]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87AITKKSMmZv",
        "colab_type": "text"
      },
      "source": [
        "But now we have a third item:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7qWLMPmMhvG",
        "colab_type": "code",
        "outputId": "93b2ba1b-f9d7-4c85-dba8-2e717390f219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "preds[2].show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>workclass</th>\n",
              "      <th>education</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>education-num_na</th>\n",
              "      <th>age</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education-num</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Private</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>#na#</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>False</td>\n",
              "      <td>49.0</td>\n",
              "      <td>101319.997692</td>\n",
              "      <td>12.0</td>\n",
              "      <td>&gt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Private</td>\n",
              "      <td>Masters</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>False</td>\n",
              "      <td>44.0</td>\n",
              "      <td>236746.000628</td>\n",
              "      <td>14.0</td>\n",
              "      <td>&gt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Private</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>#na#</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>Black</td>\n",
              "      <td>True</td>\n",
              "      <td>38.0</td>\n",
              "      <td>96185.000326</td>\n",
              "      <td>10.0</td>\n",
              "      <td>&lt;50k</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n88K0FPLMqBW",
        "colab_type": "text"
      },
      "source": [
        "Which is our `DataFrame` with the inputs and our outputs! This also works for vision as well, however it's easier to show with tabular. Now let's move onto `.predict`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef0m7z-TMyMi",
        "colab_type": "text"
      },
      "source": [
        "### `.predict`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtYYZRYKM1US",
        "colab_type": "text"
      },
      "source": [
        "`predict` has some new bits too. Specifically, we can pass in `with_input` to possibly return our inputs, similar to what I showed above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFnHGTsZMoai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name, probs, row = learn.predict(df.iloc[0], with_input=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTTQzWKFNAnG",
        "colab_type": "code",
        "outputId": "7d167233-2b95-4b25-fbe9-0ed38104ffab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "name, probs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['>=50k'], array([[0.48675266, 0.51324725]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhPgg1YTNBPK",
        "colab_type": "code",
        "outputId": "de5ed8ca-67d5-490c-989b-3d5e7a04dda8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "row.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>workclass</th>\n",
              "      <th>education</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>education-num_na</th>\n",
              "      <th>age</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education-num</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Private</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>#na#</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>False</td>\n",
              "      <td>49.0</td>\n",
              "      <td>101319.997692</td>\n",
              "      <td>12.0</td>\n",
              "      <td>&gt;=50k</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzpdp4tpNDs3",
        "colab_type": "text"
      },
      "source": [
        "And this of course will also work for vision! Lastly, I want to cover the `ONNX` support, as it can be **directly integrated into `fastai`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP-HZl3GNbnJ",
        "colab_type": "text"
      },
      "source": [
        "### ONNX\n",
        "\n",
        "ONNX is a special module, where you can potentially speed up inference by relying on C++ rather than Python, but it's not easy to export from `fastai`, or at least it was! Let's import `fastinference.onnx` and look at our tabular model one more time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-j7s1C_NDL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastinference.onnx import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fE0hogfSCwn",
        "colab_type": "text"
      },
      "source": [
        "All we need to do is call `learn.to_onnx` and pass in a `fname` to export to **both** the ONNX format and export our model. We'll see why in a moment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDssp1m1NrQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.to_onnx('tabular')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx8JFvotSIWn",
        "colab_type": "text"
      },
      "source": [
        "That's it! Note however that some models may not be `ONNX` compatable (such as the UNET), and currently it supports only one output, multiple outputs will be supported soon. Now, what can we do from there?\n",
        "\n",
        "Let's load our model into a `fastONNX` model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEU-y5diSH-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tab_inf = fastONNX('tabular')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTPXp0weSe2R",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll try to time how the two different `predict` methods stack up. First, our improved version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrvYI0bFSZtS",
        "colab_type": "code",
        "outputId": "ea58ffd4-92a4-436b-9010-b7ff0f95fa7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "_ = learn.predict(df.iloc[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 37.9 ms, sys: 2.61 ms, total: 40.5 ms\n",
            "Wall time: 39 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iLn9PjgSoVh",
        "colab_type": "text"
      },
      "source": [
        "And now for our ONNX version. We'll want to pass in a *raw* batch of data to our model, so let's grab the first batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzuHvH-USx2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dl.bs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13mFLv1vSdCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(test_dl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrMROCZRSwno",
        "colab_type": "code",
        "outputId": "66a17895-c7b1-44c0-905f-b243fe0e7eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "batch"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[5, 8, 3, 0, 6, 5, 1]], device='cuda:0'),\n",
              " tensor([[ 0.7635, -0.8441,  0.7524]], device='cuda:0'),\n",
              " tensor([[1]], device='cuda:0', dtype=torch.int8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yN9dDSzS0ES",
        "colab_type": "text"
      },
      "source": [
        "And now let's `.predict`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGBhM22xSxVU",
        "colab_type": "code",
        "outputId": "257c802e-0dae-4359-9e79-2ec1e850b4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%timeit\n",
        "_ = tab_inf.predict(batch[:2])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 31.68 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 241 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFUj4AJKS6KS",
        "colab_type": "text"
      },
      "source": [
        "As you can see, *lighting* fast! And that's all that's needed! <s>Eventually I may convert this into a more `fastai`-like scenario, but for now this is the framework as it lays. </s>\n",
        "\n",
        "I also couldn't leave enough as enough. You have the full inference capability inside of this module. Let's build a `test_dl` again and run `get_preds` as an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biGuyOREYT7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dl = tab_inf.test_dl(df.iloc[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNYlCxvsYWRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0653a215-4b5b-447e-d627-5950a272a7b0"
      },
      "source": [
        "%%time\n",
        "preds = tab_inf.get_preds(dl=dl)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.34 ms, sys: 0 ns, total: 8.34 ms\n",
            "Wall time: 9.71 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57sM-oLdYavq",
        "colab_type": "text"
      },
      "source": [
        "Just to compare with our original:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrE6sIdWYeOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dl.device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAKqd-lAYZBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a313d9f3-f089-4f20-c988-9584abca5492"
      },
      "source": [
        "%%time\n",
        "preds = learn.get_preds(dl=dl)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.6 ms, sys: 0 ns, total: 10.6 ms\n",
            "Wall time: 11.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERlYBPeIYjzd",
        "colab_type": "text"
      },
      "source": [
        "We shaved off a few milliseconds too!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKFM8o6HYKQV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "In the next article I'll be showing how to utilize `SHAP`, and then the last will include `ClassConfusion`. Thanks for reading!"
      ]
    }
  ]
}
