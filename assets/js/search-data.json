{
  
    
        "post0": {
            "title": "HuggingFace Course Notes, Chapter 1 (And Zero), Part 1",
            "content": "Chapter 0 (Setup): . Since HF in of itself has no dependancy requirements, they recommend us installing transformers[dev] so it gets all the dev requirements for &quot;any imaginable use case&quot;. . A full list of what it installs is below: . deps = { &quot;Pillow&quot;: &quot;Pillow&quot;, &quot;black&quot;: &quot;black==21.4b0&quot;, &quot;cookiecutter&quot;: &quot;cookiecutter==1.7.2&quot;, &quot;dataclasses&quot;: &quot;dataclasses&quot;, &quot;datasets&quot;: &quot;datasets&quot;, &quot;deepspeed&quot;: &quot;deepspeed&gt;=0.4.0&quot;, &quot;docutils&quot;: &quot;docutils==0.16.0&quot;, &quot;fairscale&quot;: &quot;fairscale&gt;0.3&quot;, &quot;faiss-cpu&quot;: &quot;faiss-cpu&quot;, &quot;fastapi&quot;: &quot;fastapi&quot;, &quot;filelock&quot;: &quot;filelock&quot;, &quot;flake8&quot;: &quot;flake8&gt;=3.8.3&quot;, &quot;flax&quot;: &quot;flax&gt;=0.3.4&quot;, &quot;fugashi&quot;: &quot;fugashi&gt;=1.0&quot;, &quot;huggingface-hub&quot;: &quot;huggingface-hub==0.0.8&quot;, &quot;importlib_metadata&quot;: &quot;importlib_metadata&quot;, &quot;ipadic&quot;: &quot;ipadic&gt;=1.0.0,&lt;2.0&quot;, &quot;isort&quot;: &quot;isort&gt;=5.5.4&quot;, &quot;jax&quot;: &quot;jax&gt;=0.2.8&quot;, &quot;jaxlib&quot;: &quot;jaxlib&gt;=0.1.65&quot;, &quot;jieba&quot;: &quot;jieba&quot;, &quot;keras2onnx&quot;: &quot;keras2onnx&quot;, &quot;nltk&quot;: &quot;nltk&quot;, &quot;numpy&quot;: &quot;numpy&gt;=1.17&quot;, &quot;onnxconverter-common&quot;: &quot;onnxconverter-common&quot;, &quot;onnxruntime-tools&quot;: &quot;onnxruntime-tools&gt;=1.4.2&quot;, &quot;onnxruntime&quot;: &quot;onnxruntime&gt;=1.4.0&quot;, &quot;optuna&quot;: &quot;optuna&quot;, &quot;packaging&quot;: &quot;packaging&quot;, &quot;parameterized&quot;: &quot;parameterized&quot;, &quot;protobuf&quot;: &quot;protobuf&quot;, &quot;psutil&quot;: &quot;psutil&quot;, &quot;pydantic&quot;: &quot;pydantic&quot;, &quot;pytest&quot;: &quot;pytest&quot;, &quot;pytest-sugar&quot;: &quot;pytest-sugar&quot;, &quot;pytest-xdist&quot;: &quot;pytest-xdist&quot;, &quot;python&quot;: &quot;python&gt;=3.6.0&quot;, &quot;ray&quot;: &quot;ray&quot;, &quot;recommonmark&quot;: &quot;recommonmark&quot;, &quot;regex&quot;: &quot;regex!=2019.12.17&quot;, &quot;requests&quot;: &quot;requests&quot;, &quot;rouge-score&quot;: &quot;rouge-score&quot;, &quot;sacrebleu&quot;: &quot;sacrebleu&gt;=1.4.12&quot;, &quot;sacremoses&quot;: &quot;sacremoses&quot;, &quot;sagemaker&quot;: &quot;sagemaker&gt;=2.31.0&quot;, &quot;scikit-learn&quot;: &quot;scikit-learn&quot;, &quot;sentencepiece&quot;: &quot;sentencepiece==0.1.91&quot;, &quot;soundfile&quot;: &quot;soundfile&quot;, &quot;sphinx-copybutton&quot;: &quot;sphinx-copybutton&quot;, &quot;sphinx-markdown-tables&quot;: &quot;sphinx-markdown-tables&quot;, &quot;sphinx-rtd-theme&quot;: &quot;sphinx-rtd-theme==0.4.3&quot;, &quot;sphinx&quot;: &quot;sphinx==3.2.1&quot;, &quot;sphinxext-opengraph&quot;: &quot;sphinxext-opengraph==0.4.1&quot;, &quot;starlette&quot;: &quot;starlette&quot;, &quot;tensorflow-cpu&quot;: &quot;tensorflow-cpu&gt;=2.3&quot;, &quot;tensorflow&quot;: &quot;tensorflow&gt;=2.3&quot;, &quot;timeout-decorator&quot;: &quot;timeout-decorator&quot;, &quot;timm&quot;: &quot;timm&quot;, &quot;tokenizers&quot;: &quot;tokenizers&gt;=0.10.1,&lt;0.11&quot;, &quot;torch&quot;: &quot;torch&gt;=1.0&quot;, &quot;torchaudio&quot;: &quot;torchaudio&quot;, &quot;tqdm&quot;: &quot;tqdm&gt;=4.27&quot;, &quot;unidic&quot;: &quot;unidic&gt;=1.0.2&quot;, &quot;unidic_lite&quot;: &quot;unidic_lite&gt;=1.0.7&quot;, &quot;uvicorn&quot;: &quot;uvicorn&quot;, } . . Note: after exploring a bit I found their requirements are located here . !pip install transformers[dev] -U &gt;&gt; /dev/null # Ensure we upgrade and clean the output . ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you&#39;ll have folium 0.8.3 which is incompatible. ERROR: pytest-forked 1.3.0 has requirement pytest&gt;=3.10, but you&#39;ll have pytest 3.6.4 which is incompatible. ERROR: pytest-xdist 2.2.1 has requirement pytest&gt;=6.0.0, but you&#39;ll have pytest 3.6.4 which is incompatible. ERROR: black 21.4b0 has requirement regex&gt;=2020.1.8, but you&#39;ll have regex 2019.12.20 which is incompatible. . This should take a bit to run. I noticed four incompatibility errors in Colab, we&#39;ll see if it has any issues. . !pip show transformers . Name: transformers Version: 4.6.1 Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch Home-page: https://github.com/huggingface/transformers Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors Author-email: thomas@huggingface.co License: Apache Location: /usr/local/lib/python3.7/dist-packages Requires: tokenizers, packaging, importlib-metadata, numpy, filelock, tqdm, requests, huggingface-hub, sacremoses, regex Required-by: . Alright! We can move onto Chapter 1! ðŸ¤— . Chapter 1 . Introduction . Looks as though it&#39;s split into three main chunks: . Introduction | Diving in | Advanced | . Introduction will show a very surface level with Transformers models and HF Transformers, fine-tuning a basic model, and sharing models and tokenizers. . Diving in will go further into the HF datasets and tokenizers library, basic NLP tasks, and how to ask for help (presumably on the forums or on Twitter?) . Advanced looks to be covering specialized architecture, speeding up training, custom training loops (yay!) and contributing to HF itself. . Note: This is better taken after an intro course such as Practical Deep Learning for Coders or any course developed by deeplearning.ai. It also mentions that they don&#39;t expect any prior PyTorch or Tensorflow knowledge, but some familiarity will help. (fastai likely helps here too some) . The wonderful authors: . Matthew Carrigan - MLE @ HF | Lysandre Debut - MLE @ HF, worked with Transformers library from the very beginning | Sylvain Gugger - Research Engineer @ HF, core maintainer of Transformers. And one of our favorite former fastai folk | . What we will learn: . The pipeline function | The Transformer architecture | Encoder, decoder, and encoder/decoder architectures and when to use each | . Natural Language Processing . What is it? | . Classifying whole sentences or each word in a sentence, generating text content, question answering, and generating a new sentence from an input text . Why is it challenging? | . For a human, given &quot;I am hungry&quot; and &quot;I am sad&quot; we can know how similar thye are. That&#39;s hard for ML models. . Transformers, what can they do? . We get to look at pipeline now! . The Model Hub is a super valuable resource because it contains thousands of pretrained models for you to use, and you can upload your own. The language model zoo. . Working with Pipelines, with Sylvain . Offhand note, I like that the videos are broken up into ~4-5 minute chunks . General approach to how I will take these notes: . Watch video without notes | Read the website and take notes | Go back to the video and catch anything I missed | The pipeline is a very quick and powerful way to grab inference with any HF model. . Let&#39;s break down one example below they showed: . from transformers import pipeline classifier = pipeline(&quot;sentiment-analysis&quot;) classifier(&quot;I&#39;ve been waiting for a HuggingFace course all my life!&quot;) . . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9943008422851562}] . What did this do here? . Downloaded a model (judging by the download bar). Don&#39;t know which model yet is the default | I think we downloaded a pretrained tokenizer too? | Said model was the default for a sentiment-analysis task | We asked it to classify the sentiment in our sentence. Labels are positive and negative, and it gave us back an array of dictionaries with those values | We can also pass in multiple inputs/texts: . classifier([ &quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;, &quot;I hate this so much!&quot; ]) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9598048329353333}, {&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9994558095932007}] . The default model for this task is a pretrained model fine-uned for sentient analysis in english. Let&#39;s see if I can&#39;t find it . dir(classifier) . [&#39;__abstractmethods__&#39;, &#39;__call__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__slots__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_abc_impl&#39;, &#39;_forward&#39;, &#39;_parse_and_tokenize&#39;, &#39;binary_output&#39;, &#39;check_model_type&#39;, &#39;default_input_names&#39;, &#39;device&#39;, &#39;device_placement&#39;, &#39;ensure_tensor_on_device&#39;, &#39;feature_extractor&#39;, &#39;framework&#39;, &#39;model&#39;, &#39;modelcard&#39;, &#39;predict&#39;, &#39;return_all_scores&#39;, &#39;save_pretrained&#39;, &#39;task&#39;, &#39;tokenizer&#39;, &#39;transform&#39;] . classifier.framework . &#39;pt&#39; . type(classifier.model) . transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification . So it&#39;s a DistilBertForSequenceClassification, likely using the default which would be en-sentiment . Current available pipeline classes: . feature-extraction (vector representation of a text) | fill-mask | ner (Named-Entity-Recognition) | question-answering | sentiment-analysis | text-generation | translation | zero-shot-classification | . Zero-Shot Classification . Classifying unlabelled tasks. | . zero-shot-classification pipeline let&#39;s us specify which labels to use for classification, even if they may differ from the pretrained models. . Side Note:I&#39;m going to write a quick namespace class via mk_class in fastcore to hold all of these tasks, so I can get tab-completion . pip install fastcore &gt;&gt; /dev/null . from fastcore.basics import mk_class . cls_dict = {&#39;FeatureExtraction&#39;:&#39;feature-extraction&#39;, &#39;FillMask&#39;:&#39;fill-mask&#39;, &#39;NER&#39;:&#39;ner&#39;, &#39;QuestionAnswering&#39;:&#39;question-answering&#39;, &#39;SentimentAnalysis&#39;:&#39;sentiment-analysis&#39;, &#39;Summarization&#39;:&#39;summarization&#39;, &#39;TextGeneration&#39;:&#39;text-generation&#39;, &#39;Translation&#39;:&#39;translation&#39;, &#39;ZeroShotClassification&#39;:&#39;zero-shot-classification&#39; } mk_class(&#39;Task&#39;, **cls_dict) . Task.FeatureExtraction . &#39;feature-extraction&#39; . As you can see all I&#39;ve done is load a fancy namespace-like object from fastcore that holds my dictionary values as attributes instead. . Back to the HF stuff. Let&#39;s load in a pipeline: . classifier = pipeline(Task.ZeroShotClassification) . Seems this model took quite a bit longer to download, but our Task object is working great! . classifier( &quot;This is a course about the Transformers library&quot;, candidate_labels=[&#39;education&#39;,&#39;politics&#39;,&#39;business&#39;] ) . {&#39;labels&#39;: [&#39;education&#39;, &#39;business&#39;, &#39;politics&#39;], &#39;scores&#39;: [0.844597339630127, 0.11197540909051895, 0.043427303433418274], &#39;sequence&#39;: &#39;This is a course about the Transformers library&#39;} . Very interesting, so we can see right away it could tell this was educational! (Or fit the closest to that label.) I wonder how it works under the hood, something I may peruse later. . Text Generation . Generate some fancy text given a prompt. . Similar to predictive text feature on my iPhone. . Has some randomness, so we won&#39;t 100% get the smae thing each time . generator = pipeline(Task.TextGeneration) . . generator(&quot;In this course we will teach you how to&quot;) . Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. . [{&#39;generated_text&#39;: &#39;In this course we will teach you how to create a custom project with custom options. n nBefore you begin, you will also want to check how to setup the build system. It is available in all versions, at all times we recommend using the&#39;}] . Theres a few args we can control and pass to it, such as num_return_sequences and max_length. . The homework is to try and generate two sentences of 15 words each. Let&#39;s try that: . generator( &quot;In Marine Biology,&quot;, num_return_sequences=2, max_length=15 ) . Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. . [{&#39;generated_text&#39;: &#39;In Marine Biology, the study was published in Physical Review Letters. n n&#39;}, {&#39;generated_text&#39;: &#34;In Marine Biology, a graduate of California&#39;s Stanford University, said the discovery&#34;}] . Cool! Easy to use . A headache I ran into is it&#39;s num_return_sequences, not num_returned_sequences. . Use any model from the Hub in a pipeline . I love the HuggingFace hub, so very happy to see this in here . Models can be found on the ModelHub. In this example we use distilgpt2 . generator = pipeline(Task.TextGeneration, model=&#39;distilgpt2&#39;) generator( &quot;In this course, we will teach you how to&quot;, max_length=30, num_return_sequences=2 ) . . Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. . [{&#39;generated_text&#39;: &#39;In this course, we will teach you how to integrate a Python function into your application and use a Python object. In addition, we will be teaching&#39;}, {&#39;generated_text&#39;: &#39;In this course, we will teach you how to set up your own private cloud. Our class begins with our server and takes you through a variety of&#39;}] . Mask Filling . Fill in the blanks of a given text . unmasker = pipeline(Task.FillMask) . . unmasker(&#39;This course will teach you all about &lt;mask&gt; models.&#39;, top_k=2) . [{&#39;score&#39;: 0.19619838893413544, &#39;sequence&#39;: &#39;This course will teach you all about mathematical models.&#39;, &#39;token&#39;: 30412, &#39;token_str&#39;: &#39; mathematical&#39;}, {&#39;score&#39;: 0.040527306497097015, &#39;sequence&#39;: &#39;This course will teach you all about computational models.&#39;, &#39;token&#39;: 38163, &#39;token_str&#39;: &#39; computational&#39;}] . So here it thought the best word to fill that with was mathematical, followed by computational (and showed the filled in sentence) . top_k is how many possibilities are displayed . Note: Model fills &lt;mask&gt;, and different models will have different things it will try and fill that with. One way to check this is by looking at the mask word used in the widget (on HF ModelHub) . Named Entity Recognition (NER) . Find parts of an input text that correspond to entities such as persons, locations, or organizations. . ner = pipeline(Task.NER, grouped_entities=True) . ner(&quot;My name is Zach Mueller and I go to school in Pensacola&quot;) . [{&#39;end&#39;: 23, &#39;entity_group&#39;: &#39;PER&#39;, &#39;score&#39;: 0.9987525741259257, &#39;start&#39;: 11, &#39;word&#39;: &#39;Zach Mueller&#39;}, {&#39;end&#39;: 55, &#39;entity_group&#39;: &#39;LOC&#39;, &#39;score&#39;: 0.9819004138310751, &#39;start&#39;: 46, &#39;word&#39;: &#39;Pensacola&#39;}] . What does having it not grouped do? . ner = pipeline(Task.NER, grouped_entities=False) ner(&quot;My name is Zach Mueller and I go to school in Pensacola&quot;) . [{&#39;end&#39;: 15, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 4, &#39;score&#39;: 0.9993382692337036, &#39;start&#39;: 11, &#39;word&#39;: &#39;Zach&#39;}, {&#39;end&#39;: 18, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 5, &#39;score&#39;: 0.999767005443573, &#39;start&#39;: 16, &#39;word&#39;: &#39;Mu&#39;}, {&#39;end&#39;: 23, &#39;entity&#39;: &#39;I-PER&#39;, &#39;index&#39;: 6, &#39;score&#39;: 0.9971524477005005, &#39;start&#39;: 18, &#39;word&#39;: &#39;##eller&#39;}, {&#39;end&#39;: 49, &#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 13, &#39;score&#39;: 0.9945659637451172, &#39;start&#39;: 46, &#39;word&#39;: &#39;Pen&#39;}, {&#39;end&#39;: 51, &#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 14, &#39;score&#39;: 0.956141471862793, &#39;start&#39;: 49, &#39;word&#39;: &#39;##sa&#39;}, {&#39;end&#39;: 55, &#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 15, &#39;score&#39;: 0.9949938058853149, &#39;start&#39;: 51, &#39;word&#39;: &#39;##cola&#39;}] . So we can see that the first grouped &quot;Zach&quot; and &quot;Mueller&quot; together as a single item, and Pen, Sa, Cola together too (likely split with the subword tokenizer). Having grouped=True sounds like a good default in this case . Most models that you want to have aligned with this task have some form of POS abbriviation in the name or tag . Question Answering (QA) . This is very straightforward, query a question and then receive an answer given some context. . qa = pipeline(Task.QuestionAnswering) qa( question=&quot;Where do I work?&quot;, context=&quot;My name is Zach Mueller and I go to school in Pensacola&quot; ) . {&#39;answer&#39;: &#39;Pensacola&#39;, &#39;end&#39;: 55, &#39;score&#39;: 0.9609196186065674, &#39;start&#39;: 46} . . Note: this is an extraction method, not text generation. So it just extracted Pensacola from the question. . Summarization . Reduce a text to a shorter one, while keeping most of the important aspects referenced in the text . summarizer = pipeline(Task.Summarization) . . summarizer(&quot;&quot;&quot; America has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities engineering curricula now concentrate on and encourage largely the study of engineering science. As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. While the latter is important, it should not be at the expense of more traditional engineering. Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers. &quot;&quot;&quot;) . [{&#39;summary_text&#39;: &#39; America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .&#39;}] . Translation . The last task in the tutorial/lesson is machine translation. Usually the model name will have some lang1_to_lang2 naming convention in the title. The easiest way to pick one is to search on the model hub. In this example we&#39;ll translate French to english (let&#39;s see how much I remember from my French classes in high school!) . translator = pipeline(Task.Translation, model=&#39;Helsinki-NLP/opus-mt-fr-en&#39;) . . translator(&quot;Je m&#39;apelle Zach, comment-vous est appelez-vous?&quot;) . [{&#39;translation_text&#39;: &#34;I&#39;m Zach, what&#39;s your name?&#34;}] . We can also specify a max_lenght or min_length for the generated result . In the next chapter, we&#39;ll learn what is inside a pipeline and customizing its behavior .",
            "url": "https://muellerzr.github.io/fastblog/huggingface/2021/06/14/HuggingFaceLesson1.html",
            "relUrl": "/huggingface/2021/06/14/HuggingFaceLesson1.html",
            "date": " â€¢ Jun 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch to fastai, Bridging the Gap",
            "content": ". This article is also a Jupyter Notebook available to be run from the top down. There will be code snippets that you can then run in any environment. . Below are the versions of fastai, fastcore, torch, and torchvision currently running at the time of writing this: . fastai: 2.2.5 | fastcore: 1.3.19 | torch: 1.7.0+cu101 | torchvision: 0.8.1+cu101 | . . Addressing the Elephant in the Room . I recently posted a tweet asking about what people struggle with the most in fastai, and the resounding answer was how to integrate minimally with Pytorch. An impression seems to have been made that to use fastai you must use the complete fastai API only, and nothing else. . Let&#39;s clear up that misconception now: . Important: fastai at its core is a training loop, designed to be framework agnostic. You can use any flavor of Pytorch you want, and only use fastai to quickly and effictively train a model with state-of-the-art practices . The Plan . Now that the misconceptions have been addressed, let&#39;s walk through just how that is going to happen. We&#39;re going to follow the official Pytorch CIFAR10 tutorial and show what needs to minimally happen in the fastai framework to take full advantage of the Learner. This will include: . The Dataset | The DataLoaders | The model | The optimizer | . The Dataset and DataLoaders . Following from the tutorial, we&#39;re going to load in the dataset using only torchvision. First we&#39;ll grab our imports: . import torch import torchvision import torchvision.transforms as transforms . Next we&#39;re going to definine some minimal transforms: . transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]) . Before downloading our train and test sets: . Note: I&#8217;m using naming conventions similar to how fastai names things, so you can see how these can relate to each other . dset_train = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) . Files already downloaded and verified . dset_test = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) . Files already downloaded and verified . Next we&#39;ll make our Dataloaders: . trainloader = torch.utils.data.DataLoader(dset_train, batch_size=4, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(dset_test, batch_size=4, shuffle=False, num_workers=2) . And that&#39;s as far as we&#39;ll go from there for now, let&#39;s move onto the model next . The Model . We&#39;ll bring in the architecture from the tutorial and use it here: . import torch.nn as nn import torch.nn.functional as F . class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x . And finally we&#39;ll make an instance of it: . net = Net() . Loss Function and Optimizer . Next we&#39;ll bring in their loss function and optimizer. . The loss function is simple enough: . criterion = nn.CrossEntropyLoss() . However the optimizer requires a little bit of fastai magic, specifically in the form of an OptimWrapper. Our optimizer function should be defined as below: . from fastai.optimizer import OptimWrapper from torch import optim . def opt_func(params, **kwargs): return OptimWrapper(optim.SGD(params, lr=0.001)) . Training . Now we have everything needed to train a model, so now let&#39;s bring in fastai&#39;s training loop, also known as the Learner. . fastai&#39;s Learner expects DataLoaders to be used, rather than simply one DataLoader, so let&#39;s make that: . Note: fastai also expects a validation DataLoader to be present, so we&#8217;ll be tying the testloader in here . from fastai.data.core import DataLoaders . dls = DataLoaders(trainloader, testloader) . Finally we&#39;re going to wrap it all up in a Learner. As mentioned before, the Learner is the glue that merges everything together and enables users to utilize Leslie Smith&#39;s One-Cycle Policy, the learning rate finder, and other fastai training goodies. . Let&#39;s make it by passing in our dls, the model, the optimizer, and the loss function: . from fastai.learner import Learner . To get fastai&#39;s fancy-looking progress bar, we need to import the ProgressCallback: . from fastai.callback.progress import ProgressCallback . We also need to pass in the CudaCallback so our batches can be pushed to the GPU (fastai&#39;s DataLoaders can do this automatically) . from fastai.callback.data import CudaCallback . learn = Learner(dls, net, loss_func=criterion, opt_func=opt_func, cbs=[CudaCallback]) . Finally, let&#39;s do some minimal training. . Now we have everything needed to do a basic fit: . Note: Since we already passed in a learning rate to Learner we don&#8217;t need to pass one in here . learn.fit(2) . epoch train_loss valid_loss time . 0 | 2.265952 | 2.263797 | 01:09 | . 1 | 1.867355 | 1.866925 | 01:09 | . What&#39;s Next? . Great, so now we&#39;ve trained our model, but what do we do with it? How do I get it out? . Your model lives in learn.model, and we&#39;ve already seen that we passed in a regular Pytorch model earlier. Since we&#39;re using fastai&#39;s base Learner class, the model itself was untouched. As a result, it&#39;s still a regular Pytorch model we can save away: . torch.save(learn.model.state_dict(), &#39;./cifar_net.pth&#39;) . And that&#39;s really it! As you can see, the minimalist you can absolutely get with using the fastai framework is: . Pytorch DataLoader | Pytorch model | fastai Learner | fastai Optimizer | . Closing Remarks . I hope this has enlightned you on just how flexible the fastai framework can truly be for your training needs with the idealistic goal of simply getting a model out there. . As we&#39;ve removed most of the fastai magic, from here on out you should be utilizing standard Pytorch, as fastai specific functions like test_dl and predict will no longer be able to be used, as you didn&#39;t use a fastai DataLoader. . Thank you for reading! .",
            "url": "https://muellerzr.github.io/fastblog/2021/02/14/Pytorchtofastai.html",
            "relUrl": "/2021/02/14/Pytorchtofastai.html",
            "date": " â€¢ Feb 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Generating Comparitive Baselines for CAMVID with fastai's Dynamic Unet",
            "content": "!pip install git+https://github.com/fastai/fastai !pip install git+https://github.com/fastai/fastcore . Collecting git+https://github.com/fastai/fastai Cloning https://github.com/fastai/fastai to /tmp/pip-req-build-7c5crln7 Running command git clone -q https://github.com/fastai/fastai /tmp/pip-req-build-7c5crln7 Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (19.3.1) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (20.4) Collecting fastcore&gt;=1.0.5 Downloading https://files.pythonhosted.org/packages/ce/03/04eb54f2d482e06375cbbd06fb9d71670a5607739ecfa18a4bd25bfbd9fa/fastcore-1.0.15-py3-none-any.whl (40kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40kB 3.7MB/s Requirement already satisfied: torchvision&gt;=0.7 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (0.7.0+cu101) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (3.2.2) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (1.0.5) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (2.23.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (3.13) Requirement already satisfied: fastprogress&gt;=0.2.4 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (1.0.0) Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (7.0.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (0.22.2.post1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (1.4.1) Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (2.2.4) Requirement already satisfied: torch&gt;=1.6.0 in /usr/local/lib/python3.6/dist-packages (from fastai==2.0.14) (1.6.0+cu101) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;fastai==2.0.14) (2.4.7) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;fastai==2.0.14) (1.15.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision&gt;=0.7-&gt;fastai==2.0.14) (1.18.5) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai==2.0.14) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai==2.0.14) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai==2.0.14) (0.10.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;fastai==2.0.14) (2018.9) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==2.0.14) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==2.0.14) (2020.6.20) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==2.0.14) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==2.0.14) (1.24.3) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;fastai==2.0.14) (0.16.0) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (1.0.0) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (2.0.3) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (1.0.2) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (1.1.3) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (1.0.2) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (0.4.1) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (7.4.0) Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (4.41.1) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (3.0.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (50.3.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy-&gt;fastai==2.0.14) (0.8.0) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.6.0-&gt;fastai==2.0.14) (0.16.0) Requirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai==2.0.14) (1.7.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy-&gt;fastai==2.0.14) (3.1.0) Building wheels for collected packages: fastai Building wheel for fastai (setup.py) ... done Created wheel for fastai: filename=fastai-2.0.14-cp36-none-any.whl size=185359 sha256=6b1f931451e060025a72936ebbaaa6a2d99dbb58e2d9b84e842c55f6a1721a57 Stored in directory: /tmp/pip-ephem-wheel-cache-15upts8i/wheels/83/30/a0/6fa8a74c9f5a5ab45cdc84e9f9ed56d8a72750e11ebf50a364 Successfully built fastai Installing collected packages: fastcore, fastai Found existing installation: fastai 1.0.61 Uninstalling fastai-1.0.61: Successfully uninstalled fastai-1.0.61 Successfully installed fastai-2.0.14 fastcore-1.0.15 Collecting git+https://github.com/fastai/fastcore Cloning https://github.com/fastai/fastcore to /tmp/pip-req-build-7fykshnj Running command git clone -q https://github.com/fastai/fastcore /tmp/pip-req-build-7fykshnj Requirement already satisfied (use --upgrade to upgrade): fastcore==1.0.15 from git+https://github.com/fastai/fastcore in /usr/local/lib/python3.6/dist-packages Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastcore==1.0.15) (19.3.1) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastcore==1.0.15) (20.4) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;fastcore==1.0.15) (2.4.7) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;fastcore==1.0.15) (1.15.0) Building wheels for collected packages: fastcore Building wheel for fastcore (setup.py) ... done Created wheel for fastcore: filename=fastcore-1.0.15-cp36-none-any.whl size=40887 sha256=8feb4aa9c227a90f158bed73ab4bc9a2100dcf7a55df689e1fe6d5ed3937e12b Stored in directory: /tmp/pip-ephem-wheel-cache-83grb3ie/wheels/8a/2a/23/bc50c8f5e28776b44ac837a01fcfa675724565d4813d8e51c7 Successfully built fastcore . . This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai and fastcore I am currently running at the time of writing this: . fastai: 2.0.13 . | fastcore: 1.0.13 . | . . Note: this blog is the result of joint efforts between myself and Juvian on the forums . CAMVID Benchmarks, Can&#39;t We Just Use the Code from Class? . In the fastai course, we are walked through the CAMVID dataset, semantic segmentation with a car&#39;s point of view. Ideally, we would then like to compare our results to the current state-of-the-art benchmarks. . However! In it&#39;s current state, this cannot be done. . Why you might ask? Recently the benchmarks have been adapting a few &quot;weird&quot; changes, as well as making the dataset slightly easier, although comparing against them is not as straightforward as you would hope either (more on this at the end) . The Metric: . First, the reported metrics are different. Instead of accuracy the Mean Intersection Over Union (mIOU) is reported, as well as individual IOU&#39;s per class . The Number of Classes: . In the original fastai version of the dataset, 31 classes are present, with an additional void classes that is ignored in the resulting benchmarks. . Researchers have since changed this class distribution to 11 total classes: Building, Tree, Sky, Car, Sign, Road, Pedestrian, Fence, Pole, Sidewalk, and Cyclist, with one more twelveth void class that is again, not taken into account. . This change in classes allows for a higher mIOU being reported without having the rarely-seen classes scew the results, so if you were running mIOU on the class notebooks and getting ~20% and being confused why it doesn&#39;t align, this is why! . The Splits . When we train with fastai, we wind up mixing in the baseline evaluation dataset with the training data! Not something we want at all! The train/validation/test split in most papers tends to be: 367/101/233. That is correct, there is two-times as many test images as there are validation. . The SegNet Version . This is a version that has images and labels coming in at a size of 360x480 pixels, which is half the size of what fastai&#39;s source dataset is, but has its labels with the 11 classes. What is different paper to paper however is how they use the dataset, which can lead to issues. Let&#39;s look at the current options and their pros/cons: . Using the SegNet Dataset: . If we decide to use only this dataset, there is not much room for fastai&#39;s tricks (such as progressive resizing and Pre-Sizing). That being said, there are papers which use this. If you look on the CAMVID leaderboard however, you&#39;ll notice the best model placed at 8th. So what&#39;s next? . Well, what is the SOTA we&#39;re comparing against then? . While below is a benchmark, we can&#39;t truly compare against it. However, if we wish to, we will be focusing on the models that have an ImageNet backbone: . . Using the fastai Images with Smaller Labels . fastai uses the high-quality 720x960 images and labels, so it would make logical sense to train on them and use these smaller masks as the labels, which is being done on all the upper benchmarks. . The Issue . There is a very big issue with this though, which Jeremy pointed out to us while we were discussing these new benchmark approaches. Simply upscaling the labels, without any adjustments to the fastai images, on its own sounds &quot;weird.&quot; Instead, what we do is resize the images back down to the 360x480 size before then upsampling them. This winds up increasing the final accuracy . Can We Train Now? . Okay, enough talking, can we see some code to back up your claims? . Sure, let&#39;s do it! To visualize what we will be doing, throughout this blog we will be: . Downloading a different dataset | Making a DataBlock which pre-sizes our images to the proper size | Making a unet_learner which: Uses a pretrained ResNet34 backbone | Uses the ranger optimizer function | Compare the use of ReLU and Mish in the head | Uses both IOU and mIOU metrics to properly allow us to benchmark the results | . | Make a test_dl with the proper test set to evaluate with. | Downloading the Dataset . The dataset currently lives in a the repository, so we will go ahead and clone it and make it our working directory: . !git clone https://github.com/alexgkendall/SegNet-Tutorial.git %cd SegNet-Tutorial/ . Cloning into &#39;SegNet-Tutorial&#39;... remote: Enumerating objects: 2785, done. remote: Total 2785 (delta 0), reused 0 (delta 0), pack-reused 2785 Receiving objects: 100% (2785/2785), 340.84 MiB | 26.65 MiB/s, done. Resolving deltas: 100% (81/81), done. /content/SegNet-Tutorial . Now we still want to use fastai&#39;s input images, so we&#39;ll go ahead and pull their CAMVID dataset. First let&#39;s import fastai&#39;s vision module: . from fastai.vision.all import * . Then grab the data: . path_i = untar_data(URLs.CAMVID) . Let&#39;s see how both datasets are formatted: . path_l = Path(&#39;&#39;) . path_i.ls() . (#4) [Path(&#39;/root/.fastai/data/camvid/codes.txt&#39;),Path(&#39;/root/.fastai/data/camvid/images&#39;),Path(&#39;/root/.fastai/data/camvid/labels&#39;),Path(&#39;/root/.fastai/data/camvid/valid.txt&#39;)] . path_l.ls() . (#9) [Path(&#39;.gitattributes&#39;),Path(&#39;Models&#39;),Path(&#39;Scripts&#39;),Path(&#39;README.md&#39;),Path(&#39;docker&#39;),Path(&#39;CamVid&#39;),Path(&#39;.gitignore&#39;),Path(&#39;Example_Models&#39;),Path(&#39;.git&#39;)] . So we can see that fastai has the usual images and labels folder, while we can&#39;t quite tell where the annotations are in our second one. Let&#39;s narrow down to the CamVid folder: . path_l = path_l/&#39;CamVid&#39; . path_l.ls() . (#9) [Path(&#39;CamVid/train.txt&#39;),Path(&#39;CamVid/train&#39;),Path(&#39;CamVid/test.txt&#39;),Path(&#39;CamVid/val.txt&#39;),Path(&#39;CamVid/testannot&#39;),Path(&#39;CamVid/test&#39;),Path(&#39;CamVid/trainannot&#39;),Path(&#39;CamVid/valannot&#39;),Path(&#39;CamVid/val&#39;)] . And we can see a better looking dataset! The three folders we will be caring about are trainannot, valannot and testannot, as these are where the labels live. . DataBlock . As we saw how the data was split up, fastai currently doesn&#39;t have something to work along those lines, the closest is GrandparentSplitter. We&#39;ll write something similar called FolderSplitter, which can accept names for the train and validation folders: . def _folder_idxs(items, name): def _inner(items, name): return mask2idxs(Path(o).parents[0].name == name for o in items) return [i for n in L(name) for i in _inner(items, n)] def FolderSplitter(train_name=&#39;train&#39;, valid_name=&#39;valid&#39;): &quot;Split `items` from parent folder names `parent_idx` levels above the item&quot; def _inner(o): return _folder_idxs(o, train_name),_folder_idxs(o, valid_name) return _inner . Next we will need a way to get our x images, since they live differently than our labels. We can use a custom function to do so: . def get_x(o): return path_i/&#39;images&#39;/o.name . Finally we need a get_y that will use that same filename to go grab our working masks: . def get_mask(o): return o.parent.parent/(o.parent.name + &#39;annot&#39;)/o.name . We have almost all the pieces to making our dataset now. We&#39;ll use fastai&#39;s progressive resizing when training, and pass in a set of codes for our dataset: . codes = [&#39;Sky&#39;, &#39;Building&#39;, &#39;Pole&#39;, &#39;Road&#39;, &#39;Pavement&#39;, &#39;Tree&#39;, &#39;SignSymbol&#39;, &#39;Fence&#39;, &#39;Car&#39;, &#39;Pedestrian&#39;, &#39;Bicyclist&#39;, &#39;Unlabelled&#39;] half, full = (360, 480), (720, 960) . Now for those transforms. I mentioned earlier we will be downscaling and then upscaling the images, this way the same upscaling is applied to our labels and our images, though the images start from a higher quality. Since we want to train small, we&#39;ll resize it back down in the batch transforms as well as normalize our inputs: . item_tfms = [Resize(half), Resize(full)] batch_tfms = [*aug_transforms(size=half), Normalize.from_stats(*imagenet_stats)] . And with this we can now build the DataBlock and DataLoaders: . camvid = DataBlock(blocks=(ImageBlock, MaskBlock(codes=codes)), get_items=get_image_files, splitter=FolderSplitter(valid_name=&#39;val&#39;), get_x=get_x, get_y=get_mask, item_tfms=item_tfms, batch_tfms=batch_tfms) . We&#39;ll call the .summary() to make sure our images and masks do crop to the half size: . camvid.summary(path_s) . We can see the final input and mask size is (360,480), which is what we want! Let&#39;s go ahead and make them DataLoaders: . dls = camvid.dataloaders(path_l, bs=4) . Since we have a void column, our c attribute in the DataLoaders needs to be one less: . dls.c = len(codes) - 1 . Metrics . For the next part Juvian was the one to bring this to life! We want class-wise IOU as well as mIOU, which are defined below: . class IOU(AvgMetric): &quot;Intersection over Union Metric&quot; def __init__(self, class_index, class_label, axis, ignore_index=-1): store_attr(&#39;axis,class_index,class_label,ignore_index&#39;) def accumulate(self, learn): pred, targ = learn.pred.argmax(dim=self.axis), learn.y intersec = ((pred == targ) &amp; (targ == self.class_index)).sum().item() union = (((pred == self.class_index) | (targ == self.class_index)) &amp; (targ != self.ignore_index)).sum().item() if union: self.total += intersec self.count += union @property def name(self): return self.class_label . from sklearn.metrics import confusion_matrix class MIOU(AvgMetric): &quot;Mean Intersection over Union Metric&quot; def __init__(self, classes, axis): store_attr() def accumulate(self, learn): pred, targ = learn.pred.argmax(dim=self.axis).cpu(), learn.y.cpu() pred, targ = pred.flatten().numpy(), targ.flatten().numpy() self.total += confusion_matrix(targ, pred, range(0, self.classes)) @property def value(self): conf_matrix = self.total per_class_TP = np.diagonal(conf_matrix).astype(float) per_class_FP = conf_matrix.sum(axis=0) - per_class_TP per_class_FN = conf_matrix.sum(axis=1) - per_class_TP iou_index = per_class_TP / (per_class_TP + per_class_FP + per_class_FN) iou_index = np.nan_to_num(iou_index) mean_iou_index = (np.mean(iou_index)) return mean_iou_index @property def name(self): return &#39;miou&#39; . With our metric functions defined, let&#39;s combine them all. We&#39;ll want a mIOU, as well as 11 IOU for each class: . metrics = [MIOU(11, axis=1)] . . Note: we do not need to pass in an ignore_index here, as any values larger than 10 get ignored . And now let&#39;s declare our IOU&#39;s. Since there&#39;s so many we&#39;ll just make a function instead that relies on our codes: . for x in range(11): metrics.append(IOU(x, codes[x], axis=1, ignore_index=11)) . With this we can finally move over to our model and training: . The Model and Training . For the model we will use a pretrained ResNet34 backbone architecture that has Mish on the head of the Dynamic Unet: . config = unet_config(self_attention=False, act_cls=Mish) . Our optimizer will be ranger: . opt_func = ranger . And finally, since we have an ignore_index we need to pass this into our loss function as well, otherwise we will trigger a CUDA error: device-side assert triggered . loss_func = CrossEntropyLossFlat(ignore_index=11, axis=1) . Now let&#39;s pass this all into unet_learner: . learn = unet_learner(dls, resnet34, metrics=metrics, opt_func=opt_func, loss_func=loss_func, config=config) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . Phase 1 . We&#39;ll find a good learning rate, fit for ten epochs frozen with GradientAccumulation to help with stability before unfreezing and training for a few more: . learn.lr_find() . SuggestedLRs(lr_min=0.0007585775572806596, lr_steep=0.0010000000474974513) . A good learning rate is around 2e-3, so we&#39;ll train with that using fit_flat_cos as the ranger optimizer should be paired with it: . lr = 2e-3 learn.fit_flat_cos(10, slice(lr), cbs=[GradientAccumulation(n_acc=16)]) . epoch train_loss valid_loss miou Sky Building Pole Road Pavement Tree SignSymbol Fence Car Pedestrian Bicyclist time . 0 | 1.380507 | 0.860166 | 0.269197 | 0.839513 | 0.644756 | 0.000020 | 0.730014 | 0.010095 | 0.692669 | 0.000000 | 0.000000 | 0.044094 | 0.000000 | 0.000000 | 00:38 | . 1 | 0.697614 | 0.569771 | 0.381895 | 0.889391 | 0.811681 | 0.000020 | 0.823444 | 0.581527 | 0.781517 | 0.000000 | 0.000621 | 0.312639 | 0.000000 | 0.000000 | 00:36 | . 2 | 0.467255 | 0.442739 | 0.403631 | 0.878348 | 0.775515 | 0.000122 | 0.910412 | 0.655963 | 0.808596 | 0.001485 | 0.005501 | 0.395828 | 0.004051 | 0.004118 | 00:36 | . 3 | 0.359350 | 0.333798 | 0.473028 | 0.943389 | 0.848014 | 0.000081 | 0.945536 | 0.777093 | 0.788921 | 0.008712 | 0.007907 | 0.553949 | 0.206558 | 0.123142 | 00:35 | . 4 | 0.303314 | 0.260455 | 0.576267 | 0.941747 | 0.850498 | 0.000243 | 0.952665 | 0.813468 | 0.889507 | 0.100332 | 0.357541 | 0.781372 | 0.222650 | 0.428913 | 00:35 | . 5 | 0.273420 | 0.270565 | 0.585087 | 0.932999 | 0.864418 | 0.000081 | 0.949140 | 0.818339 | 0.869284 | 0.293988 | 0.269792 | 0.811271 | 0.216093 | 0.410552 | 00:35 | . 6 | 0.254735 | 0.236601 | 0.634471 | 0.936278 | 0.870673 | 0.003281 | 0.964194 | 0.855702 | 0.889725 | 0.300149 | 0.482891 | 0.841558 | 0.286659 | 0.548068 | 00:35 | . 7 | 0.240327 | 0.270188 | 0.596441 | 0.937764 | 0.871333 | 0.032959 | 0.955953 | 0.837018 | 0.822873 | 0.273233 | 0.048920 | 0.846606 | 0.319242 | 0.614951 | 00:35 | . 8 | 0.209995 | 0.195687 | 0.673547 | 0.946722 | 0.886908 | 0.060189 | 0.963652 | 0.848611 | 0.892291 | 0.436009 | 0.496328 | 0.864719 | 0.363641 | 0.649950 | 00:35 | . 9 | 0.185206 | 0.197379 | 0.667369 | 0.946318 | 0.889515 | 0.076916 | 0.966370 | 0.858471 | 0.891145 | 0.420854 | 0.474111 | 0.864551 | 0.342494 | 0.610310 | 00:35 | . Next we&#39;ll unfreeze and train for 12 more epochs. When training we will adjust the learning rate and apply the EarlyStoppingCallback to help prevent overfitting: . lrs = slice(lr/400, lr/4) learn.unfreeze() learn.fit_flat_cos(12, lrs, cbs=[GradientAccumulation(n_acc=16)]) . epoch train_loss valid_loss miou Sky Building Pole Road Pavement Tree SignSymbol Fence Car Pedestrian Bicyclist time . 0 | 0.182300 | 0.191817 | 0.679859 | 0.944626 | 0.883056 | 0.093460 | 0.968520 | 0.857086 | 0.903970 | 0.411324 | 0.541308 | 0.858053 | 0.379852 | 0.637194 | 00:35 | . 1 | 0.171663 | 0.193553 | 0.688322 | 0.941426 | 0.889005 | 0.109457 | 0.966605 | 0.853181 | 0.892943 | 0.453734 | 0.501025 | 0.844362 | 0.418895 | 0.700908 | 00:35 | . 2 | 0.164871 | 0.225098 | 0.661339 | 0.944716 | 0.888159 | 0.106461 | 0.966710 | 0.855766 | 0.872609 | 0.494205 | 0.322309 | 0.859968 | 0.357163 | 0.606668 | 00:35 | . 3 | 0.163544 | 0.197305 | 0.690802 | 0.944984 | 0.891899 | 0.105461 | 0.964988 | 0.862533 | 0.880129 | 0.444691 | 0.444886 | 0.822281 | 0.502733 | 0.734242 | 00:35 | . 4 | 0.157475 | 0.183090 | 0.700796 | 0.937403 | 0.889857 | 0.104603 | 0.966317 | 0.860737 | 0.905106 | 0.476430 | 0.573778 | 0.816515 | 0.466653 | 0.711353 | 00:34 | . 5 | 0.149104 | 0.195971 | 0.682122 | 0.945185 | 0.888626 | 0.116620 | 0.963612 | 0.859883 | 0.897999 | 0.347324 | 0.413924 | 0.842594 | 0.493171 | 0.734406 | 00:35 | . 6 | 0.146632 | 0.229591 | 0.673781 | 0.943697 | 0.891223 | 0.150029 | 0.968885 | 0.864813 | 0.850287 | 0.487822 | 0.321995 | 0.824384 | 0.412684 | 0.695772 | 00:35 | . 7 | 0.140522 | 0.190752 | 0.684680 | 0.948050 | 0.900241 | 0.082288 | 0.963449 | 0.861177 | 0.885507 | 0.323323 | 0.454499 | 0.867523 | 0.501653 | 0.743767 | 00:35 | . 8 | 0.134812 | 0.162649 | 0.720961 | 0.946529 | 0.903660 | 0.142304 | 0.971249 | 0.882453 | 0.899834 | 0.526134 | 0.572753 | 0.840024 | 0.488276 | 0.757354 | 00:35 | . 9 | 0.129773 | 0.167973 | 0.710287 | 0.943785 | 0.902817 | 0.139235 | 0.971278 | 0.887299 | 0.901242 | 0.471411 | 0.524803 | 0.834308 | 0.491301 | 0.745677 | 00:35 | . 10 | 0.122608 | 0.160006 | 0.733646 | 0.946728 | 0.905341 | 0.141303 | 0.968730 | 0.872145 | 0.903727 | 0.495692 | 0.603912 | 0.844581 | 0.579497 | 0.808447 | 00:35 | . 11 | 0.117691 | 0.160273 | 0.733083 | 0.947101 | 0.906433 | 0.146316 | 0.969303 | 0.876323 | 0.902350 | 0.521031 | 0.582046 | 0.861685 | 0.554041 | 0.797289 | 00:35 | . We&#39;ll save away this model and quickly check how it&#39;s doing on our test set: . learn.save(&quot;360&quot;) . Path(&#39;models/360.pth&#39;) . fnames = get_image_files(path_l/&#39;test&#39;) test_dl = learn.dls.test_dl(fnames, with_labels=True) metrics = learn.validate(dl=test_dl)[1:] names = list(map(lambda x: x.name, learn.metrics)) for value, metric in zip(metrics, names): print(metric, value) . miou 0.6513576513430401 Sky 0.9236697535182652 Building 0.8305953126939783 Pole 0.23508068238078056 Road 0.945941641771041 Pavement 0.8307263165520995 Tree 0.7624501080720603 SignSymbol 0.43377705211681206 Fence 0.3818599814936706 Car 0.8266845071526181 Pedestrian 0.5090933599937093 Bicyclist 0.4850554490284065 . We can see a starting mIOU of 65% almost matching the mid-tier performer, let&#39;s see if we can take it further by using the full sized images . Phase 2: . First let&#39;s free up our memory: . del learn torch.cuda.empty_cache() import gc gc.collect() . 11257 . We&#39;ll adjust our transforms to instead keep our full sized images: . item_tfms = [Resize(half), Resize(full)] batch_tfms = [*aug_transforms(size=full), Normalize.from_stats(*imagenet_stats)] . And simply train again from there: . camvid = DataBlock(blocks=(ImageBlock, MaskBlock(codes=codes)), get_items=get_image_files, splitter=FolderSplitter(valid_name=&#39;val&#39;), get_x=get_x, get_y=get_mask, item_tfms=item_tfms, batch_tfms=batch_tfms) dls = camvid.dataloaders(path_l, bs=2) dls.c = len(codes) - 1 . We&#39;ll need to re-declare our metrics as the current ones have memory of our last training session: . metrics = [MIOU(11, axis=1)] for x in range(11): metrics.append(IOU(x, codes[x], axis=1, ignore_index=11)) . And now let&#39;s train: . learn = unet_learner(dls, resnet34, metrics=metrics, opt_func=opt_func, config=config, loss_func=loss_func) learn.load(&#39;360&#39;); . learn.freeze() lr = 1e-3 learn.fine_tune(12, lr, cbs=[GradientAccumulation(n_acc=16), EarlyStoppingCallback()]) . epoch train_loss valid_loss miou Sky Building Pole Road Pavement Tree SignSymbol Fence Car Pedestrian Bicyclist time . 0 | 0.211583 | 0.217368 | 0.711367 | 0.949347 | 0.888438 | 0.112724 | 0.925386 | 0.750803 | 0.893861 | 0.535350 | 0.629848 | 0.865719 | 0.491392 | 0.782169 | 02:00 | . epoch train_loss valid_loss miou Sky Building Pole Road Pavement Tree SignSymbol Fence Car Pedestrian Bicyclist time . 0 | 0.180150 | 0.165417 | 0.734545 | 0.940707 | 0.910473 | 0.077289 | 0.952566 | 0.826502 | 0.915739 | 0.597755 | 0.683396 | 0.874197 | 0.522924 | 0.778450 | 02:01 | . 1 | 0.163459 | 0.167075 | 0.727612 | 0.941061 | 0.910744 | 0.090534 | 0.958094 | 0.846227 | 0.907565 | 0.588291 | 0.661547 | 0.872477 | 0.487174 | 0.740012 | 02:03 | . No improvement since epoch 0: early stopping . Let&#39;s check it&#39;s final IOU: . fnames = get_image_files(path_l/&#39;test&#39;) test_dl = learn.dls.test_dl(fnames, with_labels=True) metrics = learn.validate(dl=test_dl)[1:] names = list(map(lambda x: x.name, learn.metrics)) for value, metric in zip(metrics, names): print(metric, value) . miou 0.6408294816140846 Sky 0.9200597102142184 Building 0.829854500316895 Pole 0.25428639814379245 Road 0.8889084399152402 Pavement 0.6907592069297879 Tree 0.7675178133912139 SignSymbol 0.46596359844043667 Fence 0.3685350595915968 Car 0.7864697614577011 Pedestrian 0.5871856373610818 Bicyclist 0.48958417199296694 . Results and Discussion . At first we tried a standard Unet without any special tricks, and we got a test mIOU of around 59%. From this baseline we tried applying Self-Attention, Label Smoothing, and the Mish activation function (as the default is ReLU). . What we found is that by simply applying Mish we could boost that 59% to around 64%, and do note that Mish was only applied to the head of the model, not in the ResNet backbone. (with the highest we got around 67% mIOU) . Self Attention did not seem to help as much, bringing down the mIOU to 62% when training even with the Mish activation function. . Applying Label Smoothing led to a very different result baked inside of each individual IOU. While the mIOU was not as high as a flat Mish model, the distributions of the IOU&#39;s changed. . When applying the proper presizing techniques demonstrated here, we saw a boost of 10% mIOU, confirming an idea that simply blowing up your masks to match the original image resolution can diminish the value inside of them. . Conclusions . What conclusions can we actually make from this study? Not as much as you would think, and the reason lies within current issues in Academia. Right now there are three different datasets being used: . fastai images with SegNet masks | SegNet images and masks | fastai images and labels while ignoring all the other classes | . Well... who is right then? Technically 2 and 3 are right, but the three cannot be compared equally. Remember that benchmark table I showed earlier? If you go and read the papers each use one of the three techniques done here. . So... what can we make of this? . There is one direct conclusion we can make: using Mish in the head of our Dynamic Unet boosts the mIOU by 5%. So it is absolutely worth trying and using with your projects. . Where do we go from here? . A better dataset which is much more consistant is the CityScapes dataset. It&#39;s for research only and you must upload your predictions on the test set to the website, it&#39;s essentially a Kaggle competition for researchers, a format I believe works much better. Researchers compare both how they perform on the validation set and the test set. This is certainly an easier benchmark for folks to tackle with the fastai UNet, so hopefully one day someone will try a benchmark and see how it does! .",
            "url": "https://muellerzr.github.io/fastblog/papers/2020/09/19/CAMVID.html",
            "relUrl": "/papers/2020/09/19/CAMVID.html",
            "date": " â€¢ Sep 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Zero to Hero with `fastai` - Intermediate",
            "content": "Zero to Hero . The &quot;Zero to Hero&quot; series is a series of three articles geared towards getting anyone familair with the fastai library based upon their skill sets. The previous article is aimed towards those who have barely heard of &quot;Deep Learning&quot; and have zero experience with frameworks. This article article comes from a perspective of those who utilized the original fastai library in the past and understand the broad strokes of the library. Finally, the last article will briefly explain the advanced artifacts inside of fastai and how they all function. . Note: These articles also presume you have read the previous to avoid redundancy, please read it before continuing so some context can be gathered here . Who am I . My name is Zach Mueller, I&#39;ve extensively been involved and using fastai (and the newest version) for the better part of two years now. I&#39;ve designed my own course geared around the library from an implementation standpoint without getting too complex. At the time of writing this I&#39;m still an Undergraduate at the University of West Florida majoring in Computer Science. I&#39;m also heavily involved inside the fastai community, of which I would emplore you to join! My goal is to make fastai more approachable at all levels through examples and help further Jeremy&#39;s dream in the process. My specific interests involve speeding up the framework for deployment, tabular neural networks, and providing overall usage guides to help further the community . What will we cover in this article? . In the second iteration of &quot;Zero to Hero&quot; we will be going through the major differences between the two API&#39;s. We will look more in detail at the high-level DataBlock API with a 1:1 code example to learn how to adjust your code from the old fastai. Afterwards we will look into the Mid-level API and transforms briefly to see how simple it can be to customize and adapt what you want into the framework through two seperate examples. Finally will then go into customizing test sets to include labelled and non-labelled data. . Installing the library . First let&#39;s install fastai: . !pip install fastai -qqq . |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 4.7MB/s . What&#39;s new? . Let&#39;s first look at two sets of code for a Tabular task, specifically Adult Sample . Note: Some code cells may have # DO NOT RUN. Do not run these if you choose to open this notebook in Colaboratory as they reference the old codebase and will not work anymore . As per usual, we&#39;ll import the tabular library and use untar_data to grab the dataset: . from fastai.tabular.all import * . path = untar_data(URLs.ADULT_SAMPLE) . Then we will open the DataFrame in pandas: . df = pd.read_csv(path/&#39;adult.csv&#39;) . df.head() . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 0 49 | Private | 101320 | Assoc-acdm | 12.0 | Married-civ-spouse | NaN | Wife | White | Female | 0 | 1902 | 40 | United-States | &gt;=50k | . 1 44 | Private | 236746 | Masters | 14.0 | Divorced | Exec-managerial | Not-in-family | White | Male | 10520 | 0 | 45 | United-States | &gt;=50k | . 2 38 | Private | 96185 | HS-grad | NaN | Divorced | NaN | Unmarried | Black | Female | 0 | 0 | 32 | United-States | &lt;50k | . 3 38 | Self-emp-inc | 112847 | Prof-school | 15.0 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | United-States | &gt;=50k | . 4 42 | Self-emp-not-inc | 82297 | 7th-8th | NaN | Married-civ-spouse | Other-service | Wife | Black | Female | 0 | 0 | 50 | United-States | &lt;50k | . In both versions we still need to define our variables and procs, and the naming for each has not changed: . dep_var = &#39;salary&#39; cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] procs = [FillMissing, Categorify, Normalize] . However what did change was the API. Before our code would have looked like so: . # DO NOT RUN data = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs) .split_by_idx(list(range(800,1000))) .label_from_df(cols=dep_var) .databunch()) . Where we specify our API to have a TabularList, then split that list, label the list, and finally DataBunch it. This is gone, or at least simplified in the new version. Instead we have TabularPandas, a complete rework of the tabular API, which is different from the normal API. First we have special Splitter classes that we can call and use depending on our task. Since we are splitting by a list of indicies, it would make sense to utilize the IndexSplitter class. To utilize it we&#39;ll instantate the class with our list of indicies to split by, and then split our dataset via it&#39;s indicies. To explore more of these splitters, see the documentation. . splits = IndexSplitter(list(range(800,1000)))(range_of(df)); splits . ((#32361) [0,1,2,3,4,5,6,7,8,9...], (#200) [800,801,802,803,804,805,806,807,808,809...]) . Then we can pass everything to TabularPandas: . to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=&#39;salary&#39;, splits=splits) . Something very unique and nice about TabularPandas is we can actually use it in more than just fastai! To see how we can utilize it with Random Forests and XGBoost, see my course notebook where this is discussed . And now we can build our DataLoaders: . dls = to.dataloaders(bs=512) . From here, the API remains the same. We have a tabular_learner and we can fit, fit_one_cycle, etc. One minor change is now to find the learning rate, it&#39;s just lr_find: . learn = tabular_learner(dls, layers=[200,100], metrics=accuracy) learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.0030199517495930195) . Where it will also return the two suggested learning rates seen at the top of the graph. . A Text Example . The next major upgrade is the Text API. This is a true example of the High-Level DataBlock API discussed in the previous article. It follows the same overall pattern we saw in TabularPandas, just a bit more split-up. Let&#39;s take this example from my slides: . . We&#39;ll run through this example step by step with a text example with IMDB_SAMPLE. Let&#39;s load the library and grab our data: . from fastai.text.all import * . path_imdb = untar_data(URLs.IMDB_SAMPLE) . df_imdb = pd.read_csv(path_imdb/&#39;texts.csv&#39;) . In the first version of fastai, to build our language model DataLoader it would look something like below: . # DO NOT RUN data = (TextList.from_csv(path, &#39;texts.csv&#39;, cols=&#39;text&#39;) .split_by_rand_pct(0.1) .label_for_lm() .databunch(bs=8)) . Now let&#39;s convert this to the new API following our pipeline description above . Define your input and output blocks: | Here we have text as an input, so we simply say we have a TextBlock.from_df as have a DataFrame . Note: text is a bit different in this regard as things get tokenized when generating the Dataset. As a result, we have .from_df and .from_folder, specifying where the data comes from: . blocks = TextBlock.from_df(text_cols=&#39;text&#39;, res_col_name=&#39;text&#39;, is_lm=True) . Now an important distinction here is when tokenized, we will get rid of text_cols in our DataFrame and it will be replaced with res_col_name. This is text by default. Finally we want to specify that it is a language model by passing is_lm=True. . Define our getters | Now we need to tell fastai how to grab the data. Our text will be stored in a column named text, so we will use a ColReader to grab it: . get_x = ColReader(&#39;text&#39;) . Split the Data | We&#39;ll use another splitter like we did for tabular, this time using RandomSplitter. When calling our DataBlock we won&#39;t need to pass in the direct indicies, fastai will do this for us, so we can define it as below: . splitter = RandomSplitter(valid_pct=0.2, seed=42) . Label the Data | We have already done this by specifying is_lm to True back when we defined our blocks. When we examine a non-language model classification example next you will be able to understand the difference . Build the DataLoaders | Now let&#39;s build our DataBlock by passing in what we have: . dblock = DataBlock(blocks=blocks, get_x=get_x, splitter=splitter) . And we can build our DataLoaders: . dls = dblock.dataloaders(df_imdb, bs=8) . Let&#39;s look at an example batch: . dls.show_batch(max_n=2) . text text_ . 0 xxbos xxmaj with its companion piece xxup masters xxup of xxup horror , xxup nightmares xxup and xxup xxunk can only be seen as the absolute xxunk of the genre that began so xxunk with xxup the xxup twilight xxup zone and xxup the xxup outer xxup limits . n n xxmaj of course , part of the problem is that it does nothing to be of any interest to a xxunk adult | xxmaj with its companion piece xxup masters xxup of xxup horror , xxup nightmares xxup and xxup xxunk can only be seen as the absolute xxunk of the genre that began so xxunk with xxup the xxup twilight xxup zone and xxup the xxup outer xxup limits . n n xxmaj of course , part of the problem is that it does nothing to be of any interest to a xxunk adult audience | . 1 since he quickly realises ( given his stormy relationship with xxmaj clara as boss and xxunk ) that loving the person he knows through the xxunk letters might not xxunk with loving the person herself . xxmaj his description to xxmaj clara of the fictional xxmaj xxunk xxmaj xxunk ( what a name ! ) who was to become her xxunk is hilarious in the extreme , but also his way of | he quickly realises ( given his stormy relationship with xxmaj clara as boss and xxunk ) that loving the person he knows through the xxunk letters might not xxunk with loving the person herself . xxmaj his description to xxmaj clara of the fictional xxmaj xxunk xxmaj xxunk ( what a name ! ) who was to become her xxunk is hilarious in the extreme , but also his way of proving | . Now if we wanted to train, the API still looks the same, where we call language_model_learner and pass in our data. We won&#39;t train in this example though as that can take a bit with language models: . lm_learn = language_model_learner(dls, arch=AWD_LSTM, metrics=accuracy) . Now let&#39;s move onto a text classification example. This only requires two major changes to what we had before in our DataBlock: the addition of another block and a get_y to tell fastai where our label is: . blocks = (TextBlock.from_df(text_cols=&#39;text&#39;, res_col_name=&#39;text&#39;, is_lm=False), CategoryBlock()) . We set is_lm to False (the default) and added a CategoryBlock telling fastai we will be dealing with a classification problem. Next we need a get_y to say where the label is. It&#39;s still in that same DataFrame, so we can use another ColReader: . get_y = ColReader(&#39;label&#39;) . Finally, we&#39;ll make a new splitter that splits from a column, as our DataFrame has a is_valid option: . splitter = ColSplitter(col=&#39;is_valid&#39;) . Now let&#39;s remake our DataBlock: . clas_dblock = DataBlock(blocks=blocks, get_x=get_x, get_y=get_y, splitter=splitter) . And make some new DataLoaders: . dls = clas_dblock.dataloaders(df_imdb, bs=8) . And that&#39;s it for the text example! Now you&#39;ve seen the basic building blocks and how it all works. For the final example we&#39;ll walk through the PETS dataset as we did during the previous article, and recreate it with the API . Pets . from fastai.vision.all import * . Let&#39;s first grab our data: . path = untar_data(URLs.PETS) . We&#39;ll define our blocks again. This time, since we have an image problem we&#39;ll use an ImageBlock and re-use CategoryBlock: . blocks = (ImageBlock(cls=PILImage), CategoryBlock()) . Note that we can define sub-classes for blocks to use. If we were doing a black and white image problem (such as MNIST), we could define our ImageBlock as ImageBlock(cls=PILImageBW) . Next we want our getters. This is actually just as simple as get_image_files. Why? Let&#39;s look: . imgs = get_image_files(path/&#39;images&#39;) . imgs[0] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_194.jpg&#39;) . Here we have a list of our images, so this is all we actually need since both our x and y are there. . Next we want to split the data. We did a random 80/20 split in the first article, so we will repeat this here using RandomSplitter: . splitter = RandomSplitter(valid_pct=0.2, seed=42) . Finally we need our labeller, which is a RegexLabeller: . get_y=RegexLabeller(pat = r&#39;/([^/]+)_ d+.*&#39;) . Now before we continue we need some item and batch transforms to augment our data: . item_tfms = RandomResizedCrop(460, min_scale=0.75, ratio=(1.,1.)) batch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)] . And finally we can work this into our DataBlock: . dblock = DataBlock(blocks=blocks, get_items=get_image_files, splitter=splitter, get_y=get_y, item_tfms=item_tfms, batch_tfms=batch_tfms) . Let&#39;s build our data: . dls = dblock.dataloaders(path/&#39;images&#39;, bs=64) . And view some data just to be sure: . dls.show_batch() . We have now seen three major examples of the API from a DataLoader perspective. Along with this article I invite you to read my other articles related to the DataBlock API: . fastai and the New DataBlock API | The Idea of a Transform | Looking at fastai&#39;s test_dl | . As they cover a few more specifics in regards to the API. . Test Sets . Now finally I mentioned the addition of labelled and non-labelled test sets. Originally back in the old fastai version when you did add_test for a test set and wanted it labelled you had to do a weird workaround. However this is no longer the case. With fastai&#39;s test_dl method we can pass with_labels=True and it will attempt to label our data if it is labelled the same format as it were for training. . Note: tabular problems will always assume with_labels to be True if the y is present in the DataFrame . Now let&#39;s first use the defaults for test_dl on some data: . dl = dls.test_dl(imgs[:10], with_labels=False) . We can look at a batch: . dl.show_batch() . And we can just see blank images! Now if we change this: . dl = dls.test_dl(imgs[:10], with_labels=True) dl.show_batch() . We have our labels again! This is fantastically nice as you can then just pass this DataLoader into learn.validate by doing learn.validate(dl=dl) and there will be no complaints! . Minor Changes and Closing Thoughts . Finally, let&#39;s cover some minor naming changes. . Passing callbacks to our Learner and during any fit are now called cbs rather than callbacks | Callbacks have more events in which you are able to adjust and their naming is slightly different | Metrics should inherit AccuMetric, but loss functions do not need this | . Everything is is a major API change or difference altogether. . Thank you so much for reading, and I implore you to check out the new library! It&#39;s been carefully crafted over the last year and a half (since the previous part 2) and has really turned into something special. These first two articles I wanted out the day of release, so part 3 will take me a few more days. Thanks again for reading and have fun exploring! .",
            "url": "https://muellerzr.github.io/fastblog/2020/08/21/intermediate.html",
            "relUrl": "/2020/08/21/intermediate.html",
            "date": " â€¢ Aug 21, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Zero to Hero with `fastai` - Beginner",
            "content": "Zero to Hero . The &quot;Zero to Hero&quot; series is a collection of three seperate articles geared towards getting anyone familair with the fastai library based upon their skill sets. This article is geared towards those who have barely heard of &quot;Deep Learning&quot; and have zero experience with frameworks. The intermediate article comes from a perspective of those who utilized the original fastai library in the past and understand the broad strokes of the library. Finally, the last article will briefly explain the advanced artifacts inside of fastai and how they all function. . Who am I . My name is Zach Mueller, I&#39;ve extensively been involved and using fastai (and the newest version) for the better part of two years now. I&#39;ve designed my own course geared around the library from an implementation standpoint without getting too complex. At the time of writing this I&#39;m still an Undergraduate at the University of West Florida majoring in Computer Science. I&#39;m also heavily involved inside the fastai community, of which I would emplore you to join! My goal is to make fastai more approachable at all levels through examples and help further Jeremy&#39;s dream in the process. My specific interests involve speeding up the framework for deployment, tabular neural networks, and providing overall usage guides to help further the community . What will we cover in this article? . In broad strokes, this article will cover what exactly a &quot;Jupyter Notebook&quot; is and why it is popular among the Machine Learning community. Next we will divulge into fastai, what the library aims to do, and why it can be considered a &quot;library for all.&quot; Afterwards we will close by showing examples of what a &quot;model&quot; is and how to utilize the framework at a high level to complete your Deep Learning tasks. . What is a &quot;Jupyter Notebook&quot;? . Why, you&#39;re reading one now! A Jupyter Notebook is a type of workflow that involves active coding. In a typical software development setting, normally everything would be put into various files and run from a console (such as .cpp, .py, etc). This does not provide much interactability to the user, nor anyone reading the code over. . Jupyter provides a middle-ground for this. Each &quot;cell&quot; in a notebook can be run independantly of others, such as a quick example below where we do a basic addition: . To run a Jupyter cell, you can press &quot;shift + enter&quot; . o = 1+2; print(o) . 3 . Jupyter uses pretty print to make outputs display readable, so they may differ from your regular console outputs. This in turn allows us to break up code when writing documentation directly inside the contained notebook and not rely on just code comments. As mentioned earlier, this blog was written from a Jupyter Notebook! The platform is evolving as the needs grow, and as such the library leans into this. If you are curious to how, read the documentation for nbdev and the fastai article as well. . For absolute beginners, Google Colaboratory is a great free platform to get familiar with Jupyter Notebooks and how the environment operates. For this series we will be running our code inside of Google Colab with the GPU enabled, which helps speed up training time. . Note: Google Colaboratory&#8217;s environment is not standard Jupyter, so some things may differ but the overall appearance and operation remains the same. . What is this fastai library, and Deep Learning? What is the goal? . fastai is an open-source library designed to make State-of-the-Art Machine Learning and Deep Learning approachable for everyone. Spearheaded by Jeremy Howard, this library has seen many iterations, with each looking radically different than the last. The newest version seeks to solve many of the headaches most frameworks have, where there is too much code and not enough readability to those who may not have the coding background. This is the goal of fastai: make machine learning approachable by anyone regardless of their backgrounds to help further progress. Along with this, the layered API makes it suitable for researchers that want to customize absolutely everythign while ensuring strong-performing baselines. . These problems come in a variety of shapes and sizes, but can absolutely touch every aspect of the world. Something as simple as identifying cats versus dogs to helping self-driving vehicles operate safer and more effectively. All of which, while may not be able to solved by fastai directly without some work, Deep Learning can help provide the solution. . Finally, in an attempt to mitigate this, fast.ai has provided numerous resources for you to peruse and learn from thanks to the hard efforts of Jeremy, Rachel Thomas, and Sylvain Gugger. Their new course, the fourth edition of Practical Deep Learning for Coders, is available here, and their newly released book is available off Amazon and available for free with fastbook. Finally, I have released my own sub-course based on fastai from an application-only perspective available here. . Installing the Library and a General Overview: . In Python (what these notebooks run on), we can install specific programming packages with pip. We&#39;ll do so with fastai below: . !pip install fastai -qqq . |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 2.8MB/s . We include the three q&#39;s to silence the installation and keep our notebook pretty clean. If your code outputs ever get too cumbersome, you can always clear their outputs. . Now let&#39;s talk about an interesting coding practice. The fastai library revolves around importing sections of the library with from fastai.module.all import *. This can be extremely uncomfortable (or just wrong) to some, however this library is built upon utilizing every import and making it available. In general fastai has four submodules geared towards specific tasks: . vision - Image-related problems | tabular - Structured data-related problems | text - Text related problems | collab - Collaborative filtering-related tasks | . As you can see, it&#39;s a very readable library. We would call semantically based on what task is being performed. For our example, we will classify between species of dogs and cats based on their pictures. Given we are using images, let&#39;s import the vision module: . from fastai.vision.all import * . The first step is to gather our data. Since this is a pre-made dataset, fastai can download the .tar file using untar_data. The path itself is stored inside URLs.PETS, and calling this function will return where our data was stored: . path = untar_data(URLs.PETS) . Pre-Processing our Data and the High-Level API . When training Machine Learning models, we need to gather our data into something that can be grouped into mini sets or &quot;batches&quot;, and apply some form of adjustments to our data, or augmentation. This in turn lets us feed our model data efficiently and can provide unique challenges in our data that may not have been present before. Such augmentations could be flipping the image, rotating it, adjusting the exposure, etc. fastai has available one-liners to allow our data to be processed. Let&#39;s walk through our PETs example more. . First, we want to check where and how the data is stored: . path.ls() . (#2) [Path(&#39;/root/.fastai/data/oxford-iiit-pet/annotations&#39;),Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;)] . In this particular instance, our images are stored in the images folder. Now let&#39;s pull the filenames and take a look: . imgs = get_image_files(path/&#39;images&#39;); imgs[0] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_194.jpg&#39;) . We can see that our label is inside of our filename. Using a technique called regex, we can extract it. fastai comes equipped with a RegexLabeller to do so. . There are a suit of other labellers you can explore in the documentation as well as other examples including how to build your own. . First we&#39;ll need a regex pattern capable of extracting our filename. This is provided below for our example: . pat = r&#39;(.+)_ d+.jpg$&#39; . Next let&#39;s define a few of those &quot;augmentations&quot; for our data. We&#39;ll want to ensure that before stacking multiple images into batches (so they can be run efficently through our models) they are the same size. There are many resize transforms available and for our case we will use RandomResizedCrop which will randomly resize the image before cropping it to the determined size. Our example will crop our image to a 460 pixel by 460 pixel image: . item_tfms = [RandomResizedCrop(460)] . You will notice that I named this item_tfms. To seperate what is applied individually on an input-by-input bases versus into batches, they are seperated by item and batch tfms. . Next our batch transforms will apply some basic random agumentation before finally normalizing all of our data. This example will utilize something called transfer learning. This involves taking another model which has been trained initially on some dataset and utilizing it for our own dataset. This allows for us to train models faster to our datasets. To do so we need to utilize the original model&#39;s training statistics (the mean and standard deviation) and we then normalize our input data based on these values. Our pre-trained model used the ImageNet dataset, so this will be reflected here: . batch_tfms = [*aug_transforms(size=224), Normalize.from_stats(*imagenet_stats)] . Finally we need to define how many images get fed into our model at one time. This is called our &quot;batch size&quot;. Our example will feed 64: . bs = 64 . Now let&#39;s bring it all together into DataLoaders. These contain our transform information and apply them to our data as we want to feed it to our Machine Learning models. We will feed it a relative path to our data, a list of images to use, the pattern to extract our labels, our transforms, and finally our batch size. . dls = ImageDataLoaders.from_name_re(path, imgs, pat, item_tfms=item_tfms, batch_tfms=batch_tfms, bs=bs) . Under the surface here our data was automatically split with 80% going to the training dataset and 20% going to the validation set. We can see how many classes there are in our problem as well as their names by looking inside dls.c and dls.vocab: . dls.vocab . (#37) [&#39;Abyssinian&#39;,&#39;Bengal&#39;,&#39;Birman&#39;,&#39;Bombay&#39;,&#39;British_Shorthair&#39;,&#39;Egyptian_Mau&#39;,&#39;Maine_Coon&#39;,&#39;Persian&#39;,&#39;Ragdoll&#39;,&#39;Russian_Blue&#39;...] . dls.c . 37 . We can view a batch of our data with dls.show_batch . dls.show_batch() . As well as specify which DataLoader (the training or validation) we want to see: . dls.train.show_batch() . Training Your Model . Next we will need to make a model to train on our data. This example will use the ResNet34 architecture. To do so, we will call cnn_learner (or convolutional-neural-network) to generate a Learner ready to train. . We will pass in our DataLoaders we made a moment ago, an architecture to use, as well as any metrics we would like to use. Metrics provide a human-readable understanding of the results in our model. We&#39;ll use the error rate in this example: . learn = cnn_learner(dls, resnet34, pretrained=True, metrics=error_rate) . Now all that&#39;s needed is to fit our model to our data. This can be done quickly and effectively with fastai&#39;s fine_tune method for transfer learning. We&#39;ll fit for six iterations through our data (or epochs): . learn.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 2.051884 | 0.379754 | 0.110284 | 01:07 | . epoch train_loss valid_loss error_rate time . 0 | 0.909658 | 0.320852 | 0.102842 | 01:09 | . 1 | 0.833673 | 0.328793 | 0.094046 | 01:09 | . 2 | 0.725367 | 0.286304 | 0.094723 | 01:09 | . 3 | 0.573307 | 0.249599 | 0.080514 | 01:09 | . 4 | 0.522223 | 0.260281 | 0.082544 | 01:09 | . . Note: if each epoch takes more than a minute or two, you do not have the GPU enabled on your platform and are relying on the CPU . Great! We have only a 8% error rate at identifying 37 different types of cats and dogs, and we did so in less than 15 lines of code with the defaults! Not bad! If you want to take this further I invite you to learn about the DataBlock API in the next article or through any of the other fastai-related resources mentioned earlier. . Lastly we can take a look at the results of our model by calling, you guessed it, learn.show_results()! Let&#39;s take a peek: . learn.show_results() . Now what can I do? . We have a trained model, and we need to utilize it in production. The first initial step is to export our model. This will package everything important to us in a production-level environment. Specifically this includes how our data was made (but not the data itself!), the transforms applied to them, and the model itself. . We&#39;ll do so below: . learn.export(fname=&#39;export.pkl&#39;) . To load our models back in we call load_learner. We can then further map these to the CPU (by default) or GPU. We&#39;ll map to the GPU since we are still in our GPU instance: . learn = load_learner(fname=path/&#39;export.pkl&#39;, cpu=False) . Gathering Predictions . fastai has two methods to do this, predict, and get_preds. We&#39;ll focus on the prior but still mention the latter. . Predict . Predict is aimed at predicting one item. It will apply all the transforms done to our validation set only and then feed it to the model. Let&#39;s grab one of our filenames we trained on: . test_im = imgs[0]; test_im . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_194.jpg&#39;) . We simply pass this path into predict and it will tell us the relative class, the class index in the vocabulary, as well as the percentages: . learn.predict(test_im) . (&#39;english_cocker_spaniel&#39;, tensor(18), tensor([2.7911e-06, 9.8194e-06, 5.7725e-07, 1.4929e-06, 2.4808e-06, 1.9052e-06, 1.0944e-05, 8.0723e-06, 2.7175e-07, 4.1204e-06, 1.3944e-06, 2.8187e-05, 5.4635e-06, 4.4745e-06, 1.0689e-05, 5.9237e-07, 3.9247e-06, 1.0384e-07, 9.0385e-01, 1.1940e-04, 3.4542e-05, 1.4463e-05, 1.2706e-02, 4.1305e-06, 5.4780e-06, 8.7920e-06, 8.0724e-08, 1.8775e-03, 8.8555e-07, 1.0900e-05, 8.1132e-07, 1.1343e-06, 1.4847e-03, 3.3809e-07, 1.2145e-06, 7.9747e-02, 3.1224e-05])) . If we are dealing with a slew of data at once, we can convert them into more of those batches again by calling learn.dls.test_dl. This will make a new validation DataLoader we can pass to get_preds to get predictions off of. We&#39;ll make one based on the first five images: . fnames_test = imgs[:5] dl = learn.dls.test_dl(fnames_test) . We can see it&#39;s just like our DataLoaders from earlier by calling show_batch: . dl.show_batch() . And finally we can grab predictions by passing our dl to get_preds: . preds = learn.get_preds(dl=dl, with_decoded=True) . preds[2] . tensor([18, 30, 7, 7, 2]) . You&#39;ll notice it&#39;s a little different from predict, instead we get the raw probabilities and the class indicies inside of vocab: . learn.dls.vocab[18] . &#39;english_cocker_spaniel&#39; . And that&#39;s it! Again if you wish to learn more, there is a second part to this series, Jeremy and Sylvains book, their new course, as well as my own course. Best of luck and enjoy the library! .",
            "url": "https://muellerzr.github.io/fastblog/2020/08/21/beginner.html",
            "relUrl": "/2020/08/21/beginner.html",
            "date": " â€¢ Aug 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Looking at fastai's test_dl",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this: . fastai2: 0.0.17 | fastcore: 0.1.18 | . . The test_dl, what is it? How does it work? How can I customize it? . In this article we&#39;ll be exploring fastai2&#39;s test_dl method, where it comes from, what it is actually performing, and how we can utilize it on our models. . Vision . We&#39;ll start with a Vision based problem, the standard PETS dataset. Let&#39;s quickly build some DataLoaders via our DataBlock: . from fastai2.vision.all import * . path = untar_data(URLs.PETS) fnames = get_image_files(path/&#39;images&#39;) pat = r&#39;(.+)_ d+.jpg$&#39; batch_tfms = [*aug_transforms(size=224, max_warp=.9), Normalize.from_stats(*imagenet_stats)] item_tfms = RandomResizedCrop(460, min_scale=0.75, ratio=(1.,1.)) bs=64 dls = ImageDataLoaders.from_name_re(path, fnames, pat, batch_tfms=batch_tfms, item_tfms=item_tfms, bs=bs) . I&#39;ve made our max warp extremely high here, so we can see the difference between the train and validation set . Now there&#39;s a few key points of interest I&#39;d like to point out here. Every augmentation transform has the chance to provide special directions if we are looking at the validation set. What does it mean in this case? Let&#39;s take a look. . First, our training set: . dls.train.show_batch() . And now the validation: . dls.valid.show_batch() . As we can see, our heavy training augmentation didn&#39;t get applied to the validation set. Which is good! This is what we want. Now, why do we care about this though? Time to dig into the test_dl method: . test_dl . Now if we consider what test_dl sounds like, we would presume it would be to make a test DataLoader (just like our training and validation DataLoaders from a moment ago) from new data that we do not wish to train on (ala inference). Let&#39;s see how the code stacks up: . @delegates(TfmdDL.__init__) @patch def test_dl(self:DataLoaders, test_items, rm_type_tfms=None, with_labels=False, **kwargs): &quot;Create a test dataloader from `test_items` using validation transforms of `dls`&quot; test_ds = test_set(self.valid_ds, test_items, rm_tfms=rm_type_tfms, with_labels=with_labels ) if isinstance(self.valid_ds, (Datasets, TfmdLists)) else test_items return self.valid.new(test_ds, **kwargs) . Great! So... what on earth does this actually mean? Let&#39;s walk through it line by line. . First, the odd fastcore and TypeDispatch bits. @delgates ensures that when we press tab when calling our test_dl method in DataLoaders, it will show the same initialization parameters as TfmdDL. . Later we will see how this adjusts to different DataLoader types, so don&#39;t fear about it just being available to the TfmdDL! . Next we have an @patch method, all this really means is we can define functions for classes outside their actual class definition, allowing us to modify and extend classes on the fly. . Now let&#39;s move onto the parameters: . self:DataLoaders: This is for @patch, the type (DataLoaders) tells it what we&#39;re patching onto | test_items: These, as you can imagine, simply is a collection of items that we want our test_dl to have | rm_type_tfms: we&#39;ll look into this more in-depth later but we can remove our type_tfms from our DataLoader if we expect our inputs to change | with_labels: If we expect our data to be labelled in the same way our training data was labelled, we can optionally label our data. | . Now that we know all of this, what actually happens? . So first we need to make a Dataset, just as we would with any fastai pipeline. In our case we have a special test_set function, which takes an input from a Dataset, some test items, and the rest of our parameters and generates a new dataset based on the input dataset. Since in production we want everything to mimic the validation set, we pass in dls.valid_ds . We&#39;ll briefly look at the code, however it can be intimidating so don&#39;t be scared! . def test_set(dsets, test_items, rm_tfms=None, with_labels=False): &quot;Create a test set from `test_items` using validation transforms of `dsets`&quot; if isinstance(dsets, Datasets): tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp] test_tls = [tl._new(test_items, split_idx=1) for tl in tls] if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(test_items)) for tl in test_tls] else: rm_tfms = tuplify(rm_tfms, match=test_tls) for i,j in enumerate(rm_tfms): test_tls[i].tfms.fs = test_tls[i].tfms.fs[j:] return Datasets(tls=test_tls) elif isinstance(dsets, TfmdLists): test_tl = dsets._new(test_items, split_idx=1) if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(test_items)) test_tl.tfms.fs = test_tl.tfms.fs[rm_tfms:] return test_tl else: raise Exception(f&quot;This method requires using the fastai library to assemble your data. Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}&quot;) . All we&#39;re really doing here is first taking in our base set (as I mentioned earlier), potentially removing any of the type transforms, and then building a new dataset based on our old one! . We&#39;ll quickly build two here, with the first keeping our type_tfms and the second removing them to see the difference. . Let&#39;s grab some filenames: . fns_small = fnames[:10]; fns_small[0] . Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Birman_132.jpg&#39;) . Next we&#39;ll try to make some test sets: . a = test_set(dls.valid_ds, fns_small); a . (#10) [(PILImage mode=RGB size=500x343,),(PILImage mode=RGB size=225x300,),(PILImage mode=RGB size=500x375,),(PILImage mode=RGB size=225x300,),(PILImage mode=RGB size=359x500,),(PILImage mode=RGB size=375x500,),(PILImage mode=RGB size=375x500,),(PILImage mode=RGB size=375x500,),(PILImage mode=RGB size=500x331,),(PILImage mode=RGB size=262x246,)] . We can see that we have a collection of PIL images. You may notice though that they are tuples. Why? This is so we can cover a scenario in-which we have labels, like below: . b = test_set(dls.valid_ds, fns_small, with_labels=True); b . (#10) [(PILImage mode=RGB size=500x343, TensorCategory(2)),(PILImage mode=RGB size=225x300, TensorCategory(28)),(PILImage mode=RGB size=500x375, TensorCategory(22)),(PILImage mode=RGB size=225x300, TensorCategory(1)),(PILImage mode=RGB size=359x500, TensorCategory(29)),(PILImage mode=RGB size=375x500, TensorCategory(4)),(PILImage mode=RGB size=375x500, TensorCategory(14)),(PILImage mode=RGB size=375x500, TensorCategory(3)),(PILImage mode=RGB size=500x331, TensorCategory(33)),(PILImage mode=RGB size=262x246, TensorCategory(1))] . Great! Now what about that rm_type_tfm thingy? Let&#39;s try it. We can pass in a 0 or a 1, let&#39;s explore what each does: . We&#39;ll just use the first two lines, as this gets us to the point we want: . tls = dls.valid_ds.tls[:dls.valid_ds.n_inp] test_tls = [tl._new(fns_small, split_idx=1) for tl in tls] . test_tls[0].tfms.fs . (#2) [noop: (object,object) -&gt; noop ,PILBase.create: (bytes,object) -&gt; create (ndarray,object) -&gt; create (Tensor,object) -&gt; create (str,object) -&gt; create (Path,object) -&gt; create ] . So as we can see we have a few different transforms here. If we want to remove our PILBase.create, we could simply pass idx=2: . An index of 0 won&#39;t remove anything . c = test_set(dls.valid_ds, fns_small, rm_tfms=2); c.tfms.fs . (#0) [] . And if we look at C now: . c . (#10) [(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Birman_132.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/pomeranian_116.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/havanese_94.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Bengal_166.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/pug_30.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/British_Shorthair_16.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/basset_hound_45.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Bombay_67.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/shiba_inu_78.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Bengal_157.jpg&#39;),)] . It&#39;s just paths! However, can we pass our own in, if say our data input suddenly changes? Let&#39;s try. For now this will require a little hack. First let&#39;s remove our PILImage from our dls: . dls = ImageDataLoaders.from_name_re(path, fnames, pat, batch_tfms=batch_tfms, item_tfms=item_tfms, bs=bs) . dls.valid_ds.tls[0].tfms = Pipeline() . Now if we try it, it will fail: . d = test_set(dls.valid_ds, fns_small); d . (#10) [(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Birman_132.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/pomeranian_116.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/havanese_94.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Bengal_166.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/pug_30.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/British_Shorthair_16.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/basset_hound_45.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Bombay_67.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/shiba_inu_78.jpg&#39;),),(Path(&#39;/home/ml1/.fastai/data/oxford-iiit-pet/images/Bengal_157.jpg&#39;),)] . Now if we simply override it (we override 0 for the input and 1 for the label): . new_pipe = Pipeline(PILImage.create); new_pipe . Pipeline: PILBase.create . dls.valid_ds.tls[0].tfms = new_pipe . e = test_set(dls.valid_ds, fns_small); e . (#10) [(PILImage mode=RGB size=500x343,),(PILImage mode=RGB size=225x300,),(PILImage mode=RGB size=500x375,),(PILImage mode=RGB size=225x300,),(PILImage mode=RGB size=359x500,),(PILImage mode=RGB size=375x500,),(PILImage mode=RGB size=375x500,),(PILImage mode=RGB size=375x500,),(PILImage mode=RGB size=500x331,),(PILImage mode=RGB size=262x246,)] . We can see we have it back! You can use this to override anything, such as if you were to want to grab the name from a column. We&#39;ll do a quick example: . df = pd.DataFrame(columns=[&#39;fname&#39;]) df[&#39;fname&#39;] = fns_small . df.head() . fname . 0 /home/ml1/.fastai/data/oxford-iiit-pet/images/Birman_132.jpg | . 1 /home/ml1/.fastai/data/oxford-iiit-pet/images/pomeranian_116.jpg | . 2 /home/ml1/.fastai/data/oxford-iiit-pet/images/havanese_94.jpg | . 3 /home/ml1/.fastai/data/oxford-iiit-pet/images/Bengal_166.jpg | . 4 /home/ml1/.fastai/data/oxford-iiit-pet/images/pug_30.jpg | . Now of course we weren&#39;t designed to work with this, so let&#39;s build a new Pipeline: . p = Pipeline([ColReader(&#39;fname&#39;), PILImage.create]) . Let&#39;s test it out: . p(df.iloc[0]) . Great! Let&#39;s try that overriding method one more time: . dls.valid_ds.tls[0].tfms = p . dls.valid_ds.tls[0].types.insert(0, pd.Series) . Note that this time since we&#39;re expecting a different type than before, we need to tell fastai to accept a pd.Series input! Just for clarity, here is what it was before: . dls.valid_ds.tls[0].types[1:] . [pathlib.PosixPath, (pathlib.Path, str, torch.Tensor, numpy.ndarray, bytes), fastai2.vision.core.PILImage] . Showing that fastai wanted a Path, anything that PILImage could create with, and eventually a PILImage (great debugging TIL!) . Now let&#39;s get back to that test set. Does that work? . f = test_set(dls.valid_ds, df); . f[0][0].show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9ca0817950&gt; . There it is! Great! So now we know how to completely customize test_dl to not only accept a different input pipeline, but we also understand what it is doing in the background too! . And of course we&#39;ve only looked at it from a Dataset level, lets dig into the DataLoader part. . DataLoader . Our DataLoader takes our newly created Dataset and applies any validation-set transforms present before pushing it all to a new DataLoader. As we can imagine, it&#39;s based off the validation DataLoader and looks similar to before, with the one line in question being: . self.valid.new(test_ds, **kwargs) . Now what can we pass to those kwargs? Any new transforms you want present. Remember this is not like our type transforms, that won&#39;t work here and needs our workaround. What we can do is override our batch size wanted, the item_tfms, batch_tfms, if we want it shuffled, etc. . What will then happen is for all of our transforms, we will run their validation counterpart only. For vision remember we saw that it did not apply the Warp transform. There are a few other bits and pieces, such as crops will only center crop. I invite you to go explore the augmentation docs and source code to see how it all works under the hood. And of course, this all will immediatly stem from a call to: . test_dl = dls.test_dl(df) . test_dl.show_batch() . /home/ml1/anaconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . But what happens if I&#39;ve exported my DataLoaders? . If you choose to either do learn.export or pickle the dataloaders themselves, export will actually create a &quot;draft&quot; DataLoader, keeping the pipelines in-place while getting rid of any data present. This allows us to then utilize the same transform pipeline in production very efficiently: . def export(self:Learner, fname=&#39;export.pkl&#39;, pickle_protocol=2): &quot;Export the content of `self` without the items and the optimizer state for inference&quot; if rank_distrib(): return # don&#39;t export if slave proc self.dl = None old_dbunch = self.dls self.dls = self.dls.new_empty() # Right here! state = self.opt.state_dict() if self.opt is not None else None self.opt = None with warnings.catch_warnings(): #To avoid the warning that come from PyTorch about model not being checked warnings.simplefilter(&quot;ignore&quot;) torch.save(self, self.path/fname, pickle_protocol=pickle_protocol) self.create_opt() if state is not None: self.opt.load_state_dict(state) self.dls = old_dbunch . Which is done via the call to self.dls.new_empty() . Cool, how does it differ in other types? . Let&#39;s take a look, first at Tabular then at Text: . from fastai2.tabular.all import * . Tabular&#39;s test_dl (which stems from TabularDataLoaders.test_dl) looks like so: . def test_dl(self, test_items, rm_type_tfms=None, **kwargs): to = self.train_ds.new(test_items) to.process() return self.valid.new(to, **kwargs) . Let&#39;s talk quickly about what&#39;s happening here. For those familiar with the tabular API, you may recall that fastai uses the TabularPandas to generate our dataset and apply preprocessing. We follow the same principal as earlier, creating a new TabularPandas based on the training set (as all our pre-processing statistics are based on the training data), we apply them to the dataset via to.process(), before finally creating a new validation DataLoader like before. . What about text? . from fastai2.text.all import * . def test_dl(self:DataLoaders, test_items, rm_type_tfms=None, with_labels=False, **kwargs): &quot;Create a test dataloader from `test_items` using validation transforms of `dls`&quot; test_ds = test_set(self.valid_ds, test_items, rm_tfms=rm_type_tfms, with_labels=with_labels ) if isinstance(self.valid_ds, (Datasets, TfmdLists)) else test_items return self.valid.new(test_ds, **kwargs) . Since the Text API is the same as the Vision API, nothing is different here! . Closing Remarks . I hope this helps some of you further understand how to use the test_dl in both a production environment and your own testing as you&#39;re working on your deep learning models. The latency in how fastai generates these datasets is extremely efficent (trust me, I tried to speed it up myself) and highly adaptable (with some work). Thanks for reading! .",
            "url": "https://muellerzr.github.io/fastblog/2020/08/10/testdl.html",
            "relUrl": "/2020/08/10/testdl.html",
            "date": " â€¢ Aug 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "A walk with fastinference - Part 1",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this: . fastai2: 0.0.17 | fastcore: 0.1.18 | fastinference: 0.0.4 | . . fastinference, what is it and why do I need it? . Over the last few months I&#39;ve been trying to speed up inference for fastai, and more and more I was noticing that I was using the same &quot;realm&quot; of functions! So I decided to try to fit them into a cohesive library, with some special perks along with it. In this library we have (along with the speed-up modules), some interpretability modules as well as ONNX support. Each will be getting their own article to walk you through those, today we&#39;ll be covering the fastai speed ups! . Starting with Vision . Let&#39;s begin with a vision problem. We&#39;ll use our very familiar PETs dataset and quickly train a model. What we&#39;ll be focusing on is two very specific functions, .predict and .get_preds, so first let&#39;s prepare: . I will be skipping the explaination for this part, if you&#39;d like an in-depth walkthrough of the API, see my article here. | . from fastai2.vision.all import * . path = untar_data(URLs.PETS) pat = r&#39;([^/]+)_ d+.*$&#39; splitter = RandomSplitter(valid_pct=0.2, seed=42) item_tfms = [Resize(224, method=&#39;crop&#39;)] batch_tfms=[*aug_transforms(size=256), Normalize.from_stats(*imagenet_stats)] fnames = get_image_files(path/&#39;images&#39;) . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=RegexLabeller(pat=pat), item_tfms=item_tfms, batch_tfms=batch_tfms) . pets_dls = dblock.dataloaders(path/&#39;images&#39;) . Now that we have our DataLoader, we&#39;ll use a regular resnet34 so we can see the speed: . learn = cnn_learner(pets_dls, resnet34, metrics=accuracy) . learn.fine_tune(1) . epoch train_loss valid_loss accuracy time . 0 | 1.499775 | 0.304153 | 0.901218 | 00:53 | . epoch train_loss valid_loss accuracy time . 0 | 0.453589 | 0.239638 | 0.924222 | 00:55 | . learn.save(&#39;pets&#39;) . Now we&#39;ll time a get_preds and a predict on both CPU and CUDA: . %%time # CUDA: preds = learn.get_preds() . CPU times: user 990 ms, sys: 326 ms, total: 1.32 s Wall time: 10 s . %%time # CUDA: pred = learn.predict(fnames[0]) . CPU times: user 72.3 ms, sys: 140 ms, total: 212 ms Wall time: 253 ms . learn.dls.device = &#39;cpu&#39; . %%time # CPU: preds = learn.get_preds() . Note I skipped this, as it takes ~4:54 seconds | . %%time # CPU: preds = learn.predict(fnames[0]) . CPU times: user 243 ms, sys: 9.13 ms, total: 252 ms Wall time: 263 ms . Okay, not too bad right? But can we make it better. Let&#39;s bring in fastinference: . from fastinference.inference import * . Now for those of you Python-savvy, you&#39;ll notice we don&#39;t actually import anything. Why? We override .predict and .get_preds. As such, no adjustments are needed except for rebuilding Learner! . pets_dls.device= &#39;cuda&#39; . learn = cnn_learner(pets_dls, resnet34, metrics=accuracy).load(&#39;pets&#39;) . Let&#39;s try our timings again: . %%time # CUDA: preds = learn.get_preds() . CPU times: user 649 ms, sys: 206 ms, total: 855 ms Wall time: 9.83 s . %%time # CUDA: pred = learn.predict(fnames[0]) . CPU times: user 32.1 ms, sys: 2.01 ms, total: 34.1 ms Wall time: 34.7 ms . So as you saw, we shaved down only ~0.2 seconds from get_preds, but predict we could reduce it by almost 200 milliseconds! This is only 13% of the time! . learn.dls.device = &#39;cpu&#39; learn.model.to(&#39;cpu&#39;); . %%time # CPU: preds = learn.predict(fnames[0]) . CPU times: user 229 ms, sys: 6.8 ms, total: 236 ms Wall time: 239 ms . And a 20 millsecond reduction on the CPU! Not bad at all! So what changed? get_preds now looks very reminiscent of a PyTorch loop, with no fastai parts. This is intentional, as now we can blend the two together. . Okay... you&#39;ve sped it up, what about these &quot;Quality of Life&quot; improvements you mentioned? Let&#39;s take a look: . Quality of Life Improvements . The get_preds and predict adjustments don&#39;t end there. We&#39;ll start with the major changes to get_preds: . get_preds . get_preds allows for any of the usual fastai paramters, such as ds_idx, dl, and any other **kwargs you may want. But what has changed is now we have 3 other parameters: . raw_outs | decoded_loss | fully_decoded | . raw_outs will let you choose to apply your loss functions activation or not (default is False, so it always will). Let&#39;s see what that means from a code perspective! . Here is our CrossEntropyLossFlat, the loss function for our model: . class CrossEntropyLossFlat(BaseLoss): &quot;Same as `nn.CrossEntropyLoss`, but flattens input and target.&quot; y_int = True def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs) def decodes(self, x): return x.argmax(dim=self.axis) def activation(self, x): return F.softmax(x, dim=self.axis) . As you can see, we have an activation that applies a softmax. Let&#39;s disable it: . learn.dls.device = &#39;cuda&#39; learn.model.to(&#39;cuda&#39;) preds = learn.get_preds(raw_outs=True) . So now, if we look at the second item returned: . preds[1][0] . array([ 1.0002726 , -0.24976277, -1.058503 , -3.0604253 , -1.0200855 , -2.0843894 , -0.5362588 , -3.2754717 , -1.7710001 , -2.2774923 , -1.7333597 , -4.1619153 , 0.9354493 , 3.4019513 , 12.7702875 , 7.132491 , 2.0602374 , -3.1376252 , 1.4149432 , 5.9531183 , 4.3040857 , 1.0570182 , -2.5935738 , -1.620131 , -0.4416199 , 2.527862 , -0.9089688 , 0.51930547, -3.564454 , -3.3547723 , 6.0538034 , -1.4624698 , -4.14351 , -1.7813267 , -0.72445834, 2.1540437 , 0.07214043], dtype=float32) . We have the non-scaled results, versus: . preds = learn.get_preds(raw_outs=False) . preds[1][0] . array([7.68456448e-06, 2.20158699e-06, 9.80627874e-07, 1.32458638e-07, 1.01903447e-06, 3.51534027e-07, 1.65314952e-06, 1.06828480e-07, 4.80917777e-07, 2.89803666e-07, 4.99364774e-07, 4.40260024e-08, 7.20222897e-06, 8.48506534e-05, 9.93737578e-01, 3.53840739e-03, 2.21797090e-05, 1.22617607e-07, 1.16334395e-05, 1.08795962e-03, 2.09144753e-04, 8.13323913e-06, 2.11266979e-07, 5.59232149e-07, 1.81724408e-06, 3.54032127e-05, 1.13879651e-06, 4.75048228e-06, 8.00172444e-08, 9.86841755e-08, 1.20320532e-03, 6.54732105e-07, 4.48438442e-08, 4.75977203e-07, 1.36954998e-06, 2.43610393e-05, 3.03764227e-06], dtype=float32) . So here, if you wanted to apply some sigmoid threshold, etc for labelling on inference you may not have done while during training (such as tell your model &quot;I do not know&quot;), you can do this directly with your outputs! . Now what is in slot [0] I hear you ask? . preds[0][:5] . [&#39;basset_hound&#39;, &#39;american_bulldog&#39;, &#39;english_setter&#39;, &#39;samoyed&#39;, &#39;British_Shorthair&#39;] It&#39;s our decoded classes! Right away! This is extremely helpful for people who want to align their outputs. This relies on your dls.vocab, so let&#39;s see what happens if we don&#39;t have a vocab: . vocab = learn.dls.vocab learn.dls.vocab = None . preds = learn.get_preds(raw_outs=False) . preds[0][:5] . tensor([14, 12, 19, 31, 4], device=&#39;cuda:0&#39;) . You can see instead it&#39;s a argmax&#39;d tensor of our previous probabilties, run through the decodes of the loss function! . Alright, anything else new with get_preds? We&#39;ll move on to a tabular model for our fully_decoded: . from fastai2.tabular.all import * . path = untar_data(URLs.ADULT_SAMPLE) . df = pd.read_csv(path/&#39;adult.csv&#39;) splits = RandomSplitter()(range_of(df)) cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] procs = [Categorify, FillMissing, Normalize] y_names = &#39;salary&#39; . dls = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names, splits=splits).dataloaders(bs=512) . learn = tabular_learner(dls, layers=[200,100], metrics=accuracy) learn.fit(1) . epoch train_loss valid_loss accuracy time . 0 | 0.380894 | 0.452286 | 0.820639 | 00:00 | . Now we can make a test_dl to work with our get_preds too, let&#39;s look at one for fully_decoded: . test_dl = learn.dls.test_dl(df.iloc[:3]) . preds = learn.get_preds(dl=test_dl, fully_decoded=True) . So first we have our classes and probabilities again . preds[0], preds[1] . ([&#39;&gt;=50k&#39;, &#39;&gt;=50k&#39;, &#39;&lt;50k&#39;], array([[0.48675266, 0.51324725], [0.41627738, 0.58372265], [0.72597593, 0.27402407]], dtype=float32)) . But now we have a third item: . preds[2].show() . workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary . 0 Private | Assoc-acdm | Married-civ-spouse | #na# | Wife | White | False | 49.0 | 101319.997692 | 12.0 | &gt;=50k | . 1 Private | Masters | Divorced | Exec-managerial | Not-in-family | White | False | 44.0 | 236746.000628 | 14.0 | &gt;=50k | . 2 Private | HS-grad | Divorced | #na# | Unmarried | Black | True | 38.0 | 96185.000326 | 10.0 | &lt;50k | . Which is our DataFrame with the inputs and our outputs! This also works for vision as well, however it&#39;s easier to show with tabular. Now let&#39;s move onto .predict: . .predict . predict has some new bits too. Specifically, we can pass in with_input to possibly return our inputs, similar to what I showed above: . name, probs, row = learn.predict(df.iloc[0], with_input=True) . name, probs . ([&#39;&gt;=50k&#39;], array([[0.48675266, 0.51324725]], dtype=float32)) . row.show() . workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary . 0 Private | Assoc-acdm | Married-civ-spouse | #na# | Wife | White | False | 49.0 | 101319.997692 | 12.0 | &gt;=50k | . And this of course will also work for vision! Lastly, I want to cover the ONNX support, as it can be directly integrated into fastai . ONNX . ONNX is a special module, where you can potentially speed up inference by relying on C++ rather than Python, but it&#39;s not easy to export from fastai, or at least it was! Let&#39;s import fastinference.onnx and look at our tabular model one more time: . from fastinference.onnx import * . All we need to do is call learn.to_onnx and pass in a fname to export to both the ONNX format and export our model. We&#39;ll see why in a moment: . learn.to_onnx(&#39;tabular&#39;) . That&#39;s it! Note however that some models may not be ONNX compatable (such as the UNET), and currently it supports only one output, multiple outputs will be supported soon. Now, what can we do from there? . Let&#39;s load our model into a fastONNX model: . tab_inf = fastONNX(&#39;tabular&#39;) . Next, we&#39;ll try to time how the two different predict methods stack up. First, our improved version: . %%time _ = learn.predict(df.iloc[0]) . CPU times: user 37.9 ms, sys: 2.61 ms, total: 40.5 ms Wall time: 39 ms . And now for our ONNX version. We&#39;ll want to pass in a raw batch of data to our model, so let&#39;s grab the first batch: . test_dl.bs = 1 . batch = next(iter(test_dl)) . batch . (tensor([[5, 8, 3, 0, 6, 5, 1]], device=&#39;cuda:0&#39;), tensor([[ 0.7635, -0.8441, 0.7524]], device=&#39;cuda:0&#39;), tensor([[1]], device=&#39;cuda:0&#39;, dtype=torch.int8)) . And now let&#39;s .predict: . %%timeit _ = tab_inf.predict(batch[:2]) . The slowest run took 31.68 times longer than the fastest. This could mean that an intermediate result is being cached. 1000 loops, best of 3: 241 Âµs per loop . As you can see, lighting fast! And that&#39;s all that&#39;s needed! Eventually I may convert this into a more `fastai`-like scenario, but for now this is the framework as it lays. . I also couldn&#39;t leave enough as enough. You have the full inference capability inside of this module. Let&#39;s build a test_dl again and run get_preds as an example: . dl = tab_inf.test_dl(df.iloc[:100]) . %%time preds = tab_inf.get_preds(dl=dl) . CPU times: user 8.34 ms, sys: 0 ns, total: 8.34 ms Wall time: 9.71 ms . Just to compare with our original: . dl.device = &#39;cuda&#39; . %%time preds = learn.get_preds(dl=dl) . CPU times: user 10.6 ms, sys: 0 ns, total: 10.6 ms Wall time: 11.7 ms . We shaved off a few milliseconds too! . In the next article I&#39;ll be showing how to utilize SHAP, and then the last will include ClassConfusion. Thanks for reading! .",
            "url": "https://muellerzr.github.io/fastblog/2020/06/08/fastinference.html",
            "relUrl": "/2020/06/08/fastinference.html",
            "date": " â€¢ Jun 8, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Speeding up fastai Tabular with NumPy",
            "content": "What is fastai Tabular? A TL;DR . When working with tabular data, fastai has introduced a powerful tool to help with prerocessing your data: TabularPandas. It&#39;s super helpful and useful as you can have everything in one place, encode and decode all of your tables at once, and the memory usage on top of your Pandas dataframe can be very minimal. Let&#39;s look at an example of it. . First let&#39;s import the tabular module: . from fastai2.tabular.all import * . For our particular tests today, we&#39;ll be using the ADULT_SAMPLE dataset, where we need to identify if a particular individual makes above or below $50,000. Let&#39;s grab the data: . path = untar_data(URLs.ADULT_SAMPLE) . And now we can open it in Pandas: . df = pd.read_csv(path/&#39;adult.csv&#39;) . df.head() . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 0 49 | Private | 101320 | Assoc-acdm | 12.0 | Married-civ-spouse | NaN | Wife | White | Female | 0 | 1902 | 40 | United-States | &gt;=50k | . 1 44 | Private | 236746 | Masters | 14.0 | Divorced | Exec-managerial | Not-in-family | White | Male | 10520 | 0 | 45 | United-States | &gt;=50k | . 2 38 | Private | 96185 | HS-grad | NaN | Divorced | NaN | Unmarried | Black | Female | 0 | 0 | 32 | United-States | &lt;50k | . 3 38 | Self-emp-inc | 112847 | Prof-school | 15.0 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | United-States | &gt;=50k | . 4 42 | Self-emp-not-inc | 82297 | 7th-8th | NaN | Married-civ-spouse | Other-service | Wife | Black | Female | 0 | 0 | 50 | United-States | &lt;50k | . Now that we have our DataFrame, let&#39;s fit it into a TabularPandas object for preprocessing. To do so, we need to decalre the following: . procs (pre-processing our data, such as normalization and converting categorical values to numbers) | cat_names (categorical variables) | cont_names (continuous variables) | y_names (our y columns) | . For our case, these look like so: . cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] procs = [Categorify, FillMissing, Normalize] y_names = &#39;salary&#39; . We&#39;ll also need to tell TabularPandas how we want to split our data. We&#39;ll use a random 20% subsample: . splits = RandomSplitter()(range_of(df)) . Now let&#39;s make a TabularPandas! . to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names, splits=splits) . Now all of our data is pre-processed here and we can grab all of the raw values if we wanted to say use it with XGBoost like so: . to.train.xs.iloc[:3] . workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num . 3337 8 | 13 | 5 | 11 | 2 | 2 | 1 | -1.071338 | -0.396740 | 1.532950 | . 13162 8 | 12 | 3 | 12 | 1 | 5 | 1 | 0.030019 | 0.469650 | -0.417817 | . 10215 5 | 10 | 1 | 5 | 2 | 5 | 1 | -0.557371 | 0.392678 | 1.142797 | . Andi it&#39;s fully encoded! Now that we&#39;re a bit familiar with TabularPandas, let&#39;s do some speed tests! . The Baseline . For our tests, we&#39;ll run 4 different tests: . One batch of the training data | Iterating over the entire training dataset | Iterating over the entire validation set | Fitting for 10 epochs (GPU only) | And for each of these we will compare the times on the CPU and the GPU. . CPU: . First let&#39;s grab the first batch. The reason this is important is each time we iterate over the training DataLoader, we actually shuffle our data, which can add some time: . dls = to.dataloaders(bs=128, device=&#39;cpu&#39;) . To test our times, we&#39;ll use %%timeit. It measures the execution time of a Python function for a certain amount of loops, and reports back the fastest one. For iterating over the entire DataLoader we&#39;ll look at the time per batch as well. . First, a batch from training: . %%timeit _ = next(iter(dls.train)) . 10 loops, best of 3: 18.3 ms per loop . Now the validation: . %%timeit _ = next(iter(dls.valid)) . 100 loops, best of 3: 3.37 ms per loop . Alright, so first we can see that our shuffling function is adding almost 15 milliseconds on our time, something we can improve on! Let&#39;s then go through the entire DataLoader: . %%timeit for _ in dls.train: _ . 1 loop, best of 3: 661 ms per loop . Now let&#39;s get an average time per batch: . print(661/len(dls.train)) . 3.2561576354679804 . About 3.25 milliseconds per batch on the training dataset, let&#39;s look at the validation: . %%timeit for _ in dls.valid: _ . 10 loops, best of 3: 159 ms per loop . print(159/len(dls.valid)) . 3.1176470588235294 . And about 3.11 milliseconds per batch on the validation, so we can see that it&#39;s about the same after shuffling. Now let&#39;s compare some GPU times: . GPU . dls = to.dataloaders(bs=128, device=&#39;cuda&#39;) . %%timeit _ = next(iter(dls.train)) . 100 loops, best of 3: 18.8 ms per loop . %%timeit _ = next(iter(dls.valid)) . 100 loops, best of 3: 3.49 ms per loop . So first, grabbing just one batch we can see it added about a half a millisecond on the training and .2 milliseconds on the validation, so we&#39;re not utilizing the GPU for this process much (which makes sense, TabularPandas is CPU bound). And now let&#39;s iterate: . %%timeit for _ in dls.train: _ . 1 loop, best of 3: 693 ms per loop . print(693/len(dls.train)) . 3.413793103448276 . %%timeit for _ in dls.valid: _ . 10 loops, best of 3: 163 ms per loop . print(163/len(dls.valid)) . 3.196078431372549 . And here we can see a little bit more being added here as well. Now that we have those baselines, let&#39;s fit for ten epochs real quick: . learn = tabular_learner(dls, layers=[200,100], metrics=accuracy) . %%time learn.fit(10, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.377574 | 0.364423 | 0.833999 | 00:02 | . 1 | 0.356772 | 0.357792 | 0.835688 | 00:02 | . 2 | 0.358388 | 0.358207 | 0.833692 | 00:02 | . 3 | 0.352414 | 0.352521 | 0.840602 | 00:02 | . 4 | 0.349441 | 0.350070 | 0.840756 | 00:02 | . 5 | 0.347263 | 0.358235 | 0.841370 | 00:02 | . 6 | 0.346777 | 0.352908 | 0.838606 | 00:02 | . 7 | 0.352095 | 0.352776 | 0.839681 | 00:02 | . 8 | 0.347428 | 0.348187 | 0.840909 | 00:02 | . 9 | 0.346684 | 0.352819 | 0.835074 | 00:02 | . CPU times: user 22.2 s, sys: 263 ms, total: 22.4 s Wall time: 22.9 s . After fitting, we got about 22.9 seconds in total and ~2.29 seconds per epoch! Now that we have our baselines, let&#39;s try to speed that up! . Bringing in NumPy . The Dataset . With speeding everything up, I wanted to keep TabularPandas as it is, as it&#39;s a great way to pre-process your data! So instead we&#39;ll create a new Dataset class where we will convert our TabularPandas object into a NumPy array. Why is that important? NumPy is a super-fast library that has been hyper-optimized by using as much C code as it possibly can which is leagues faster than Python. Let&#39;s build our Dataset! . We&#39;ll want it to maintain the cats, conts, and ys from our TabularPandas object seperate. We can call to_numpy() on all of them because they are simply stored as a DataFrame! Finally, to deal with categorical versus continuous variables, we&#39;ll assign our cats as np.long and our conts as np.float32 (we also have our ys as np.int8, but this is because we&#39;re doing classification): . class TabDataset(): &quot;A `NumPy` dataset from a `TabularPandas` object&quot; def __init__(self, to): self.cats = to.cats.to_numpy().astype(np.long) self.conts = to.conts.to_numpy().astype(np.float32) self.ys = to.ys.to_numpy() . Great! Now we need a few more bits for everything to work! For our Dataset to function, we need to be able to gather the values from it each time we call from it. We use the __getitem__ function to do so! For our particular problem, we need it to return some cats, conts, and our ys. And to save on more time we&#39;ll return a whole batch of values: . class TabDataset(): &quot;A `NumPy` dataset from a `TabularPandas` object&quot; def __init__(self, to): self.cats = to.cats.to_numpy().astype(np.long) self.conts = to.conts.to_numpy().astype(np.float32) self.ys = to.ys.to_numpy() def __getitem__(self, idx): idx = idx[0] return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs] . You&#39;ll notice we don&#39;t explicitly pass in a batch size, so where is that coming from? This is added when we build our DataLoader, as we&#39;ll see later. Let&#39;s finish up our Dataset class by adding in an option to get the length of the dataset (we&#39;ll do the length of our categorical table in this case). . class TabDataset(): &quot;A `NumPy` dataset from a `TabularPandas` object&quot; def __init__(self, to): self.cats = to.cats.to_numpy().astype(np.long) self.conts = to.conts.to_numpy().astype(np.float32) self.ys = to.ys.to_numpy() def __getitem__(self, idx): idx = idx[0] return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs] def __len__(self): return len(self.cats) . And now we can make some Datasets! . train_ds = TabDataset(to.train) valid_ds = TabDataset(to.valid) . We can look at some data real quick if we want to as well! First we need to assign a batch size: . train_ds.bs = 3 . And now let&#39;s look at some data: . train_ds[[3]] . (array([[ 5, 10, 5, 5, 2, 5, 1], [ 2, 16, 3, 5, 1, 3, 1], [ 5, 16, 3, 5, 1, 5, 1]]), array([[-0.9979143 , 0.07715245, 1.1427965 ], [ 0.8376807 , 1.4486277 , -0.02766372], [ 1.4984949 , -1.4280752 , -0.02766372]], dtype=float32), array([[0], [0], [1]], dtype=int8)) . We can see that we output what could be considered a batch of data! The only thing missing is to make it into a tensor! Fantastic! Now let&#39;s build the DataLoader, as there&#39;s some pieces in it that we need, so simply having this Dataset won&#39;t be enough . The DataLoader . Now to build our DataLoader, we&#39;re going to want to modify 4 particular functions: . create_item | create_batch | get_idxs | shuffle_ds | Each of these play a particular role. First let&#39;s look at our template: . class TabDataLoader(DataLoader): def __init__(self, dataset, bs=1, num_workers=0, device=&#39;cuda&#39;, shuffle=False, **kwargs): &quot;A `DataLoader` based on a `TabDataset`&quot; super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, device=device, drop_last=shuffle, **kwargs) self.dataset.bs=bs . As you can see, our __init__ will build a DataLoader, and we keep track of our Dataset and set the Datasets batch size here as well . dl = TabDataLoader(train_ds, bs=3) . dl.dataset.bs . 3 . dl.dataset[[0]] . (array([[ 8, 13, 5, 11, 2, 2, 1], [ 8, 12, 3, 12, 1, 5, 1], [ 5, 10, 1, 5, 2, 5, 1]]), array([[-1.071338 , -0.39674038, 1.5329499 ], [ 0.03001888, 0.46965045, -0.41781712], [-0.5573715 , 0.39267784, 1.1427965 ]], dtype=float32), array([[0], [0], [0]], dtype=int8)) . And we can see that we grab everything as normal in the Dataset! Great! Now let&#39;s work on create_item and create_batch. create_item is very simple as we already do so when we make our call to the dataset, so we just pass it on. create_batch is also very simplistic. We&#39;ll take some index&#39;s from our Dataset and convert them all to Tensors! . class TabDataLoader(DataLoader): def __init__(self, dataset, bs=1, num_workers=0, device=&#39;cuda&#39;, shuffle=False, **kwargs): &quot;A `DataLoader` based on a `TabDataset`&quot; super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, device=device, drop_last=shuffle, **kwargs) self.dataset.bs=bs def create_item(self, s): return s def create_batch(self, b): cat, cont, y = self.dataset[b] return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device) . Now we&#39;re almost done. The last two pieces missing is get_idxs and shuffle_fn. These are needed as after each epoch we actually shuffle the dataset and we need to get a list of index&#39;s for our DataLoader to use! To save on time (as weâ€™re using array indexing), we can shuffle the interior dataset instead! A major benefit is slicing (consecutive idxs) instead of indexing (non-consecutive idxs). Let&#39;s look at what that looks like: . class TabDataLoader(DataLoader): def __init__(self, dataset, bs=1, num_workers=0, device=&#39;cuda&#39;, shuffle=False, **kwargs): &quot;A `DataLoader` based on a `TabDataset`&quot; super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, device=device, drop_last=shuffle, **kwargs) self.dataset.bs=bs def create_item(self, s): return s def create_batch(self, b): &quot;Create a batch of data&quot; cat, cont, y = self.dataset[b] return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device) def get_idxs(self): &quot;Get index&#39;s to select&quot; idxs = Inf.count if self.indexed else Inf.nones if self.n is not None: idxs = list(range(len(self.dataset))) return idxs def shuffle_fn(self): &quot;Shuffle the interior dataset&quot; rng = np.random.permutation(len(self.dataset)) self.dataset.cats = self.dataset.cats[rng] self.dataset.conts = self.dataset.conts[rng] self.dataset.ys = self.dataset.ys[rng] . And now we have all the pieces we need to build a DataLoader with NumPy! We&#39;ll examine it&#39;s speed now and then we&#39;ll build some convience functions later. First let&#39;s build the Datasets: . train_ds = TabDataset(to.train) valid_ds = TabDataset(to.valid) . And then the DataLoader: . train_dl = TabDataLoader(train_ds, device=&#39;cpu&#39;, shuffle=True, bs=128) valid_dl = TabDataLoader(valid_ds, device=&#39;cpu&#39;, bs=128) . And now let&#39;s grab some CPU timings similar to what we did before: . %%timeit _ = next(iter(train_dl)) . 1000 loops, best of 3: 669 Âµs per loop . %%timeit _ = next(iter(valid_dl)) . 1000 loops, best of 3: 300 Âµs per loop . Right away we can see that we are leagues faster than the previous version. Shuffling only added ~370 microseconds, which means we used 4% of the time! Now let&#39;s iterate over the entire DataLoader: . %%timeit for _ in train_dl: _ . 10 loops, best of 3: 31.8 ms per loop . print(31.8/len(train_dl)) . 0.1566502463054187 . %%timeit for _ in valid_dl: _ . 100 loops, best of 3: 8.07 ms per loop . print(8.07/len(valid_dl)) . 0.15823529411764706 . And as we can see, each individual batch of data is about 0.158 milliseconds! Yet again, about 6% of time time, quite a decrease! So we have sucessfully decreased the time! Let&#39;s look at the GPU now: . train_dl = TabDataLoader(train_ds, device=&#39;cuda&#39;, shuffle=True, bs=128) valid_dl = TabDataLoader(valid_ds, device=&#39;cuda&#39;, bs=128) . %%timeit _ = next(iter(train_dl)) . 1000 loops, best of 3: 835 Âµs per loop . %%timeit _ = next(iter(valid_dl)) . 1000 loops, best of 3: 451 Âµs per loop . %%timeit for _ in train_dl: _ . 10 loops, best of 3: 51.5 ms per loop . print(51.5/len(train_dl)) . 0.2536945812807882 . %%timeit for _ in valid_dl: _ . 100 loops, best of 3: 12.8 ms per loop . print(12.8/len(valid_dl)) . 0.25098039215686274 . Which as we can see, it adds a little bit of time from converting the tensors over to cuda. You could save a little bit more by converting first, but as this should be seperate from the dataset I decided to just keep it here. Now that we have all the steps, finally we can take a look at training! First let&#39;s build a quick helper function to make DataLoaders similar to what fastai&#39;s tabular_learner would be expecting: . class TabDataLoaders(DataLoaders): def __init__(self, to, bs=64, val_bs=None, shuffle_train=True, device=&#39;cpu&#39;, **kwargs): train_ds = TabDataset(to.train) valid_ds = TabDataset(to.valid) val_bs = bs if val_bs is None else val_bs train = TabDataLoader(train_ds, bs=bs, shuffle=shuffle_train, device=device, **kwargs) valid = TabDataLoader(valid_ds, bs=val_bs, shuffle=False, device=device, **kwargs) super().__init__(train, valid, device=device, **kwargs) . dls = TabDataLoaders(to, bs=128, device=&#39;cuda&#39;) . And now we can build our model and train! We need to build our own TabularModel here, so we&#39;ll need to grab the size of our embeddings and build a Learner. For simplicity we&#39;ll still use TabularPandas to get those sizes: . emb_szs = get_emb_sz(to) net = TabularModel(emb_szs, n_cont=3, out_sz=2, layers=[200,100]).cuda() learn = Learner(dls, net, metrics=accuracy, loss_func=CrossEntropyLossFlat()) . And now let&#39;s train! . %%time learn.fit(10, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.369785 | 0.358381 | 0.837531 | 00:01 | . 1 | 0.359938 | 0.354405 | 0.840602 | 00:01 | . 2 | 0.353965 | 0.354380 | 0.837838 | 00:01 | . 3 | 0.350551 | 0.355998 | 0.837684 | 00:01 | . 4 | 0.349042 | 0.357085 | 0.838606 | 00:01 | . 5 | 0.347858 | 0.354116 | 0.839988 | 00:01 | . 6 | 0.344613 | 0.352649 | 0.840448 | 00:01 | . 7 | 0.343187 | 0.351604 | 0.840909 | 00:01 | . 8 | 0.342587 | 0.353344 | 0.841523 | 00:01 | . 9 | 0.342127 | 0.355749 | 0.841216 | 00:01 | . CPU times: user 13.4 s, sys: 203 ms, total: 13.6 s Wall time: 13.8 s . As you can see, we cut the speed down 60%! So we saw a tremendous speed up! Let&#39;s quickly revisit all of the times and results in a pretty table. . Results . CPU? First Batch Per Batch Per Epoch Ten Epochs . fastai2 | Yes | 18.3ms (train) 3.37ms (valid) | 3.25ms (train) 3.11ms (valid) | | | . | No | 18.8ms (train) 3.49ms (valid) | 3.41ms (train) 3.19ms (valid) | 2.29s | 22.9s | . NumPy | Yes | 0.669ms (train) 0.3ms (valid | 0.15ms (train) 0.15ms (valid) | | | . | No | 0.835ms (train) 0.451ms (valid) | 0.25ms (train) 0.25ms (valid) | 1.38s | 13.8s | . So in summary, we first sped up the time to grab a single batch of data by converting everything from Pandas to NumPy. Afterwards we made a custom DataLoader that could handle these NumPy arrays and induce the speedup we saw! I hope this article helps you better understand how the interior DataLoader can be integrated in with NumPy, and that it helps you speed up your tabular training! . Small note: show_batch() etc will not work with this particular code base, this is simply a proof of concept | .",
            "url": "https://muellerzr.github.io/fastblog/2020/04/22/TabularNumpy.html",
            "relUrl": "/2020/04/22/TabularNumpy.html",
            "date": " â€¢ Apr 22, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Coral Species Identification with fastai, a Paper Comparison",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any Jupyter environment. This post was written using: . fastai2: 0.0.13 . | fastcore: 0.1.15 . | . . What is this series? . This new series of blogs will be focused on implementing various papers and attempting to beat benchmarks with the tools and techniques Fast.AI teaches. The paper we will be focusing on for this first part is Coral species identification with texture or structure images using a two-level classifier based on Convolutional Neural Networks by Gomez-Rios, Tabik, et al. I wanted to try this paper and dataset out because I was originally a marine biology major before switching to computer science and doing machine learning, and I&#39;ve been wanting to take the tools I&#39;ve learned and apply it back into the field. For a quick ELI5 of what a coral is, corals are actually living animals that pack themselves into a skeleton, living as one giant body. These skeletons are then what we see in the ocean (and what&#39;s left after coral bleaching!) . The Paper . In this paper, they describe a problem for underwater identification of corals where the current methods are very unreliable and complicated because the datasets are unrealistic. These datasets provide texture-based images while images taken in practice, say by underwater vehicles, are the complete coral structure. For reference, here is a snippet from their paper: . . As we can see, the texture images are much closer than the structured images. . These images come from the RSMAS dataset linked here. With this dataset as a guide, the authors introduce a new dataset: StructureRSMAS. While RSMAS has 766 256 by 256 pixel images of the coral textures, StructureRSMAS contains images of a variety of size, with 409 total pictures. For this first post, we&#39;ll be focusing on their results from StructureRSMAS dataset. . We&#39;ll be focusing on the Structure Classifier in this article and the StructureRSMAS dataset . In the paper they also mention how the overall final structure of the models are. Essentially they have a pyramid of models that are run dependant on the model prior to it. See the visualization below: . . Their Experiment Design . When training on StructureRSMAS, the authors looked at three different architecture designs, three different number of training epochs, and three different batch sizes. Their work was done in TensorFlow, so there&#39;s ample opportunity to bring in some of the fastai functionalities. Here was their hyperparameters they tested (table from page 17): . . What they found through their experiments was that a ResNet 50 at 300 epochs and a batch size of 32 had the highest accuracy of 83.158% without image augmentation. From here, they tried a variety of image enhancements such as Debluring, Saliency, and Contrast and Brightness Enhancement. Using a Deblurring method they further increased the accuracy to 85%. Afterwards they tried a variety of image augmentations and found it did not improve the accuracy. The results of these experiments are the average of a five-fold Cross Validation on the data. Now that we know how it was set up, we can try fitting in fastai functionalities to beat this benchmark. . Bringing in fastai . With a benchmark to work off of let&#39;s see if we can apply the fastai framework here with two goals in mind: . Higher accuracy | Less time training | . First, let&#39;s grab the library: . from fastai2.vision.all import * . As this notebook can be run top-down in Jupyter, we&#39;ll also grab the dataset in-line . url = &#39;https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/CoralClassification/StructureRSMAS.zip&#39; . We can use wget to pull our data . !wget {url} -O coral.zip -q . And then extract the zip file with ZipFile . from zipfile import ZipFile with ZipFile(&#39;coral.zip&#39;, &#39;r&#39;) as zip_ref: zip_ref.extractall(&#39;coral&#39;) . Let&#39;s look at how they have their data structured: . path = Path(&#39;coral/StructureRSMAS&#39;) . path.ls()[:2] . (#2) [Path(&#39;coral/StructureRSMAS/APAL&#39;),Path(&#39;coral/StructureRSMAS/MMEA&#39;)] . We can see that each category is stored inside of its own folder, so let&#39;s build a DataBlock for this. (For those who are unfamilair with the new API, read my post here. . The DataBlock . For augmentation, we&#39;ll utilize a presizing method taught by Jeremy, along with some augmentation. This includes increasing the brightness and contrast, something that the authors found improved the results. When doing Pre-sizing, you&#39;d generally take an image that is large and resize it to a much smaller size through cropping (either randomly cropping the image or cropping at the center), and then I further shrink this image size down. What this allows is smaller or more uncommon features in the image have a chance to be fully scene to our network. This has an advantage when your input images are all a variety of sizes and shapes. . item_tfms = Resize(256) batch_tfms = [RandomResizedCrop(224), *aug_transforms(mult=1.0, do_flip=True, max_rotate=30.0, max_zoom=1.5, max_lighting=.8, max_warp=0.3, p_lighting=.9)] . block = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter = RandomSplitter(), get_items=get_image_files, get_y=parent_label, item_tfms=item_tfms, batch_tfms=batch_tfms) . We&#39;ll build our DataLoaders using the same batch size too: . dls = block.dataloaders(path, bs=32) . Let&#39;s look at a batch . dls.show_batch() . Training, and what we will be doing differently . For their experiments they used 5 Folds for K-Fold Cross Validation, we&#39;ll recreate the process based on this notebook. We&#39;ll also use a few different image augmentations and Test Time Augmentation (TTA), the prior was not explored much and the latter was not used at all. TTA is explained further in the article. Also to note: as the dataset is extremely small we won&#39;t have a hold-out test set we evaluate on. . from sklearn.model_selection import StratifiedKFold . We&#39;ll get some setups to use scikit-learn&#39;s StratifiedKFold in the library, specifically our training images: . imgs = get_image_files(path) . Shuffle them around . random.shuffle(imgs) . And then grab their labels . lbls = [parent_label(im) for im in imgs] . We&#39;ll also be utilizing the progressive resizing technique, where we train initially on a smaller set of images before moving upwards to a larger size. We&#39;re doing this as our data comes from a variety of sized images, so this will be a good way to get the most out of our data. . Now you&#39;re probably confused about the difference between pre-sizing and progressive resizing. I&#39;ll try to explain the difference. Pre-sizing is where we initially make our image larger before applying a random crop of the image (such as 256x256 to 224x224). This can bring an opportunity for some smaller or finer details to show up more prominantly. . Progressive resizing is a technique where we start training at a small image size and then increase this image size while training. Here we&#39;ll start at an image size of 128x128 and then train on an image size of 224x224 afterwards . Now let&#39;s make a function to help us build our DataLoaders in such a way as to support progressive resizing . def get_dls(bs, size, val_idx): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=IndexSplitter(val_idx), item_tfms = Resize(256), batch_tfms = [RandomResizedCrop(size), *aug_transforms(mult=1.0, do_flip=True, max_rotate=30.0, max_zoom=1.5, max_lighting=.8, max_warp=0.3, p_lighting=.9), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . To use Stratified K-Fold Cross Validation in fastai, we&#39;ll use an IndexSplitter to pass it into the framework. Now from here our training loop will be setup to first train on 128x128 sized images for three epochs, followed by increase the size to 224x224 and train for six more. In most cases they will not run for this full time however as EarlyStopping is being used to cut-off training once the model stops improving. . From here we will employ &quot;Test Time Augmentation&quot; and report back the accuracy, which will then be averaged from all models. Test Time Augmentation is a technique where you also perform some augmentation on your test data, as it sounds like. In fastai this operates by making copies of your test images and augmenting them. By default it will make 3 copies and then these results are averaged. This has been shown to have the potential to further increase your accuracy. We&#39;ll take it for a test drive now: . Note, the output from training is not displayed as this takes a large amount of article space to run, I have summarized the results below this code block. | . val_pct = [] skf = StratifiedKFold(n_splits=5, shuffle=True) i = 0 for _, val_idx in skf.split(np.array(imgs), lbls): dls = get_dls(32, 128, val_idx) learn = cnn_learner(dls, resnet50, metrics=accuracy) learn.fine_tune(2, cbs=[EarlyStoppingCallback(monitor=&#39;accuracy&#39;)]) learn.dls = get_dls(32, 224, val_idx) learn.fine_tune(5, 1e-3, cbs=[EarlyStoppingCallback(monitor=&#39;accuracy&#39;)]) preds,targs = learn.tta() print(accuracy(preds, targs).item()) val_pct.append(accuracy(preds, targs).item()) i+=1 . Results . The results from our training were: . Fold 1: 82.9% | Fold 2: 89.0% | Fold 3: 89.0% | Fold 4: 90.2% | Fold 5: 90.1% | . All of these folds then further average to 88%, outperforming the 83% they found without augmentation, and the 85% used with augmentation. There&#39;s many little neat &quot;tricks&quot; the fastai library provides and each one can increase and further how your model performs. When we began this article there were two goals in mind, let&#39;s review them: . Decrease training time: The paper trained for 300 total epochs, we brought it down to 9 | . | Increase accuracy: The paper had an accuracy at best of 85%, we were able to boost it to 88%, with one instance where the accuracy was 90%. | Note: I did run a test with just Resizing and could match their baseline of ~83% | . | . In the next blog we&#39;ll take a look at the other ideas that were implemented in the paper and compare how different optimizers may perform on the data. .",
            "url": "https://muellerzr.github.io/fastblog/papers/2020/03/23/CoralID.html",
            "relUrl": "/papers/2020/03/23/CoralID.html",
            "date": " â€¢ Mar 23, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "The Idea of a Transform",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this: . fastai2: 0.0.13 . | fastcore: 0.1.15 . | . . The DataBlock API Continued . This is part two of my series exploring the DataBlock API. If you have not read part one, fastai and the New DataBlock API, see here In it, we discussed the ideas of the DataBlock as a whole and how each of the lego-bricks can fit together to help solve some interesting problems. In this next blog, we&#39;ll be slowly diving into more complex ideas and uses with it, such as adjusting our y values inside of our get_y, dealing with classification data seperated by folders (and the splitters we can use) . Also, as a little note, this blog is not explaining the Transform class, this will come later . From here on we&#39;ll be focusing solely on generating the DataLoaders. Seperate blogs will be made about training the various models. Now, onto the code! As we&#39;re still Vision based, we&#39;ll use the vision sub-library: . from fastai2.vision.all import * . ImageWoof . ImageWoof is a subset of 10 dogs from ImageNet. The idea is that these 10 species of dogs are extremely similar, and so they&#39;re hard to classify from scratch. We won&#39;t care about that part today, let&#39;s go through and see how the data is formatted and apply the DataBlock. First let&#39;s grab the data: . path = untar_data(URLs.IMAGEWOOF) . Now if we take a look at the path first, we&#39;ll notice that we have train and val folders. The two ideas I&#39;ll be introducing with this dataset for splitting and labelling are GrandparentSplitter and parent_label . path.ls() . (#2) [Path(&#39;/root/.fastai/data/imagewoof2/train&#39;),Path(&#39;/root/.fastai/data/imagewoof2/val&#39;)] . What do each of these do? I&#39;ll go into heavy detail on fastai&#39;s splitters and labellers but GrandparentSplitter operates with the assumption our data is split like ImageNet, where we have training data in a training folder and validation data into a validation folder such as here. Let&#39;s make a splitter now by passing in the name of the training folder and the validation folder: . splitter = GrandparentSplitter(train_name=&#39;train&#39;, valid_name=&#39;val&#39;) . Let&#39;s look at some splits. First we&#39;ll grab our list of images then use our GrandparentSplitter to seperate out two indicies for us, which we&#39;ll then look at to make sure it&#39;s working properly . items = get_image_files(path) . splits = splitter(items) . splits[0][0], splits[1][0] . (0, 9025) . Now let&#39;s look at images 0 and 9025: . items[0], items[9025] . (Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_15618.JPEG&#39;), Path(&#39;/root/.fastai/data/imagewoof2/val/n02087394/n02087394_13440.JPEG&#39;)) . And we can see that the folders line up! . Now that we have the splitter out of the way, we need a way to get our classes! But what do they look like? We&#39;ll look inside the train folder at some of the images for some examples: . train_p = path/&#39;train&#39; . train_p.ls()[:3] . (#3) [Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02115641&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02111889&#39;)] . items = get_image_files(train_p)[:5]; items . (#5) [Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_15618.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_6198.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_2253.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_7428.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_28267.JPEG&#39;)] . We can visualize this folder setup like so: . . What this tells us is that our labels are in the folder one level above the actual image, or in the parent folder (if we consider it like a tree). As such, we can use the parent_label function to extract it! Let&#39;s look: . labeller = parent_label . labeller(items[0]) . &#39;n02087394&#39; . From here we can simply build our DataBlock similar to the last post: . blocks = (ImageBlock, CategoryBlock) item_tfms=[Resize(224)] batch_tfms=[Normalize.from_stats(*imagenet_stats)] . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=parent_label, item_tfms=item_tfms, batch_tfms=batch_tfms) . And make our DataLoaders: . dls = block.dataloaders(path, bs=64) . To make sure it all worked out, let&#39;s look at a batch: . dls.show_batch(max_n=3) . The idea of a transform . Now we&#39;re still going to use the ImageWoof dataset here, but I want to introduce you to the concept of a transform. From an outside perspective and what we&#39;ve seen so far, this is normally limited to what we would call &quot;augmentation.&quot; With the new fastai this is no longer the case. Instead, let&#39;s think of a transform as &quot;any modification we can apply to our data at any point in time.&quot; . But what does that really mean? What is a transform? A function! Any transform can be written out as a simple function that we pass in at any moment. . What do I mean by this though? Let&#39;s take a look at those labels again. If we notice, we see bits like: . labeller(items[0]), labeller(items[1200]) . (&#39;n02087394&#39;, &#39;n02115641&#39;) . But that has no actual meaning to us (or anyone else reading to what we are doing). Let&#39;s use a transform that will change this into something readable. . First we&#39;ll build a dictionary that keeps track of what each original class name means: . lbl_dict = dict(n02086240= &#39;Shih-Tzu&#39;, n02087394= &#39;Rhodesian ridgeback&#39;, n02088364= &#39;Beagle&#39;, n02089973= &#39;English foxhound&#39;, n02093754= &#39;Australian terrier&#39;, n02096294= &#39;Border terrier&#39;, n02099601= &#39;Golden retriever&#39;, n02105641= &#39;Old English sheepdog&#39;, n02111889= &#39;Samoyed&#39;, n02115641= &#39;Dingo&#39; ) . Now to use this as a function, we need a way to look into the dictionary with any raw input and return back our string. This can be done via the __getitem__ function: . lbl_dict.__getitem__(labeller(items[0])) . &#39;Rhodesian ridgeback&#39; . But what is __getitem__? It&#39;s a generic python function in classes that will look up objects via a key. In our case, our object is a dictionary and so we can pass in a key value to use (such as n02105641) and it will know to return back &quot;Old English sheepdog&quot; when called . Looks readable enough now, right? So where do I put this into the API. We can stack these mini-transforms anywhere we&#39;d like them applied. For instance here, we want it done on our get_y, but after parent_label has been applied. Let&#39;s do that: . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=[parent_label, lbl_dict.__getitem__], item_tfms=item_tfms, batch_tfms=batch_tfms) . dls = block.dataloaders(path, bs=64) . dls.show_batch(max_n=3) . Awesome! It worked, and that was super simple. Does the order matter here though? Let&#39;s try reversing it: . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=[lbl_dict.__getitem__, parent_label], item_tfms=item_tfms, batch_tfms=batch_tfms) . dls = block.dataloaders(path, bs=64) . . Oh no, I got an error! What is it telling me? That I was passing in the full image path to the dictionary before we extracted the parent_label, so order does matter in how you place these functions! Further, these functions can go in any of the building blocks for the DataBlock except during data augmentation (as these require special modifications we&#39;ll look at later). .",
            "url": "https://muellerzr.github.io/fastblog/datablock/2020/03/22/TransformFunctions.html",
            "relUrl": "/datablock/2020/03/22/TransformFunctions.html",
            "date": " â€¢ Mar 22, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "fastai and the New DataBlock API",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this: . fastai2: 0.0.13 . | fastcore: 0.1.15 . | . . What is the DataBlock API? . The DataBlock API is certainly nothing new to fastai. It was here in a lesser form in the previous version, and the start of an idea. This idea was: &quot;How do we let the users of the fastai library build DataLoaders in a way that is simple enough that someone with minimal coding knowledge could get the hang of it, but be advanced enough to allow for exploration.&quot; The old version was a struggle to do this from a high-level API standpoint, as you were very limited in what you could do: variables must be passed in a particular order, the error checking wasn&#39;t very explanatory (to those unaccustomed to debugging issues), and while the general idea seemed to flow, sometimes it didn&#39;t quite work well enough. For our first example, we&#39;ll look at the Pets dataset and compare it from fastai version 1 to fastai version 2 . The DataBlock itself is built on &quot;building blocks&quot;, think of them as legos. (For more information see fastai: A Layered API for Deep Learning) They can go in any order but together they&#39;ll always build something. Our lego bricks go by these general names: . blocks | get_items | get_x/get_y | getters | splitter | item_tfms | batch_tfms | . We&#39;ll be exploring each one more closely throughout this series, so we won&#39;t hit on all of them today . Importing from the library . The library itself is still split up into modules, similar to the first version where we have Vision, Text, and Tabular. To import from these libraries, we&#39;ll be calling their .all files. Our example problem for today will involve Computer Vision so we will call from the .vision library . from fastai2.vision.all import * . Pets . Pets is a dataset in which you try to identify one of 37 different species of cats and dogs. To get the dataset, we&#39;re going to use functions very familiar to those that used fastai version 1. We&#39;ll use untar_data to grab the dataset we want. In our case, the Pets dataset lives in URLs.PETS . URLs.PETS . &#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&#39; . path = untar_data(URLs.PETS) . Looking at the dataset . When starting to look at adapting the API for a particular problem, we need to know just how the data is stored. We have an image problem here so we can use the get_image_files function to go grab all the file locations of our images and we can look at the data! . fnames = get_image_files(path/&#39;images&#39;) . To investigate how the files are named and where they are located, let&#39;s look at the first one: . fnames[0] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/pug_119.jpg&#39;) . Now as get_image_files grabs the filename of our x for us, we don&#39;t need to include our get_x here (which defaults to None) as we just want to use this filepath! Now onto our file paths and how they relate to our labels. If we look at our returned path, this particular image has the class of pug. . Where do I see that? . Here: Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/pug_119.jpg&#39;) . All the images follow this same format, and we can use a Regular Expression: to get it out. In our case, it would look something like so: . pat = r&#39;([^/]+)_ d+.*$&#39; . How do we know it worked? Let&#39;s apply it to the first file path real quick with re.search where we pass in the pattern followed by an item to try and find a match in the first group (set of matches) with a Regular Expression . re.search(pat, str(fnames[0])).group(1) . &#39;pug&#39; . We have our label! So what parts do we have so far? We know how to grab our items (get_items and get_x), our labels (get_y), what&#39;s left? Well, we&#39;ll want some way to split our data and our data augmentation. Let&#39;s focus on the prior. . Splitting and Augmentation . Any time we train a model, the data must be split between a training and validation dataset. The general idea is that the training dataset is what the model adjusts and fits its weights to, while the validation set is for us to understand how the model is performing. fastai2 has a family of split functions to look at that will slowly get covered throughout these blogs. For today we&#39;ll randomly split our data so 80% goes into our training set and 20% goes into the validation. We can utilize RandomSplitter to do so by passing in a percentage to split by, and optionally a seed as well to get the same validation split on multiple runs . splitter = RandomSplitter(valid_pct=0.2, seed=42) . How is this splitter applied? The splitter itself is a function that we can then apply over some set of data or numbers (an array). It works off of indexes. What does that look like? Let&#39;s see: . splitter(fnames) . ((#5912) [5643,5317,5806,3460,613,5456,2968,3741,10,4908...], (#1478) [4512,4290,5770,706,2200,4320,6450,501,1290,6435...]) . That doesn&#39;t look like filenames! Correct, instead its the location in our list of filenames and what group it belongs to. What this special looking list (or L) also tells us is how many items are in each list. In this example, the first (which is our training data) has 5,912 samples and the second (which is our validation) contains 1,478 samples. . Now let&#39;s move onto the augmentation. As noted earlier, there are two kinds: item_tfms and batch_tfms. Each do what it sounds like: an item transform is applied on an individual item basis, and a batch transform is applied over each batch of data. The role of the item transform is to prepare everything for a batch level (and to apply any specific item transformations you need), and the batch transform is to further apply any augmentations on the batch level efficently (normalization of your data also happens on a batch level). One of the biggest differences between the two though is where each is done. Item transforms are done on the CPU while batch transforms are performed on the GPU. . Now that we know this, let&#39;s build a basic transformation pipeline that looks something like so: . Resize our images to a fixed size (224x224 pixels) | After they are batched together, choose a quick basic augmentation function | Normalize all of our image data | Let&#39;s build it! . item_tfms = [Resize(224, method=&#39;crop&#39;)] batch_tfms=[*aug_transforms(size=256), Normalize.from_stats(*imagenet_stats)] . Woah, woah, woah, what in the world is this aug_transforms thing you just showed me I hear you ask? It runs a series of augmentations similar to the get_transforms() from version 1. The entire list is quite exhaustive and we&#39;ll discuss it in a later blog, but for now know we can pass in an image size to resize our images to (we&#39;ll make our images a bit larger, doing 256x256). . Alright, we know how we want to get our data, how to label it, split it, and augment it, what&#39;s left? That block bit I mentioned before. . The Block . Block&#39;s are used to help nest transforms inside of pre-defined problem domains. . Lazy-man&#39;s explaination? . If it&#39;s an image problem I can tell the library to use Pillow without explicitly saying it, or if we have a Bounding Box problem I can tell the DataBlock to expect two coordinates for boxes and to apply the transforms for points, again without explicitly saying these transforms. . What will we use today? Well let&#39;s think about our problem: we are using an image for our x, and our labels (or y&#39;s) are some category. Is there blocks for this? Yes! And they&#39;re labeled ImageBlock and CategoryBlock! Remember how I said it just &quot;made more sense?&quot; This is a direct example. Let&#39;s define them: . blocks = (ImageBlock, CategoryBlock) . Now let&#39;s build this DataBlock thing already! . Alright we have all the pieces now, let&#39;s see how they fit together. We&#39;ll all wrap them up in a nice little package of a DataBlock. Think of the DataBlock as a list of instructions to do when we&#39;re building batches and our DataLoaders. It doesn&#39;t need any items explicitly to be done, and instead is a blueprint of how to operate. We define it like so: . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=RegexLabeller(pat), splitter=splitter, item_tfms=item_tfms, batch_tfms=batch_tfms) . Once we have our DataBlock, we can build some DataLoaders off of it. To do so we simply pass in a source for our data that our DataBlock would be expecting, specifically our get_x and get_y, so we&#39;ll follow the same idea we did above to get our filenames and pass in a path to the folder we want to use along with a batch size: . dls = block.dataloaders(path, bs=64) . While it&#39;s a bit long, you can understand why we had to define everything the way that we did. If you&#39;re used to how fastai v1 looked with the ImageDataBunch.from_x, well this is stil here too: . dls = ImageDataLoaders.from_name_re(path, fnames, pat, item_tfms=item_tfms, batch_tfms=batch_tfms, bs=64) . I&#39;m personally a much larger fan of the first example, and if you&#39;re planning on using the library quite a bit you should get used to it more as well! This blog series will be focusing on that nomenclature specifically. To make sure everything looks okay and we like our augmentation we can show a batch of images from our DataLoader. It&#39;s as simple as: . dls.show_batch() . Fitting a Model . Now from here everything looks and behaves exactly how it did in fastai version 1: . Define a Learner | Find a learning rate | Fit | We&#39;ll quickly see that fastai2 has a quick function for transfer learning problems like we are doing, but first let&#39;s build the Learner. This will use cnn_learner, as we are doing transfer learning, and we&#39;ll tell the function to use a resnet34 architecture with accuracy metrics . learn = cnn_learner(dls, resnet34, metrics=accuracy) . Now normally we would do learn.lr_find() and find a learning rate, but with the new library, we now have a fine_tune() function we can use instead specifically designed for transfer learning scenarios. It runs a specified number of epochs (the number of times we fully go through the dataset) on a frozen model (where all but the last layer&#39;s weights are not trainable) and then the last few will be on an unfrozen model (where all weights are trainable again). When just passing in one set of epochs, like below, it will run frozen for one and unfrozen for the rest. Let&#39;s try it! . learn.fine_tune(3) . epoch train_loss valid_loss accuracy time . 0 | 1.488222 | 0.331919 | 0.893099 | 00:42 | . epoch train_loss valid_loss accuracy time . 0 | 0.471458 | 0.363768 | 0.890392 | 00:43 | . 1 | 0.368975 | 0.250430 | 0.926252 | 00:43 | . 2 | 0.205113 | 0.215602 | 0.935047 | 00:44 | . As we can see we did pretty goood just with this default! Generally when the accuracy is this high, we want to turn instead to error_rate for our metric, as this would show ~6.5% and is a better comparison when it gets very fine tuned. . But that&#39;s it for this first introduction! We looked at how the Pets dataset can be loaded into the new high-level DataBlock API, and what it&#39;s built with. In the next blog we will be exploring more variations with the DataBlock as we get more and more creative. Thanks for reading! .",
            "url": "https://muellerzr.github.io/fastblog/datablock/2020/03/21/DataBlockAPI.html",
            "relUrl": "/datablock/2020/03/21/DataBlockAPI.html",
            "date": " â€¢ Mar 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Zachary Mueller and I am currently an undergraduate student at the University of West Florida, with a graduation date of Fall 2022. Afterwards, I plan on going to Graduate School to get my Masterâ€™s Degree in Data Science . At the University I facilitate and teach the Fast.AI classes to Undergraduates and Graduates, with Jeremyâ€™s notes being my forefront go-to. Currently, the Practical Deep Learning course is happening, with plans to slowly integrate more to help teach students the benefits of knowing Deep Learning in the modern world. . Contact me . muellerzr@gmail.com .",
          "url": "https://muellerzr.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}