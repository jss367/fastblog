{
  
    
        "post0": {
            "title": "Coral Species Identification with fastai, a Paper Comparison",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any Jupyter environment. This post was written using: . fastai2: 0.0.13 . | fastcore: 0.1.15 . | . . What is this series? . This new series of blogs will be focused on implementing various papers and attempting to beat benchmarks with the tools and techniques Fast.AI teaches. The paper we will be focusing on for this first part is Coral species identification with texture or structure images using a two-level classifier based on Convolutional Neural Networks by Gomez-Rios, Tabik, et al. I wanted to try this paper and dataset out because I was originally a marine biology major before switching to computer science and doing machine learning, and I&#39;ve been wanting to take the tools I&#39;ve learned and apply it back into the field. For a quick ELI5 of what a coral is, corals are actually living animals that pack themselves into a skeleton, living as one giant body. These skeletons are then what we see in the ocean (and what&#39;s left after coral bleaching!) . The Paper . In this paper, they describe a problem for underwater identification of corals where the current methods are very unreliable and complicated because the datasets are unrealistic. These datasets provide texture-based images while images taken in practice, say by underwater vehicles, are the complete coral structure. For reference, here is a snippet from their paper: . . As we can see, the texture images are much closer than the structured images. . These images come from the RSMAS dataset linked here. With this dataset as a guide, the authors introduce a new dataset: StructureRSMAS. While RSMAS has 766 256 by 256 pixel images of the coral textures, StructureRSMAS contains images of a variety of size, with 409 total pictures. For this first post, we&#39;ll be focusing on their results from StructureRSMAS dataset. . We&#39;ll be focusing on the Structure Classifier in this article and the StructureRSMAS dataset . In the paper they also mention how the overall final structure of the models are. Essentially they have a pyramid of models that are run dependant on the model prior to it. See the visualization below: . . Their Experiment Design . When training on StructureRSMAS, the authors looked at three different architecture designs, three different number of training epochs, and three different batch sizes. Their work was done in TensorFlow, so there&#39;s ample opportunity to bring in some of the fastai functionalities. Here was their hyperparameters they tested (table from page 17): . . What they found through their experiments was that a ResNet 50 at 300 epochs and a batch size of 32 had the highest accuracy of 83.158% without image augmentation. From here, they tried a variety of image enhancements such as Debluring, Saliency, and Contrast and Brightness Enhancement. Using a Deblurring method they further increased the accuracy to 85%. Afterwards they tried a variety of image augmentations and found it did not improve the accuracy. The results of these experiments are the average of a five-fold Cross Validation on the data. Now that we know how it was set up, we can try fitting in fastai functionalities to beat this benchmark. . Bringing in fastai . With a benchmark to work off of let&#39;s see if we can apply the fastai framework here with two goals in mind: . Higher accuracy | Less time training | . First, let&#39;s grab the library: . from fastai2.vision.all import * . As this notebook can be run top-down in Jupyter, we&#39;ll also grab the dataset in-line . url = &#39;https://sci2s.ugr.es/sites/default/files/files/ComplementaryMaterial/CoralClassification/StructureRSMAS.zip&#39; . !wget {url} -O coral.zip -q . from zipfile import ZipFile with ZipFile(&#39;coral.zip&#39;, &#39;r&#39;) as zip_ref: zip_ref.extractall(&#39;coral&#39;) . Let&#39;s look at how they have their data structured: . path = Path(&#39;coral/StructureRSMAS&#39;) . path.ls()[:2] . (#2) [Path(&#39;coral/StructureRSMAS/APAL&#39;),Path(&#39;coral/StructureRSMAS/MMEA&#39;)] . We can see that each category is stored inside of its own folder, so let&#39;s build a DataBlock for this. (For those who are unfamilair with the new API, read my post here. . The DataBlock . For augmentation, we&#39;ll utilize a presizing method taught by Jeremy, along with some augmentation. This includes increasing the brightness and contrast, something that the authors found improved the results. When doing Pre-sizing, you&#39;d generally take an image that is large and resize it to a much smaller size through cropping (either randomly cropping the image or cropping at the center), and then I further shrink this image size down. What this allows is smaller or more uncommon features in the image have a chance to be fully scene to our network. This has an advantage when your input images are all a variety of sizes and shapes. . item_tfms = Resize(256) batch_tfms = [RandomResizedCrop(224), *aug_transforms(mult=1.0, do_flip=True, max_rotate=30.0, max_zoom=1.5, max_lighting=.8, max_warp=0.3, p_lighting=.9)] . block = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter = RandomSplitter(), get_items=get_image_files, get_y=parent_label, item_tfms=item_tfms, batch_tfms=batch_tfms) . We&#39;ll build our DataLoaders using the same batch size too: . dls = block.dataloaders(path, bs=32) . Let&#39;s look at a batch . dls.show_batch() . Training, and what we will be doing differently . For their experiments they used 5 Folds for K-Fold Cross Validation, we&#39;ll recreate the process based on this notebook. We&#39;ll also use a few different image augmentations and Test Time Augmentation (TTA), the prior was not explored much and the latter was not used at all. TTA is explained further in the article. Also to note: as the dataset is extremely small we won&#39;t have a hold-out test set we evaluate on. . from sklearn.model_selection import StratifiedKFold . We&#39;ll get some setups to use scikit-learn&#39;s StratifiedKFold in the library, specifically our training images: . imgs = get_image_files(path) . Shuffle them around . random.shuffle(imgs) . And then grab their labels . lbls = [parent_label(im) for im in imgs] . We&#39;ll also be utilizing the progressive resizing technique, where we train initially on a smaller set of images before moving upwards to a larger size. We&#39;re doing this as our data comes from a variety of sized images, so this will be a good way to get the most out of our data. . Now you&#39;re probably confused about the difference between pre-sizing and progressive resizing. I&#39;ll try to explain the difference. Pre-sizing is where we initially make our image larger before applying a random crop of the image (such as 256x256 to 224x224). This can bring an opportunity for some smaller or finer details to show up more prominantly. . Progressive resizing is a technique where we start training at a small image size and then increase this image size while training. Here we&#39;ll start at an image size of 128x128 and then train on an image size of 224x224 afterwards . Now let&#39;s make a function to help us build our DataLoaders in such a way as to support progressive resizing . def get_dls(bs, size, val_idx): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=IndexSplitter(val_idx), item_tfms = Resize(256), batch_tfms = [RandomResizedCrop(size), *aug_transforms(mult=1.0, do_flip=True, max_rotate=30.0, max_zoom=1.5, max_lighting=.8, max_warp=0.3, p_lighting=.9), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . To use Stratified K-Fold Cross Validation in fastai, we&#39;ll use an IndexSplitter to pass it into the framework. Now from here our training loop will be setup to first train on 128x128 sized images for three epochs, followed by increase the size to 224x224 and train for six more. In most cases they will not run for this full time however as EarlyStopping is being used to cut-off training once the model stops improving. . From here we will employ &quot;Test Time Augmentation&quot; and report back the accuracy, which will then be averaged from all models. Test Time Augmentation is a technique where you also perform some augmentation on your test data, as it sounds like. In fastai this operates by making copies of your test images and augmenting them. By default it will make 3 copies and then these results are averaged. This has been shown to have the potential to further increase your accuracy. We&#39;ll take it for a test drive now: . Note, the output from training is not displayed as this takes a large amount of article space to run, I have summarized the results below this code block. | . val_pct = [] skf = StratifiedKFold(n_splits=5, shuffle=True) i = 0 for _, val_idx in skf.split(np.array(imgs), lbls): dls = get_dls(32, 128, val_idx) learn = cnn_learner(dls, resnet50, metrics=accuracy) learn.fine_tune(2, cbs=[EarlyStoppingCallback(monitor=&#39;accuracy&#39;)]) learn.dls = get_dls(32, 224, val_idx) learn.fine_tune(5, 1e-3, cbs=[EarlyStoppingCallback(monitor=&#39;accuracy&#39;)]) preds,targs = learn.tta() print(accuracy(preds, targs).item()) val_pct.append(accuracy(preds, targs).item()) i+=1 . Results . The results from our training were: . Fold 1: 82.9% | Fold 2: 89.0% | Fold 3: 89.0% | Fold 4: 90.2% | Fold 5: 90.1% | . All of these folds then further average to 88%, outperforming the 83% they found without augmentation, and the 85% used with augmentation. There&#39;s many little neat &quot;tricks&quot; the fastai library provides and each one can increase and further how your model performs. When we began this article there were two goals in mind, let&#39;s review them: . Decrease training time: The paper trained for 300 total epochs, we brought it down to nine | . | Increase accuracy: The paper had an accuracy at best of 85%, we were able to boost it to 88%, with one instance where the accuracy was 90%. | Note: I did run a test with just Resizing and could match their baseline of ~83% | . | . In the next blog we&#39;ll take a look at the other ideas that were implemented in the paper and compare how different optimizers may perform on the data. .",
            "url": "https://muellerzr.github.io/fastblog/2020/03/23/CoralID.html",
            "relUrl": "/2020/03/23/CoralID.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "The Idea of a Transform",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this: . fastai2: 0.0.13 . | fastcore: 0.1.15 . | . . The DataBlock API Continued . This is part two of my series exploring the DataBlock API. If you have not read part one, fastai and the New DataBlock API, see here In it, we discussed the ideas of the DataBlock as a whole and how each of the lego-bricks can fit together to help solve some interesting problems. In this next blog, we&#39;ll be slowly diving into more complex ideas and uses with it, such as adjusting our y values inside of our get_y, dealing with classification data seperated by folders (and the splitters we can use) . Also, as a little note, this blog is not explaining the Transform class, this will come later . From here on we&#39;ll be focusing solely on generating the DataLoaders. Seperate blogs will be made about training the various models. Now, onto the code! As we&#39;re still Vision based, we&#39;ll use the vision sub-library: . from fastai2.vision.all import * . ImageWoof . ImageWoof is a subset of 10 dogs from ImageNet. The idea is that these 10 species of dogs are extremely similar, and so they&#39;re hard to classify from scratch. We won&#39;t care about that part today, let&#39;s go through and see how the data is formatted and apply the DataBlock. First let&#39;s grab the data: . path = untar_data(URLs.IMAGEWOOF) . Now if we take a look at the path first, we&#39;ll notice that we have train and val folders. The two ideas I&#39;ll be introducing with this dataset for splitting and labelling are GrandparentSplitter and parent_label . path.ls() . (#2) [Path(&#39;/root/.fastai/data/imagewoof2/train&#39;),Path(&#39;/root/.fastai/data/imagewoof2/val&#39;)] . What do each of these do? I&#39;ll go into heavy detail on fastai&#39;s splitters and labellers but GrandparentSplitter operates with the assumption our data is split like ImageNet, where we have training data in a training folder and validation data into a validation folder such as here. Let&#39;s make a splitter now by passing in the name of the training folder and the validation folder: . splitter = GrandparentSplitter(train_name=&#39;train&#39;, valid_name=&#39;val&#39;) . Let&#39;s look at some splits. First we&#39;ll grab our list of images then use our GrandparentSplitter to seperate out two indicies for us, which we&#39;ll then look at to make sure it&#39;s working properly . items = get_image_files(path) . splits = splitter(items) . splits[0][0], splits[1][0] . (0, 9025) . Now let&#39;s look at images 0 and 9025: . items[0], items[9025] . (Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_15618.JPEG&#39;), Path(&#39;/root/.fastai/data/imagewoof2/val/n02087394/n02087394_13440.JPEG&#39;)) . And we can see that the folders line up! . Now that we have the splitter out of the way, we need a way to get our classes! But what do they look like? We&#39;ll look inside the train folder at some of the images for some examples: . train_p = path/&#39;train&#39; . train_p.ls()[:3] . (#3) [Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02115641&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02111889&#39;)] . items = get_image_files(train_p)[:5]; items . (#5) [Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_15618.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_6198.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_2253.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_7428.JPEG&#39;),Path(&#39;/root/.fastai/data/imagewoof2/train/n02087394/n02087394_28267.JPEG&#39;)] . We can visualize this folder setup like so: . . What this tells us is that our labels are in the folder one level above the actual image, or in the parent folder (if we consider it like a tree). As such, we can use the parent_label function to extract it! Let&#39;s look: . labeller = parent_label . labeller(items[0]) . &#39;n02087394&#39; . From here we can simply build our DataBlock similar to the last post: . blocks = (ImageBlock, CategoryBlock) item_tfms=[Resize(224)] batch_tfms=[Normalize.from_stats(*imagenet_stats)] . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=parent_label, item_tfms=item_tfms, batch_tfms=batch_tfms) . And make our DataLoaders: . dls = block.dataloaders(path, bs=64) . To make sure it all worked out, let&#39;s look at a batch: . dls.show_batch(max_n=3) . The idea of a transform . Now we&#39;re still going to use the ImageWoof dataset here, but I want to introduce you to the concept of a transform. From an outside perspective and what we&#39;ve seen so far, this is normally limited to what we would call &quot;augmentation.&quot; With the new fastai this is no longer the case. Instead, let&#39;s think of a transform as &quot;any modification we can apply to our data at any point in time.&quot; . But what does that really mean? What is a transform? A function! Any transform can be written out as a simple function that we pass in at any moment. . What do I mean by this though? Let&#39;s take a look at those labels again. If we notice, we see bits like: . labeller(items[0]), labeller(items[1200]) . (&#39;n02087394&#39;, &#39;n02115641&#39;) . But that has no actual meaning to us (or anyone else reading to what we are doing). Let&#39;s use a transform that will change this into something readable. . First we&#39;ll build a dictionary that keeps track of what each original class name means: . lbl_dict = dict(n02086240= &#39;Shih-Tzu&#39;, n02087394= &#39;Rhodesian ridgeback&#39;, n02088364= &#39;Beagle&#39;, n02089973= &#39;English foxhound&#39;, n02093754= &#39;Australian terrier&#39;, n02096294= &#39;Border terrier&#39;, n02099601= &#39;Golden retriever&#39;, n02105641= &#39;Old English sheepdog&#39;, n02111889= &#39;Samoyed&#39;, n02115641= &#39;Dingo&#39; ) . Now to use this as a function, we need a way to look into the dictionary with any raw input and return back our string. This can be done via the __getitem__ function: . lbl_dict.__getitem__(labeller(items[0])) . &#39;Rhodesian ridgeback&#39; . But what is __getitem__? It&#39;s a generic python function in classes that will look up objects via a key. In our case, our object is a dictionary and so we can pass in a key value to use (such as n02105641) and it will know to return back &quot;Old English sheepdog&quot; when called . Looks readable enough now, right? So where do I put this into the API. We can stack these mini-transforms anywhere we&#39;d like them applied. For instance here, we want it done on our get_y, but after parent_label has been applied. Let&#39;s do that: . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=[parent_label, lbl_dict.__getitem__], item_tfms=item_tfms, batch_tfms=batch_tfms) . dls = block.dataloaders(path, bs=64) . dls.show_batch(max_n=3) . Awesome! It worked, and that was super simple. Does the order matter here though? Let&#39;s try reversing it: . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=[lbl_dict.__getitem__, parent_label], item_tfms=item_tfms, batch_tfms=batch_tfms) . dls = block.dataloaders(path, bs=64) . . Oh no, I got an error! What is it telling me? That I was passing in the full image path to the dictionary before we extracted the parent_label, so order does matter in how you place these functions! Further, these functions can go in any of the building blocks for the DataBlock except during data augmentation (as these require special modifications we&#39;ll look at later). .",
            "url": "https://muellerzr.github.io/fastblog/datablock/2020/03/22/TransformFunctions.html",
            "relUrl": "/datablock/2020/03/22/TransformFunctions.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "fastai and the New DataBlock API",
            "content": ". This blog is also a Jupyter notebook available to run from the top down. There will be code snippets that you can then run in any environment. In this section I will be posting what version of fastai2 and fastcore I am currently running at the time of writing this: . fastai2: 0.0.13 . | fastcore: 0.1.15 . | . . What is the DataBlock API? . The DataBlock API is certainly nothing new to fastai. It was here in a lesser form in the previous version, and the start of an idea. This idea was: &quot;How do we let the users of the fastai library build DataLoaders in a way that is simple enough that someone with minimal coding knowledge could get the hang of it, but be advanced enough to allow for exploration.&quot; The old version was a struggle to do this from a high-level API standpoint, as you were very limited in what you could do: variables must be passed in a particular order, the error checking wasn&#39;t very explanatory (to those unaccustomed to debugging issues), and while the general idea seemed to flow, sometimes it didn&#39;t quite work well enough. For our first example, we&#39;ll look at the Pets dataset and compare it from fastai version 1 to fastai version 2 . The DataBlock itself is built on &quot;building blocks&quot;, think of them as legos. (For more information see fastai: A Layered API for Deep Learning) They can go in any order but together they&#39;ll always build something. Our lego bricks go by these general names: . blocks | get_items | get_x/get_y | getters | splitter | item_tfms | batch_tfms | . We&#39;ll be exploring each one more closely throughout this series, so we won&#39;t hit on all of them today . Importing from the library . The library itself is still split up into modules, similar to the first version where we have Vision, Text, and Tabular. To import from these libraries, we&#39;ll be calling their .all files. Our example problem for today will involve Computer Vision so we will call from the .vision library . from fastai2.vision.all import * . Pets . Pets is a dataset in which you try to identify one of 37 different species of cats and dogs. To get the dataset, we&#39;re going to use functions very familiar to those that used fastai version 1. We&#39;ll use untar_data to grab the dataset we want. In our case, the Pets dataset lives in URLs.PETS . URLs.PETS . &#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&#39; . path = untar_data(URLs.PETS) . Looking at the dataset . When starting to look at adapting the API for a particular problem, we need to know just how the data is stored. We have an image problem here so we can use the get_image_files function to go grab all the file locations of our images and we can look at the data! . fnames = get_image_files(path/&#39;images&#39;) . To investigate how the files are named and where they are located, let&#39;s look at the first one: . fnames[0] . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/pug_119.jpg&#39;) . Now as get_image_files grabs the filename of our x for us, we don&#39;t need to include our get_x here (which defaults to None) as we just want to use this filepath! Now onto our file paths and how they relate to our labels. If we look at our returned path, this particular image has the class of pug. . Where do I see that? . Here: Path(&#39;/root/.fastai/data/oxford-iiit-pet/images/pug_119.jpg&#39;) . All the images follow this same format, and we can use a Regular Expression: to get it out. In our case, it would look something like so: . pat = r&#39;([^/]+)_ d+.*$&#39; . How do we know it worked? Let&#39;s apply it to the first file path real quick with re.search where we pass in the pattern followed by an item to try and find a match in the first group (set of matches) with a Regular Expression . re.search(pat, str(fnames[0])).group(1) . &#39;pug&#39; . We have our label! So what parts do we have so far? We know how to grab our items (get_items and get_x), our labels (get_y), what&#39;s left? Well, we&#39;ll want some way to split our data and our data augmentation. Let&#39;s focus on the prior. . Splitting and Augmentation . Any time we train a model, the data must be split between a training and validation dataset. The general idea is that the training dataset is what the model adjusts and fits its weights to, while the validation set is for us to understand how the model is performing. fastai2 has a family of split functions to look at that will slowly get covered throughout these blogs. For today we&#39;ll randomly split our data so 80% goes into our training set and 20% goes into the validation. We can utilize RandomSplitter to do so by passing in a percentage to split by, and optionally a seed as well to get the same validation split on multiple runs . splitter = RandomSplitter(valid_pct=0.2, seed=42) . How is this splitter applied? The splitter itself is a function that we can then apply over some set of data or numbers (an array). It works off of indexes. What does that look like? Let&#39;s see: . splitter(fnames) . ((#5912) [5643,5317,5806,3460,613,5456,2968,3741,10,4908...], (#1478) [4512,4290,5770,706,2200,4320,6450,501,1290,6435...]) . That doesn&#39;t look like filenames! Correct, instead its the location in our list of filenames and what group it belongs to. What this special looking list (or L) also tells us is how many items are in each list. In this example, the first (which is our training data) has 5,912 samples and the second (which is our validation) contains 1,478 samples. . Now let&#39;s move onto the augmentation. As noted earlier, there are two kinds: item_tfms and batch_tfms. Each do what it sounds like: an item transform is applied on an individual item basis, and a batch transform is applied over each batch of data. The role of the item transform is to prepare everything for a batch level (and to apply any specific item transformations you need), and the batch transform is to further apply any augmentations on the batch level efficently (normalization of your data also happens on a batch level). One of the biggest differences between the two though is where each is done. Item transforms are done on the CPU while batch transforms are performed on the GPU. . Now that we know this, let&#39;s build a basic transformation pipeline that looks something like so: . Resize our images to a fixed size (224x224 pixels) | After they are batched together, choose a quick basic augmentation function | Normalize all of our image data | Let&#39;s build it! . item_tfms = [Resize(224, method=&#39;crop&#39;)] batch_tfms=[*aug_transforms(size=256), Normalize.from_stats(*imagenet_stats)] . Woah, woah, woah, what in the world is this aug_transforms thing you just showed me I hear you ask? It runs a series of augmentations similar to the get_transforms() from version 1. The entire list is quite exhaustive and we&#39;ll discuss it in a later blog, but for now know we can pass in an image size to resize our images to (we&#39;ll make our images a bit larger, doing 256x256). . Alright, we know how we want to get our data, how to label it, split it, and augment it, what&#39;s left? That block bit I mentioned before. . The Block . Block&#39;s are used to help nest transforms inside of pre-defined problem domains. . Lazy-man&#39;s explaination? . If it&#39;s an image problem I can tell the library to use Pillow without explicitly saying it, or if we have a Bounding Box problem I can tell the DataBlock to expect two coordinates for boxes and to apply the transforms for points, again without explicitly saying these transforms. . What will we use today? Well let&#39;s think about our problem: we are using an image for our x, and our labels (or y&#39;s) are some category. Is there blocks for this? Yes! And they&#39;re labeled ImageBlock and CategoryBlock! Remember how I said it just &quot;made more sense?&quot; This is a direct example. Let&#39;s define them: . blocks = (ImageBlock, CategoryBlock) . Now let&#39;s build this DataBlock thing already! . Alright we have all the pieces now, let&#39;s see how they fit together. We&#39;ll all wrap them up in a nice little package of a DataBlock. Think of the DataBlock as a list of instructions to do when we&#39;re building batches and our DataLoaders. It doesn&#39;t need any items explicitly to be done, and instead is a blueprint of how to operate. We define it like so: . block = DataBlock(blocks=blocks, get_items=get_image_files, get_y=RegexLabeller(pat), splitter=splitter, item_tfms=item_tfms, batch_tfms=batch_tfms) . Once we have our DataBlock, we can build some DataLoaders off of it. To do so we simply pass in a source for our data that our DataBlock would be expecting, specifically our get_x and get_y, so we&#39;ll follow the same idea we did above to get our filenames and pass in a path to the folder we want to use along with a batch size: . dls = block.dataloaders(path, bs=64) . While it&#39;s a bit long, you can understand why we had to define everything the way that we did. If you&#39;re used to how fastai v1 looked with the ImageDataBunch.from_x, well this is stil here too: . dls = ImageDataLoaders.from_name_re(path, fnames, pat, item_tfms=item_tfms, batch_tfms=batch_tfms, bs=64) . I&#39;m personally a much larger fan of the first example, and if you&#39;re planning on using the library quite a bit you should get used to it more as well! This blog series will be focusing on that nomenclature specifically. To make sure everything looks okay and we like our augmentation we can show a batch of images from our DataLoader. It&#39;s as simple as: . dls.show_batch() . Fitting a Model . Now from here everything looks and behaves exactly how it did in fastai version 1: . Define a Learner | Find a learning rate | Fit | We&#39;ll quickly see that fastai2 has a quick function for transfer learning problems like we are doing, but first let&#39;s build the Learner. This will use cnn_learner, as we are doing transfer learning, and we&#39;ll tell the function to use a resnet34 architecture with accuracy metrics . learn = cnn_learner(dls, resnet34, metrics=accuracy) . Now normally we would do learn.lr_find() and find a learning rate, but with the new library, we now have a fine_tune() function we can use instead specifically designed for transfer learning scenarios. It runs a specified number of epochs (the number of times we fully go through the dataset) on a frozen model (where all but the last layer&#39;s weights are not trainable) and then the last few will be on an unfrozen model (where all weights are trainable again). When just passing in one set of epochs, like below, it will run frozen for one and unfrozen for the rest. Let&#39;s try it! . learn.fine_tune(3) . epoch train_loss valid_loss accuracy time . 0 | 1.488222 | 0.331919 | 0.893099 | 00:42 | . epoch train_loss valid_loss accuracy time . 0 | 0.471458 | 0.363768 | 0.890392 | 00:43 | . 1 | 0.368975 | 0.250430 | 0.926252 | 00:43 | . 2 | 0.205113 | 0.215602 | 0.935047 | 00:44 | . As we can see we did pretty goood just with this default! Generally when the accuracy is this high, we want to turn instead to error_rate for our metric, as this would show ~6.5% and is a better comparison when it gets very fine tuned. . But that&#39;s it for this first introduction! We looked at how the Pets dataset can be loaded into the new high-level DataBlock API, and what it&#39;s built with. In the next blog we will be exploring more variations with the DataBlock as we get more and more creative. Thanks for reading! .",
            "url": "https://muellerzr.github.io/fastblog/datablock/2020/03/21/DataBlockAPI.html",
            "relUrl": "/datablock/2020/03/21/DataBlockAPI.html",
            "date": " • Mar 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am currently an undergraduate student at the University of West Florida, with a graduation date of Fall 2022. Afterwards, I plan on going to Graduate School to get my Master’s Degree in Data Science . At the University I facilitate and teach the Fast.AI classes to Undergraduates and Graduates, with Jeremy’s notes being my forefront go-to. Currently, the Practical Deep Learning course is happening, with plans to slowly integrate more to help teach students the benefits of knowing Deep Learning in the modern world. . Contact me . muellerzr@gmail.com .",
          "url": "https://muellerzr.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}